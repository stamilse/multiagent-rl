python train.py --scenario simple_push --num-adversaries 1 --lr_actor 0.005 --lr_critic 0.01 --lr_lamda 0.0001 --alpha 0.01
2018-10-27 23:32:07.949827: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -26.954966745206757, agent episode reward: [2.355784296570984, -29.310751041777745], time: 18.712
/anaconda2/envs/py36/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/anaconda2/envs/py36/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
Running avgs for agent 0: q_loss: nan, q_loss2: nan, p_loss: nan, mean_rew: nan, var_rew: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, q_loss2: nan, p_loss: nan, mean_rew: nan, var_rew: nan, lamda: nan
steps: 49975, episodes: 2000, mean episode reward: -20.488170578486365, agent episode reward: [-1.1114596006213602, -19.376710977865002], time: 29.82
Running avgs for agent 0: q_loss: 0.34325891733169556, q_loss2: -0.32113754749298096, p_loss: -0.07949621230363846, mean_rew: 0.09280487437770804, var_rew: 0.11417030543088913, lamda: 1.0112390518188477
Running avgs for agent 1: q_loss: 0.5231966376304626, q_loss2: -0.4835143983364105, p_loss: 1.609240174293518, mean_rew: -1.1319972740092126, var_rew: -0.011886323802173138, lamda: 1.011323094367981
steps: 74975, episodes: 3000, mean episode reward: -12.119756798115596, agent episode reward: [-4.57084892204766, -7.548907876067935], time: 29.105
Running avgs for agent 0: q_loss: 1.4340596199035645, q_loss2: -1.3646337985992432, p_loss: -0.08683586120605469, mean_rew: -0.01700700243642842, var_rew: -0.01209213025867939, lamda: 1.0368971824645996
Running avgs for agent 1: q_loss: 2.1635191440582275, q_loss2: -2.1476011276245117, p_loss: 2.4632463455200195, mean_rew: -0.8513052904202044, var_rew: -0.051150284707546234, lamda: 1.0370793342590332
steps: 99975, episodes: 4000, mean episode reward: -9.933298853982611, agent episode reward: [-2.694806387192952, -7.238492466789658], time: 29.15
Running avgs for agent 0: q_loss: 6.089259147644043, q_loss2: -5.583078384399414, p_loss: 0.012250291183590889, mean_rew: -0.05477709773354457, var_rew: -0.020282894372940063, lamda: 1.0636656284332275
Running avgs for agent 1: q_loss: 6.532200813293457, q_loss2: -6.537534236907959, p_loss: 2.801980495452881, mean_rew: -0.6881813695937864, var_rew: -0.02960267663002014, lamda: 1.0634260177612305
steps: 124975, episodes: 5000, mean episode reward: -9.157527357115775, agent episode reward: [-1.831136007463771, -7.326391349652005], time: 29.458
Running avgs for agent 0: q_loss: 11.420578956604004, q_loss2: -10.518692970275879, p_loss: -0.005680338013917208, mean_rew: -0.06190137844361105, var_rew: -0.022062717005610466, lamda: 1.0896321535110474
Running avgs for agent 1: q_loss: 12.583284378051758, q_loss2: -12.152052879333496, p_loss: 2.8261444568634033, mean_rew: -0.596750995270669, var_rew: -0.03370180353522301, lamda: 1.0891534090042114
steps: 149975, episodes: 6000, mean episode reward: -9.677094500556509, agent episode reward: [-2.071826623027779, -7.6052678775287275], time: 29.075
Running avgs for agent 0: q_loss: 17.4675350189209, q_loss2: -15.399991989135742, p_loss: -0.14298035204410553, mean_rew: -0.06629261341927685, var_rew: -0.06310015916824341, lamda: 1.1152392625808716
Running avgs for agent 1: q_loss: 24.597305297851562, q_loss2: -23.285358428955078, p_loss: 2.723355531692505, mean_rew: -0.5434605307304552, var_rew: 0.007252447307109833, lamda: 1.1146129369735718
steps: 174975, episodes: 7000, mean episode reward: -9.82629924818502, agent episode reward: [-2.0837285527991463, -7.742570695385873], time: 29.952
Running avgs for agent 0: q_loss: 39.403141021728516, q_loss2: -33.576515197753906, p_loss: -0.32160434126853943, mean_rew: -0.06578631704817346, var_rew: -0.08564581722021103, lamda: 1.1406508684158325
Running avgs for agent 1: q_loss: 24.819019317626953, q_loss2: -23.37318229675293, p_loss: 2.5562524795532227, mean_rew: -0.5080908023793557, var_rew: 0.06346473097801208, lamda: 1.139925241470337
steps: 199975, episodes: 8000, mean episode reward: -9.836859192711039, agent episode reward: [-1.7349549742515114, -8.101904218459529], time: 29.567
Running avgs for agent 0: q_loss: 32.13459777832031, q_loss2: -28.300682067871094, p_loss: -0.41882267594337463, mean_rew: -0.06904242196826382, var_rew: -0.03460202366113663, lamda: 1.1659420728683472
Running avgs for agent 1: q_loss: 54.37772750854492, q_loss2: -52.855350494384766, p_loss: 2.469095468521118, mean_rew: -0.4793894358760927, var_rew: -0.09352555125951767, lamda: 1.1651467084884644
steps: 224975, episodes: 9000, mean episode reward: -10.11192336599144, agent episode reward: [-1.8898003425001535, -8.222123023491287], time: 29.897
Running avgs for agent 0: q_loss: 52.2100715637207, q_loss2: -41.90331268310547, p_loss: -0.5569136738777161, mean_rew: -0.06948689198045623, var_rew: -0.03237514570355415, lamda: 1.1911540031433105
Running avgs for agent 1: q_loss: 41.6976318359375, q_loss2: -37.186553955078125, p_loss: 2.453864336013794, mean_rew: -0.46355625714354826, var_rew: 0.03303292766213417, lamda: 1.1903076171875
steps: 249975, episodes: 10000, mean episode reward: -10.15584641529437, agent episode reward: [-1.0212330552731512, -9.13461336002122], time: 29.597
Running avgs for agent 0: q_loss: 49.775516510009766, q_loss2: -39.64357376098633, p_loss: -0.7485620379447937, mean_rew: -0.06988830451625387, var_rew: -0.09801916033029556, lamda: 1.2163105010986328
Running avgs for agent 1: q_loss: 65.84371185302734, q_loss2: -55.84535598754883, p_loss: 2.47892427444458, mean_rew: -0.45027513608958214, var_rew: -0.03960075601935387, lamda: 1.215427279472351
steps: 274975, episodes: 11000, mean episode reward: -10.37350943636465, agent episode reward: [-1.2293603761300833, -9.144149060234566], time: 30.67
Running avgs for agent 0: q_loss: 73.7405014038086, q_loss2: -64.22118377685547, p_loss: -0.8790537714958191, mean_rew: -0.06597113653683667, var_rew: -0.06650692224502563, lamda: 1.241428256034851
Running avgs for agent 1: q_loss: 54.74270248413086, q_loss2: -47.96913146972656, p_loss: 2.5087709426879883, mean_rew: -0.4430514494939085, var_rew: -0.058921173214912415, lamda: 1.2405176162719727
steps: 299975, episodes: 12000, mean episode reward: -11.030419349113819, agent episode reward: [-2.338654657023622, -8.691764692090198], time: 29.835
Running avgs for agent 0: q_loss: 66.95758056640625, q_loss2: -57.4587287902832, p_loss: -0.9085779786109924, mean_rew: -0.06515488466763353, var_rew: 0.015300550498068333, lamda: 1.2665181159973145
Running avgs for agent 1: q_loss: 82.29389190673828, q_loss2: -72.62476348876953, p_loss: 2.5953104496002197, mean_rew: -0.4356940181341072, var_rew: 0.03846988081932068, lamda: 1.2655842304229736
steps: 324975, episodes: 13000, mean episode reward: -10.95605090374469, agent episode reward: [-2.4443284438170085, -8.511722459927684], time: 29.498
Running avgs for agent 0: q_loss: 68.4708023071289, q_loss2: -56.167049407958984, p_loss: -0.9137078523635864, mean_rew: -0.06816513072441852, var_rew: -0.03512250632047653, lamda: 1.2915846109390259
Running avgs for agent 1: q_loss: 61.16421890258789, q_loss2: -53.53508758544922, p_loss: 2.6648964881896973, mean_rew: -0.4290101132450161, var_rew: -0.05194056034088135, lamda: 1.2906382083892822
steps: 349975, episodes: 14000, mean episode reward: -11.545249416270387, agent episode reward: [-3.007040434894385, -8.538208981376004], time: 29.607
Running avgs for agent 0: q_loss: 91.67596435546875, q_loss2: -73.14456176757812, p_loss: -0.8953467607498169, mean_rew: -0.07172330970625265, var_rew: 0.04506745934486389, lamda: 1.3166391849517822
Running avgs for agent 1: q_loss: 86.50015258789062, q_loss2: -72.32223510742188, p_loss: 2.699955940246582, mean_rew: -0.42362612765674124, var_rew: 0.018413512036204338, lamda: 1.3156726360321045
steps: 374975, episodes: 15000, mean episode reward: -11.855247584956686, agent episode reward: [-3.619652068954228, -8.235595516002457], time: 29.874
Running avgs for agent 0: q_loss: 66.13826751708984, q_loss2: -50.040584564208984, p_loss: -0.8769815564155579, mean_rew: -0.07585936368488169, var_rew: -0.04635845497250557, lamda: 1.341673731803894
Running avgs for agent 1: q_loss: 49.92808532714844, q_loss2: -41.988834381103516, p_loss: 2.6435322761535645, mean_rew: -0.41577616948438906, var_rew: 0.008102619089186192, lamda: 1.3407065868377686
steps: 399975, episodes: 16000, mean episode reward: -12.065273964476978, agent episode reward: [-3.6984539560898226, -8.366820008387156], time: 29.692
Running avgs for agent 0: q_loss: 78.21179962158203, q_loss2: -61.43760681152344, p_loss: -0.9197737574577332, mean_rew: -0.07928667902916788, var_rew: -0.07883259654045105, lamda: 1.3667075634002686
Running avgs for agent 1: q_loss: 54.91505813598633, q_loss2: -47.457645416259766, p_loss: 2.588977575302124, mean_rew: -0.41106630244163445, var_rew: 0.04255381599068642, lamda: 1.3657399415969849
steps: 424975, episodes: 17000, mean episode reward: -11.516549165453082, agent episode reward: [-3.0119003753369613, -8.504648790116121], time: 29.623
Running avgs for agent 0: q_loss: 89.21204376220703, q_loss2: -74.85305786132812, p_loss: -0.8915085196495056, mean_rew: -0.08432793618158856, var_rew: -0.02133330889046192, lamda: 1.391741156578064
Running avgs for agent 1: q_loss: 66.63263702392578, q_loss2: -54.700469970703125, p_loss: 2.596578598022461, mean_rew: -0.40429750600264147, var_rew: -0.02205985225737095, lamda: 1.3907538652420044
steps: 449975, episodes: 18000, mean episode reward: -11.598181485344945, agent episode reward: [-2.570718967817704, -9.027462517527239], time: 29.452
Running avgs for agent 0: q_loss: 110.4876937866211, q_loss2: -94.25678253173828, p_loss: -0.9189032316207886, mean_rew: -0.08453237516129677, var_rew: -0.046591512858867645, lamda: 1.4167563915252686
Running avgs for agent 1: q_loss: 62.368934631347656, q_loss2: -55.34485626220703, p_loss: 2.62766170501709, mean_rew: -0.40348343051582175, var_rew: 0.008280333131551743, lamda: 1.4157580137252808
steps: 474975, episodes: 19000, mean episode reward: -11.245991146502915, agent episode reward: [-2.120684347123269, -9.125306799379645], time: 29.459
Running avgs for agent 0: q_loss: 104.92095184326172, q_loss2: -89.59143829345703, p_loss: -0.8101513385772705, mean_rew: -0.08558793617357476, var_rew: 0.036380767822265625, lamda: 1.441760540008545
Running avgs for agent 1: q_loss: 48.29652786254883, q_loss2: -39.742591857910156, p_loss: 2.6501119136810303, mean_rew: -0.4003110424596701, var_rew: 0.03915269300341606, lamda: 1.4407621622085571
steps: 499975, episodes: 20000, mean episode reward: -11.542263764534187, agent episode reward: [-2.1875174341737393, -9.354746330360447], time: 30.142
Running avgs for agent 0: q_loss: 102.03539276123047, q_loss2: -78.31779479980469, p_loss: -0.8691810965538025, mean_rew: -0.08446485097618402, var_rew: -0.10455616563558578, lamda: 1.4667646884918213
Running avgs for agent 1: q_loss: 78.5597915649414, q_loss2: -65.65484619140625, p_loss: 2.687120199203491, mean_rew: -0.3995775964650169, var_rew: 0.008402503095567226, lamda: 1.465766191482544
steps: 524975, episodes: 21000, mean episode reward: -12.067835816828559, agent episode reward: [-2.2850148194268423, -9.782820997401716], time: 30.319
Running avgs for agent 0: q_loss: 83.3267822265625, q_loss2: -61.841819763183594, p_loss: -0.812613308429718, mean_rew: -0.08568040181210852, var_rew: -0.019744275137782097, lamda: 1.491768717765808
Running avgs for agent 1: q_loss: 53.66852951049805, q_loss2: -43.083335876464844, p_loss: 2.7419826984405518, mean_rew: -0.3981414325591748, var_rew: -0.054545316845178604, lamda: 1.4907704591751099
steps: 549975, episodes: 22000, mean episode reward: -12.91800220416176, agent episode reward: [-2.331013387265619, -10.58698881689614], time: 29.968
Running avgs for agent 0: q_loss: 97.02386474609375, q_loss2: -76.21879577636719, p_loss: -0.7183855175971985, mean_rew: -0.08650087132019331, var_rew: -0.006786803249269724, lamda: 1.516772985458374
Running avgs for agent 1: q_loss: 34.81016540527344, q_loss2: -27.613269805908203, p_loss: 2.7644095420837402, mean_rew: -0.39867858187647126, var_rew: 0.06857409328222275, lamda: 1.5157746076583862
steps: 574975, episodes: 23000, mean episode reward: -12.579749124224007, agent episode reward: [-2.304179436076598, -10.27556968814741], time: 29.71
Running avgs for agent 0: q_loss: 111.41165924072266, q_loss2: -92.80309295654297, p_loss: -0.6705528497695923, mean_rew: -0.08665177742166302, var_rew: -0.047222916036844254, lamda: 1.5417771339416504
Running avgs for agent 1: q_loss: 75.1978759765625, q_loss2: -58.383148193359375, p_loss: 2.807124376296997, mean_rew: -0.39896614256020624, var_rew: -0.020639639347791672, lamda: 1.5407787561416626
steps: 599975, episodes: 24000, mean episode reward: -12.549055506184406, agent episode reward: [-2.105565761205288, -10.443489744979116], time: 29.702
Running avgs for agent 0: q_loss: 83.83714294433594, q_loss2: -66.05301666259766, p_loss: -0.5929123163223267, mean_rew: -0.08719906469878193, var_rew: 0.03269777074456215, lamda: 1.5667812824249268
Running avgs for agent 1: q_loss: 42.87602996826172, q_loss2: -35.20944595336914, p_loss: 2.867356061935425, mean_rew: -0.4005929839733917, var_rew: 0.0015474243555217981, lamda: 1.5657827854156494
steps: 624975, episodes: 25000, mean episode reward: -12.688718254701978, agent episode reward: [-2.2656175663729043, -10.423100688329074], time: 29.699
Running avgs for agent 0: q_loss: 82.9687271118164, q_loss2: -63.92279052734375, p_loss: -0.5862224102020264, mean_rew: -0.08580738499897635, var_rew: -0.0371224544942379, lamda: 1.5917854309082031
Running avgs for agent 1: q_loss: 50.00452423095703, q_loss2: -41.35679626464844, p_loss: 2.9480462074279785, mean_rew: -0.40153024573274787, var_rew: -0.06408648937940598, lamda: 1.5907870531082153
steps: 649975, episodes: 26000, mean episode reward: -12.413286927775825, agent episode reward: [-3.3263564862440544, -9.086930441531772], time: 29.52
Running avgs for agent 0: q_loss: 80.12822723388672, q_loss2: -60.438194274902344, p_loss: -0.5640870928764343, mean_rew: -0.08812464921358122, var_rew: -0.0055799032561481, lamda: 1.6167895793914795
Running avgs for agent 1: q_loss: 44.51380157470703, q_loss2: -37.8005485534668, p_loss: 2.9942705631256104, mean_rew: -0.400433792168574, var_rew: -0.003173095639795065, lamda: 1.6157910823822021
steps: 674975, episodes: 27000, mean episode reward: -12.125640476478454, agent episode reward: [-3.1893671004742785, -8.936273376004173], time: 30.271
Running avgs for agent 0: q_loss: 86.2767333984375, q_loss2: -65.1146240234375, p_loss: -0.5335836410522461, mean_rew: -0.08917368886699012, var_rew: 0.006041335873305798, lamda: 1.6417936086654663
Running avgs for agent 1: q_loss: 74.38085174560547, q_loss2: -61.94258117675781, p_loss: 2.996178388595581, mean_rew: -0.39979817587123495, var_rew: 0.06693345308303833, lamda: 1.6407954692840576
steps: 699975, episodes: 28000, mean episode reward: -11.89372271573353, agent episode reward: [-3.2775997147379785, -8.61612300099555], time: 29.796
Running avgs for agent 0: q_loss: 121.97819519042969, q_loss2: -98.0016860961914, p_loss: -0.5739573836326599, mean_rew: -0.09179546206278098, var_rew: -0.004051458090543747, lamda: 1.6667978763580322
Running avgs for agent 1: q_loss: 47.85984420776367, q_loss2: -39.922019958496094, p_loss: 2.9650092124938965, mean_rew: -0.3979782931437473, var_rew: -0.005243193823844194, lamda: 1.665799617767334
steps: 724975, episodes: 29000, mean episode reward: -11.891188821200414, agent episode reward: [-3.6010550965793064, -8.290133724621107], time: 30.087
Running avgs for agent 0: q_loss: 67.29425048828125, q_loss2: -54.34016418457031, p_loss: -0.6433777809143066, mean_rew: -0.09316688162911423, var_rew: -0.015984466299414635, lamda: 1.6918020248413086
Running avgs for agent 1: q_loss: 42.406227111816406, q_loss2: -35.61187744140625, p_loss: 2.8958866596221924, mean_rew: -0.3960430867812473, var_rew: -0.04785759001970291, lamda: 1.6908036470413208
steps: 749975, episodes: 30000, mean episode reward: -12.527340424921181, agent episode reward: [-4.178925773001655, -8.348414651919528], time: 29.765
Running avgs for agent 0: q_loss: 54.5815544128418, q_loss2: -39.7219123840332, p_loss: -0.6866380572319031, mean_rew: -0.0941076055063671, var_rew: -0.05887194350361824, lamda: 1.7168060541152954
Running avgs for agent 1: q_loss: 37.26372528076172, q_loss2: -31.58110237121582, p_loss: 2.780409097671509, mean_rew: -0.3932102067561547, var_rew: 0.030656974762678146, lamda: 1.7158077955245972
steps: 774975, episodes: 31000, mean episode reward: -12.915107593370179, agent episode reward: [-4.94556708911261, -7.969540504257568], time: 29.379
Running avgs for agent 0: q_loss: 61.69532775878906, q_loss2: -50.903812408447266, p_loss: -0.6889563202857971, mean_rew: -0.0962743860377779, var_rew: -0.023477645590901375, lamda: 1.7418103218078613
Running avgs for agent 1: q_loss: 33.92043685913086, q_loss2: -26.64080238342285, p_loss: 2.662954568862915, mean_rew: -0.39128194093274704, var_rew: 0.0007926998077891767, lamda: 1.740812063217163
steps: 799975, episodes: 32000, mean episode reward: -13.127813437653376, agent episode reward: [-5.007112413906525, -8.12070102374685], time: 30.292
Running avgs for agent 0: q_loss: 92.21279907226562, q_loss2: -65.82505798339844, p_loss: -0.6128228306770325, mean_rew: -0.1011122553261828, var_rew: 0.010416320525109768, lamda: 1.7668144702911377
Running avgs for agent 1: q_loss: 45.7435188293457, q_loss2: -34.690086364746094, p_loss: 2.520623207092285, mean_rew: -0.3888785222548026, var_rew: 0.001862101606093347, lamda: 1.76581609249115
steps: 824975, episodes: 33000, mean episode reward: -13.315986732638233, agent episode reward: [-5.42223967273564, -7.8937470599025925], time: 29.554
Running avgs for agent 0: q_loss: 46.43647766113281, q_loss2: -33.94840621948242, p_loss: -0.6307076215744019, mean_rew: -0.10464602734371192, var_rew: -0.01818857342004776, lamda: 1.7918184995651245
Running avgs for agent 1: q_loss: 33.0964469909668, q_loss2: -26.034347534179688, p_loss: 2.3811795711517334, mean_rew: -0.38799702121404506, var_rew: 0.01804901845753193, lamda: 1.7908203601837158
steps: 849975, episodes: 34000, mean episode reward: -14.04301112208595, agent episode reward: [-5.826672883512225, -8.216338238573725], time: 29.686
Running avgs for agent 0: q_loss: 70.70292663574219, q_loss2: -54.30390167236328, p_loss: -0.6661885976791382, mean_rew: -0.10806515179875928, var_rew: -0.028889473527669907, lamda: 1.8168226480484009
Running avgs for agent 1: q_loss: 38.071922302246094, q_loss2: -29.098352432250977, p_loss: 2.2285964488983154, mean_rew: -0.38421595001938386, var_rew: 0.019284889101982117, lamda: 1.8158245086669922
steps: 874975, episodes: 35000, mean episode reward: -15.382611371518756, agent episode reward: [-7.029217479322682, -8.353393892196072], time: 29.235
Running avgs for agent 0: q_loss: 42.939815521240234, q_loss2: -29.88677978515625, p_loss: -0.6479917168617249, mean_rew: -0.11242988505899376, var_rew: 0.01054291520267725, lamda: 1.8418269157409668
Running avgs for agent 1: q_loss: 33.33859634399414, q_loss2: -24.378644943237305, p_loss: 2.082573652267456, mean_rew: -0.3837164039839222, var_rew: 0.029697589576244354, lamda: 1.840828537940979
steps: 899975, episodes: 36000, mean episode reward: -15.746347945283505, agent episode reward: [-7.3197691213121345, -8.426578823971372], time: 29.304
Running avgs for agent 0: q_loss: 35.76298904418945, q_loss2: -29.285858154296875, p_loss: -0.6015881896018982, mean_rew: -0.11808422964691608, var_rew: 0.02240302786231041, lamda: 1.8668309450149536
Running avgs for agent 1: q_loss: 35.9262580871582, q_loss2: -26.806488037109375, p_loss: 1.957573413848877, mean_rew: -0.3825645448156749, var_rew: 0.011895780451595783, lamda: 1.8658326864242554
steps: 924975, episodes: 37000, mean episode reward: -16.64192264253707, agent episode reward: [-8.141250189489552, -8.50067245304752], time: 29.564
Running avgs for agent 0: q_loss: 43.59309387207031, q_loss2: -30.07076644897461, p_loss: -0.6205856800079346, mean_rew: -0.12251485323464652, var_rew: -0.02176658622920513, lamda: 1.8918352127075195
Running avgs for agent 1: q_loss: 37.452640533447266, q_loss2: -29.02511215209961, p_loss: 1.8552724123001099, mean_rew: -0.38070595657017253, var_rew: -0.05069797486066818, lamda: 1.8908369541168213
steps: 949975, episodes: 38000, mean episode reward: -16.73315129355171, agent episode reward: [-8.05304916234854, -8.680102131203174], time: 29.672
Running avgs for agent 0: q_loss: 51.76713943481445, q_loss2: -40.14052200317383, p_loss: -0.693466305732727, mean_rew: -0.1266411044587775, var_rew: 0.005898280069231987, lamda: 1.916839361190796
Running avgs for agent 1: q_loss: 35.20965576171875, q_loss2: -27.531105041503906, p_loss: 1.691961646080017, mean_rew: -0.3804988543500709, var_rew: -0.009371759369969368, lamda: 1.915840983390808
steps: 974975, episodes: 39000, mean episode reward: -16.967392816590426, agent episode reward: [-8.468252057735155, -8.499140758855273], time: 29.494
Running avgs for agent 0: q_loss: 50.71498107910156, q_loss2: -36.832977294921875, p_loss: -0.669940173625946, mean_rew: -0.13309491265092566, var_rew: 0.039086274802684784, lamda: 1.9418435096740723
Running avgs for agent 1: q_loss: 33.52643585205078, q_loss2: -24.645828247070312, p_loss: 1.4467483758926392, mean_rew: -0.37843127681130373, var_rew: -0.031250111758708954, lamda: 1.9408451318740845
steps: 999975, episodes: 40000, mean episode reward: -15.024959665472391, agent episode reward: [-6.402432384417472, -8.622527281054918], time: 29.271
Running avgs for agent 0: q_loss: 60.05987548828125, q_loss2: -45.264949798583984, p_loss: -0.6733300685882568, mean_rew: -0.13686362053422943, var_rew: 0.018134335055947304, lamda: 1.966847538948059
Running avgs for agent 1: q_loss: 42.47178649902344, q_loss2: -32.759788513183594, p_loss: 1.121748924255371, mean_rew: -0.37673355861520297, var_rew: -0.0626688227057457, lamda: 1.9658491611480713
steps: 1024975, episodes: 41000, mean episode reward: -14.377930248248548, agent episode reward: [-5.3419066417853855, -9.036023606463162], time: 29.351
Running avgs for agent 0: q_loss: 74.30255126953125, q_loss2: -55.05310821533203, p_loss: -0.6178972125053406, mean_rew: -0.14136864293236712, var_rew: 0.030816486105322838, lamda: 1.9918513298034668
Running avgs for agent 1: q_loss: 27.277605056762695, q_loss2: -20.515092849731445, p_loss: 0.6672013401985168, mean_rew: -0.36627887797266667, var_rew: -0.09655643999576569, lamda: 1.990853190422058
steps: 1049975, episodes: 42000, mean episode reward: -13.560012395861884, agent episode reward: [-4.770197981093239, -8.789814414768644], time: 29.531
Running avgs for agent 0: q_loss: 7.412476539611816, q_loss2: -6.492300033569336, p_loss: -0.5865080952644348, mean_rew: -0.14954391603230596, var_rew: 0.02724345028400421, lamda: 2.016835927963257
Running avgs for agent 1: q_loss: 15.674968719482422, q_loss2: -12.598024368286133, p_loss: 0.10032042860984802, mean_rew: -0.34921764737709166, var_rew: -0.029830966144800186, lamda: 2.015838384628296
steps: 1074975, episodes: 43000, mean episode reward: -12.8982569850322, agent episode reward: [-4.658191193781469, -8.240065791250728], time: 30.035
Running avgs for agent 0: q_loss: 3.246293544769287, q_loss2: -2.6799426078796387, p_loss: -0.576662003993988, mean_rew: -0.14959591075428652, var_rew: 0.02830396592617035, lamda: 2.0418102741241455
Running avgs for agent 1: q_loss: 4.396275520324707, q_loss2: -3.9253475666046143, p_loss: -0.20975945889949799, mean_rew: -0.345984954570546, var_rew: 0.02637954242527485, lamda: 2.0408127307891846
steps: 1099975, episodes: 44000, mean episode reward: -12.674300474459168, agent episode reward: [-4.620961498602779, -8.053338975856388], time: 30.214
Running avgs for agent 0: q_loss: 3.10465669631958, q_loss2: -2.6066172122955322, p_loss: -0.5380645990371704, mean_rew: -0.1510995205601715, var_rew: 0.036596719175577164, lamda: 2.066784620285034
Running avgs for agent 1: q_loss: 5.0143513679504395, q_loss2: -4.460676193237305, p_loss: -0.3553818464279175, mean_rew: -0.34861706874979154, var_rew: 0.0515923947095871, lamda: 2.0657870769500732
steps: 1124975, episodes: 45000, mean episode reward: -12.446457612570054, agent episode reward: [-4.701051611572806, -7.745406000997249], time: 29.405
Running avgs for agent 0: q_loss: 3.492264747619629, q_loss2: -2.949680805206299, p_loss: -0.4944586157798767, mean_rew: -0.15131901796568425, var_rew: 0.02551761083304882, lamda: 2.091758966445923
Running avgs for agent 1: q_loss: 4.6395440101623535, q_loss2: -4.058417320251465, p_loss: -0.40616095066070557, mean_rew: -0.34826730247526905, var_rew: 0.06728668510913849, lamda: 2.090761423110962
steps: 1149975, episodes: 46000, mean episode reward: -12.074214617558722, agent episode reward: [-4.518797530135368, -7.555417087423355], time: 30.075
Running avgs for agent 0: q_loss: 3.3701260089874268, q_loss2: -2.8244335651397705, p_loss: -0.4848559498786926, mean_rew: -0.15448135402995655, var_rew: 0.014120387844741344, lamda: 2.1167333126068115
Running avgs for agent 1: q_loss: 4.981470108032227, q_loss2: -4.3761444091796875, p_loss: -0.4236357808113098, mean_rew: -0.3485007562036843, var_rew: 0.06953897327184677, lamda: 2.1157357692718506
steps: 1174975, episodes: 47000, mean episode reward: -13.146910720095274, agent episode reward: [-5.305466112891565, -7.841444607203711], time: 29.823
Running avgs for agent 0: q_loss: 3.8607139587402344, q_loss2: -3.2523739337921143, p_loss: -0.4746694266796112, mean_rew: -0.15811563412481378, var_rew: 0.015005436725914478, lamda: 2.141707420349121
Running avgs for agent 1: q_loss: 4.23957633972168, q_loss2: -3.7036802768707275, p_loss: -0.3599074184894562, mean_rew: -0.3474388871794534, var_rew: 0.073112852871418, lamda: 2.1407101154327393
^[[Bsteps: 1199975, episodes: 48000, mean episode reward: -13.949619795959004, agent episode reward: [-5.836122687836723, -8.113497108122282], time: 30.034
Running avgs for agent 0: q_loss: 4.148516654968262, q_loss2: -3.5171191692352295, p_loss: -0.5136573314666748, mean_rew: -0.16332957285290253, var_rew: 0.015555895864963531, lamda: 2.166682243347168
Running avgs for agent 1: q_loss: 3.3296895027160645, q_loss2: -2.9196577072143555, p_loss: -0.3092493712902069, mean_rew: -0.3495616463729129, var_rew: 0.06182144954800606, lamda: 2.165684461593628
steps: 1224975, episodes: 49000, mean episode reward: -13.776641153561219, agent episode reward: [-5.826334603118123, -7.950306550443096], time: 30.258
Running avgs for agent 0: q_loss: 4.179136753082275, q_loss2: -3.5822947025299072, p_loss: -0.5885449051856995, mean_rew: -0.1670731562050401, var_rew: -0.0024011987261474133, lamda: 2.1916563510894775
Running avgs for agent 1: q_loss: 3.173776865005493, q_loss2: -2.775916576385498, p_loss: -0.19980072975158691, mean_rew: -0.34920794309218967, var_rew: 0.07221441715955734, lamda: 2.1906588077545166
steps: 1249975, episodes: 50000, mean episode reward: -13.76923883061799, agent episode reward: [-5.226191244103656, -8.543047586514332], time: 30.184
Running avgs for agent 0: q_loss: 3.824329137802124, q_loss2: -3.292386293411255, p_loss: -0.6444303393363953, mean_rew: -0.1699805912556783, var_rew: -0.005912393797188997, lamda: 2.216630697250366
Running avgs for agent 1: q_loss: 3.295635223388672, q_loss2: -2.8912436962127686, p_loss: -0.03909622132778168, mean_rew: -0.34889629284674173, var_rew: 0.037277694791555405, lamda: 2.2156333923339844
steps: 1274975, episodes: 51000, mean episode reward: -14.467849724525278, agent episode reward: [-5.186089544082731, -9.281760180442546], time: 30.15
Running avgs for agent 0: q_loss: 3.9939846992492676, q_loss2: -3.3993430137634277, p_loss: -0.6999189257621765, mean_rew: -0.1752512762801491, var_rew: -0.0013614468043670058, lamda: 2.241605043411255
Running avgs for agent 1: q_loss: 3.7252392768859863, q_loss2: -3.202432632446289, p_loss: 0.09930185973644257, mean_rew: -0.3483081003528546, var_rew: -0.021711982786655426, lamda: 2.240607500076294
steps: 1299975, episodes: 52000, mean episode reward: -13.693330926837795, agent episode reward: [-3.9951377306397218, -9.698193196198073], time: 30.028
Running avgs for agent 0: q_loss: 3.8610663414001465, q_loss2: -3.365948438644409, p_loss: -0.7274054884910583, mean_rew: -0.1781120928663541, var_rew: -0.000897022255230695, lamda: 2.2665793895721436
Running avgs for agent 1: q_loss: 3.3567700386047363, q_loss2: -2.983295440673828, p_loss: 0.11220097541809082, mean_rew: -0.34831851163683986, var_rew: -0.04702656716108322, lamda: 2.2655820846557617
steps: 1324975, episodes: 53000, mean episode reward: -13.063705465340737, agent episode reward: [-2.7999649350417766, -10.26374053029896], time: 30.344
Running avgs for agent 0: q_loss: 3.7847676277160645, q_loss2: -3.2739017009735107, p_loss: -0.7252873778343201, mean_rew: -0.17838345584410464, var_rew: -0.005763573572039604, lamda: 2.2915537357330322
Running avgs for agent 1: q_loss: 3.376321315765381, q_loss2: -3.007357358932495, p_loss: 0.018642675131559372, mean_rew: -0.3483716285078749, var_rew: -0.06472337990999222, lamda: 2.2905561923980713
steps: 1349975, episodes: 54000, mean episode reward: -13.03689471466557, agent episode reward: [-2.1382282401785893, -10.89866647448698], time: 30.278
Running avgs for agent 0: q_loss: 3.7633252143859863, q_loss2: -3.2799668312072754, p_loss: -0.725090503692627, mean_rew: -0.17847894314278567, var_rew: -0.00853728037327528, lamda: 2.316528081893921
Running avgs for agent 1: q_loss: 3.975029706954956, q_loss2: -3.5435142517089844, p_loss: -0.08547523617744446, mean_rew: -0.3512413198987742, var_rew: -0.051092661917209625, lamda: 2.31553053855896
steps: 1374975, episodes: 55000, mean episode reward: -13.201460802854893, agent episode reward: [-0.7423664148237462, -12.459094388031145], time: 29.922
Running avgs for agent 0: q_loss: 3.8656890392303467, q_loss2: -3.3743929862976074, p_loss: -0.7323566675186157, mean_rew: -0.17653697163255463, var_rew: -0.001201788429170847, lamda: 2.3415024280548096
Running avgs for agent 1: q_loss: 4.696512699127197, q_loss2: -4.214826583862305, p_loss: -0.19076703488826752, mean_rew: -0.354408835104235, var_rew: -0.038010239601135254, lamda: 2.3405048847198486
steps: 1399975, episodes: 56000, mean episode reward: -13.97307670853448, agent episode reward: [-0.49297571467039203, -13.480100993864086], time: 30.178
Running avgs for agent 0: q_loss: 4.1197967529296875, q_loss2: -3.5461976528167725, p_loss: -0.7841782569885254, mean_rew: -0.17403624870709394, var_rew: -0.02059820108115673, lamda: 2.366476535797119
Running avgs for agent 1: q_loss: 4.8634161949157715, q_loss2: -4.428330898284912, p_loss: -0.28881415724754333, mean_rew: -0.35957282711482597, var_rew: -0.020824717357754707, lamda: 2.3654792308807373
steps: 1424975, episodes: 57000, mean episode reward: -14.009918319196247, agent episode reward: [0.21569765136455618, -14.225615970560803], time: 30.121
Running avgs for agent 0: q_loss: 4.237088680267334, q_loss2: -3.7141103744506836, p_loss: -0.8316690921783447, mean_rew: -0.17056170090039263, var_rew: -0.005807365756481886, lamda: 2.391451120376587
Running avgs for agent 1: q_loss: 5.384072303771973, q_loss2: -4.677952289581299, p_loss: -0.32315489649772644, mean_rew: -0.36209045176044574, var_rew: -0.01992069184780121, lamda: 2.390453577041626
steps: 1449975, episodes: 58000, mean episode reward: -14.015520867063223, agent episode reward: [0.9832377506144901, -14.998758617677716], time: 30.488
Running avgs for agent 0: q_loss: 4.411768913269043, q_loss2: -3.90104079246521, p_loss: -0.8604028820991516, mean_rew: -0.16756439078334354, var_rew: -0.0016056890599429607, lamda: 2.4164252281188965
Running avgs for agent 1: q_loss: 5.1476149559021, q_loss2: -4.482554912567139, p_loss: -0.2606205940246582, mean_rew: -0.3709337439759656, var_rew: -0.04762835055589676, lamda: 2.4154279232025146
steps: 1474975, episodes: 59000, mean episode reward: -14.617201713010582, agent episode reward: [1.3275657275294752, -15.944767440540057], time: 29.831
Running avgs for agent 0: q_loss: 5.671164035797119, q_loss2: -5.03480863571167, p_loss: -0.9268192648887634, mean_rew: -0.16435878119203454, var_rew: 0.010709496214985847, lamda: 2.4413998126983643
Running avgs for agent 1: q_loss: 4.863673686981201, q_loss2: -4.259219169616699, p_loss: -0.18360668420791626, mean_rew: -0.37691082617703897, var_rew: -0.08828509598970413, lamda: 2.4404022693634033
steps: 1499975, episodes: 60000, mean episode reward: -13.752312935288426, agent episode reward: [0.49505220558352864, -14.247365140871954], time: 29.795
Running avgs for agent 0: q_loss: 5.595641613006592, q_loss2: -4.970043182373047, p_loss: -0.9783222675323486, mean_rew: -0.1603598230168843, var_rew: -0.0015385294100269675, lamda: 2.466373920440674
Running avgs for agent 1: q_loss: 5.008494853973389, q_loss2: -4.334197044372559, p_loss: -0.10688343644142151, mean_rew: -0.38335181042687644, var_rew: -0.1385505199432373, lamda: 2.465376615524292
Traceback (most recent call last):

