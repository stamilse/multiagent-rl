python train.py --scenario simple_push --num-adversaries 1 --independent-learner True --lr_actor 0.005 --lr_critic 0.01 --lr_lamda 0.0001 --alpha 0.05 --independent-learner True
2018-10-28 00:08:14.497056: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
In MADDPGAgentTrainerIndepLearner
In MADDPGAgentTrainerIndepLearner
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -25.964392467912425, agent episode reward: [1.544093315870503, -27.508485783782927], time: 18.021
/anaconda2/envs/py36/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/anaconda2/envs/py36/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
Running avgs for agent 0: q_loss: nan, q_loss2: nan, p_loss: nan, mean_rew: nan, var_rew: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, q_loss2: nan, p_loss: nan, mean_rew: nan, var_rew: nan, lamda: nan
steps: 49975, episodes: 2000, mean episode reward: -25.271680129923194, agent episode reward: [-8.090022552607014, -17.181657577316184], time: 25.383
Running avgs for agent 0: q_loss: 0.825354278087616, q_loss2: -0.7772278189659119, p_loss: 0.16569767892360687, mean_rew: -0.028269062891661943, var_rew: 0.1595100611448288, lamda: 1.0122520923614502
Running avgs for agent 1: q_loss: 0.3134351968765259, q_loss2: -0.2686934471130371, p_loss: 1.4886209964752197, mean_rew: -1.0386858571720037, var_rew: 0.01465841569006443, lamda: 1.009281039237976
steps: 74975, episodes: 3000, mean episode reward: -17.31898762085627, agent episode reward: [-9.427238118752559, -7.891749502103709], time: 24.856
Running avgs for agent 0: q_loss: 2.8601274490356445, q_loss2: -2.6490447521209717, p_loss: 0.35625550150871277, mean_rew: -0.18064562136486711, var_rew: 0.10327351093292236, lamda: 1.0369561910629272
Running avgs for agent 1: q_loss: 0.6498895883560181, q_loss2: -0.6471952795982361, p_loss: 2.1641221046447754, mean_rew: -0.7837015305120894, var_rew: 0.012760074809193611, lamda: 1.035332441329956
steps: 99975, episodes: 4000, mean episode reward: -15.197760701806743, agent episode reward: [-7.831493577449341, -7.366267124357402], time: 25.001
Running avgs for agent 0: q_loss: 8.285778999328613, q_loss2: -7.3812127113342285, p_loss: 0.4665486514568329, mean_rew: -0.22569742782064547, var_rew: 0.08330991864204407, lamda: 1.061960220336914
Running avgs for agent 1: q_loss: 1.8916584253311157, q_loss2: -1.9030709266662598, p_loss: 2.4269211292266846, mean_rew: -0.6447963421460974, var_rew: -0.01018292922526598, lamda: 1.0664056539535522
steps: 124975, episodes: 5000, mean episode reward: -16.209136865762524, agent episode reward: [-8.285497994587942, -7.92363887117458], time: 24.98
Running avgs for agent 0: q_loss: 15.738322257995605, q_loss2: -13.809345245361328, p_loss: 0.6131477952003479, mean_rew: -0.24876519115086895, var_rew: 0.07356138527393341, lamda: 1.08696448802948
Running avgs for agent 1: q_loss: 4.616195201873779, q_loss2: -4.550775051116943, p_loss: 2.4040470123291016, mean_rew: -0.5686330071489129, var_rew: 0.007447768002748489, lamda: 1.0945509672164917
steps: 149975, episodes: 6000, mean episode reward: -16.08317659975699, agent episode reward: [-7.935788538436577, -8.147388061320411], time: 25.133
Running avgs for agent 0: q_loss: 25.989625930786133, q_loss2: -22.10637092590332, p_loss: 0.794927716255188, mean_rew: -0.26154395722274687, var_rew: 0.06589904427528381, lamda: 1.1119686365127563
Running avgs for agent 1: q_loss: 5.901134014129639, q_loss2: -5.761703968048096, p_loss: 2.253848075866699, mean_rew: -0.522922109353083, var_rew: 0.020419932901859283, lamda: 1.1214343309402466
steps: 174975, episodes: 7000, mean episode reward: -15.512620899363908, agent episode reward: [-7.011433143926795, -8.501187755437115], time: 25.424
Running avgs for agent 0: q_loss: 43.31231689453125, q_loss2: -36.996376037597656, p_loss: 0.9863001704216003, mean_rew: -0.26830183300337496, var_rew: 0.02036229707300663, lamda: 1.1369726657867432
Running avgs for agent 1: q_loss: 6.063333034515381, q_loss2: -5.892961502075195, p_loss: 2.139085531234741, mean_rew: -0.49469412203024, var_rew: 0.03823312371969223, lamda: 1.1476786136627197
steps: 199975, episodes: 8000, mean episode reward: -15.450391429165535, agent episode reward: [-6.548131154201482, -8.902260274964052], time: 25.386
Running avgs for agent 0: q_loss: 61.85943603515625, q_loss2: -52.03638458251953, p_loss: 1.1375066041946411, mean_rew: -0.269604217337166, var_rew: 0.011387815698981285, lamda: 1.161976933479309
Running avgs for agent 1: q_loss: 8.679134368896484, q_loss2: -7.987710475921631, p_loss: 2.0277440547943115, mean_rew: -0.4726822432236058, var_rew: 0.037150949239730835, lamda: 1.173546314239502
steps: 224975, episodes: 9000, mean episode reward: -14.904897409628902, agent episode reward: [-6.031039846691427, -8.873857562937475], time: 25.214
Running avgs for agent 0: q_loss: 86.0327377319336, q_loss2: -69.98550415039062, p_loss: 1.206878900527954, mean_rew: -0.26714208785159616, var_rew: 0.019727537408471107, lamda: 1.1869810819625854
Running avgs for agent 1: q_loss: 8.52995777130127, q_loss2: -7.89618444442749, p_loss: 2.023923635482788, mean_rew: -0.4615408852961432, var_rew: 0.0394536517560482, lamda: 1.1991711854934692
steps: 249975, episodes: 10000, mean episode reward: -15.029890997344785, agent episode reward: [-6.684018624137254, -8.34587237320753], time: 25.052
Running avgs for agent 0: q_loss: 104.92804718017578, q_loss2: -84.72124481201172, p_loss: 1.1873698234558105, mean_rew: -0.2649417601392428, var_rew: 0.061240486800670624, lamda: 1.2119851112365723
Running avgs for agent 1: q_loss: 9.956222534179688, q_loss2: -8.937310218811035, p_loss: 1.983163595199585, mean_rew: -0.4478873813794228, var_rew: 0.0419057235121727, lamda: 1.2246309518814087
steps: 274975, episodes: 11000, mean episode reward: -15.810241853270236, agent episode reward: [-7.2598469326872, -8.550394920583033], time: 25.173
Running avgs for agent 0: q_loss: 144.73448181152344, q_loss2: -114.33175659179688, p_loss: 1.144283652305603, mean_rew: -0.2653387980835547, var_rew: 0.04419706389307976, lamda: 1.2369892597198486
Running avgs for agent 1: q_loss: 9.500327110290527, q_loss2: -8.736214637756348, p_loss: 1.91703462600708, mean_rew: -0.4394222612851058, var_rew: -0.00264616496860981, lamda: 1.2499743700027466
steps: 299975, episodes: 12000, mean episode reward: -15.447301223920867, agent episode reward: [-6.910237651353484, -8.537063572567384], time: 25.284
Running avgs for agent 0: q_loss: 185.05230712890625, q_loss2: -150.08602905273438, p_loss: 1.112716555595398, mean_rew: -0.2693589137599859, var_rew: 0.07523953169584274, lamda: 1.2619935274124146
Running avgs for agent 1: q_loss: 8.522574424743652, q_loss2: -7.548402786254883, p_loss: 1.800532341003418, mean_rew: -0.43018923465298453, var_rew: 0.04820539802312851, lamda: 1.2752336263656616
steps: 324975, episodes: 13000, mean episode reward: -16.324796738983444, agent episode reward: [-8.028237443632495, -8.296559295350947], time: 25.219
Running avgs for agent 0: q_loss: 208.7834014892578, q_loss2: -166.3417205810547, p_loss: 1.1295315027236938, mean_rew: -0.27156084334571895, var_rew: 0.03957256302237511, lamda: 1.2869975566864014
Running avgs for agent 1: q_loss: 10.123997688293457, q_loss2: -9.137214660644531, p_loss: 1.7198549509048462, mean_rew: -0.4230520456010506, var_rew: 0.02334933914244175, lamda: 1.3004305362701416
steps: 349975, episodes: 14000, mean episode reward: -16.519368997120775, agent episode reward: [-8.178726849833188, -8.340642147287587], time: 25.259
Running avgs for agent 0: q_loss: 244.7299041748047, q_loss2: -196.091552734375, p_loss: 1.0658808946609497, mean_rew: -0.27315745182297757, var_rew: -0.0265397597104311, lamda: 1.3120017051696777
Running avgs for agent 1: q_loss: 11.369547843933105, q_loss2: -8.96023941040039, p_loss: 1.617361068725586, mean_rew: -0.4152372161897656, var_rew: 0.018847212195396423, lamda: 1.3255813121795654
steps: 374975, episodes: 15000, mean episode reward: -17.3844062422287, agent episode reward: [-8.618326171887354, -8.766080070341342], time: 25.531
Running avgs for agent 0: q_loss: 335.057861328125, q_loss2: -262.18536376953125, p_loss: 1.030389666557312, mean_rew: -0.2801403048368678, var_rew: 0.11638666689395905, lamda: 1.3370059728622437
Running avgs for agent 1: q_loss: 8.358684539794922, q_loss2: -7.089276313781738, p_loss: 1.4821953773498535, mean_rew: -0.4120134969134773, var_rew: 0.017133936285972595, lamda: 1.3506970405578613
steps: 399975, episodes: 16000, mean episode reward: -16.862405808686674, agent episode reward: [-8.01428192550419, -8.848123883182485], time: 25.297
Running avgs for agent 0: q_loss: 427.0306701660156, q_loss2: -338.29248046875, p_loss: 1.0602840185165405, mean_rew: -0.28212044895887833, var_rew: 0.0061098309233784676, lamda: 1.36201012134552
Running avgs for agent 1: q_loss: 7.432615756988525, q_loss2: -5.869548797607422, p_loss: 1.3531748056411743, mean_rew: -0.405539695191947, var_rew: 0.005765971262007952, lamda: 1.3757871389389038
steps: 424975, episodes: 17000, mean episode reward: -17.613159858692125, agent episode reward: [-8.61117687442571, -9.001982984266414], time: 25.27
Running avgs for agent 0: q_loss: 488.1561279296875, q_loss2: -380.7790222167969, p_loss: 1.0388456583023071, mean_rew: -0.28433505468512627, var_rew: -0.005645921919494867, lamda: 1.3870141506195068
Running avgs for agent 1: q_loss: 9.933001518249512, q_loss2: -8.038727760314941, p_loss: 1.252840518951416, mean_rew: -0.4042798901432702, var_rew: -0.006411265581846237, lamda: 1.4008539915084839
steps: 449975, episodes: 18000, mean episode reward: -16.243483239919186, agent episode reward: [-7.22836938715593, -9.015113852763257], time: 25.281
Running avgs for agent 0: q_loss: 576.8201904296875, q_loss2: -444.9493713378906, p_loss: 1.0641117095947266, mean_rew: -0.2876357619035152, var_rew: 0.13845661282539368, lamda: 1.4120182991027832
Running avgs for agent 1: q_loss: 6.825942039489746, q_loss2: -5.811451435089111, p_loss: 1.160300612449646, mean_rew: -0.40225290450520207, var_rew: 0.003409998957067728, lamda: 1.4259105920791626
steps: 474975, episodes: 19000, mean episode reward: -16.31117630344341, agent episode reward: [-7.095707184215738, -9.215469119227672], time: 25.43
Running avgs for agent 0: q_loss: 668.447021484375, q_loss2: -515.22802734375, p_loss: 0.9753121137619019, mean_rew: -0.2873595053039817, var_rew: 0.15004462003707886, lamda: 1.4370224475860596
Running avgs for agent 1: q_loss: 6.979477405548096, q_loss2: -5.873592376708984, p_loss: 1.028052568435669, mean_rew: -0.4006862413916075, var_rew: -0.006659429986029863, lamda: 1.4509460926055908
steps: 499975, episodes: 20000, mean episode reward: -15.696641241513847, agent episode reward: [-6.528104259269434, -9.168536982244412], time: 25.389
Running avgs for agent 0: q_loss: 882.7432861328125, q_loss2: -666.3270263671875, p_loss: 0.9345744848251343, mean_rew: -0.28526095633919313, var_rew: 0.029240772128105164, lamda: 1.462026596069336
Running avgs for agent 1: q_loss: 9.710641860961914, q_loss2: -7.946622848510742, p_loss: 0.8678536415100098, mean_rew: -0.39724482595508265, var_rew: -0.02919028140604496, lamda: 1.4759800434112549
steps: 524975, episodes: 21000, mean episode reward: -16.520608420276833, agent episode reward: [-7.272810502623744, -9.24779791765309], time: 25.193
Running avgs for agent 0: q_loss: 1199.500732421875, q_loss2: -917.15185546875, p_loss: 0.7370534539222717, mean_rew: -0.28459387224917154, var_rew: -0.053180597722530365, lamda: 1.4870308637619019
Running avgs for agent 1: q_loss: 10.624222755432129, q_loss2: -8.84963321685791, p_loss: 0.7942053079605103, mean_rew: -0.395911933984428, var_rew: -0.01554033998399973, lamda: 1.5010141134262085
steps: 549975, episodes: 22000, mean episode reward: -17.326916349095576, agent episode reward: [-7.96677872484404, -9.360137624251538], time: 25.171
Running avgs for agent 0: q_loss: 1530.709228515625, q_loss2: -1251.051025390625, p_loss: 0.6055971384048462, mean_rew: -0.2859214755798508, var_rew: 0.026305878534913063, lamda: 1.5120350122451782
Running avgs for agent 1: q_loss: 6.977481365203857, q_loss2: -6.0979719161987305, p_loss: 0.7496525049209595, mean_rew: -0.39602715475404304, var_rew: -0.03520253300666809, lamda: 1.5260342359542847
steps: 574975, episodes: 23000, mean episode reward: -17.193834981867806, agent episode reward: [-7.968229260966026, -9.22560572090178], time: 25.357
Running avgs for agent 0: q_loss: 1687.4510498046875, q_loss2: -1265.3980712890625, p_loss: 0.5612102150917053, mean_rew: -0.2874052737411877, var_rew: 0.07012202590703964, lamda: 1.537039041519165
Running avgs for agent 1: q_loss: 7.1850666999816895, q_loss2: -5.964865207672119, p_loss: 0.7329621315002441, mean_rew: -0.39525762602067754, var_rew: -0.0157795287668705, lamda: 1.551038384437561
steps: 599975, episodes: 24000, mean episode reward: -19.062116344251947, agent episode reward: [-9.423925527539387, -9.638190816712559], time: 25.365
Running avgs for agent 0: q_loss: 2163.29296875, q_loss2: -1631.6412353515625, p_loss: 0.5773054361343384, mean_rew: -0.29059962347111756, var_rew: 0.009349990636110306, lamda: 1.5620431900024414
Running avgs for agent 1: q_loss: 8.528687477111816, q_loss2: -6.528803825378418, p_loss: 0.7691611051559448, mean_rew: -0.3938497392339131, var_rew: 0.007531317416578531, lamda: 1.5760425329208374
steps: 624975, episodes: 25000, mean episode reward: -18.351336762581653, agent episode reward: [-9.170019595669576, -9.181317166912082], time: 25.023
Running avgs for agent 0: q_loss: 3729.544677734375, q_loss2: -2863.919677734375, p_loss: 0.5718404054641724, mean_rew: -0.2950143317545155, var_rew: -0.033365655690431595, lamda: 1.5870473384857178
Running avgs for agent 1: q_loss: 11.757027626037598, q_loss2: -8.795652389526367, p_loss: 0.8393576741218567, mean_rew: -0.3922930191014543, var_rew: 0.012577606365084648, lamda: 1.6010466814041138
steps: 649975, episodes: 26000, mean episode reward: -18.55049160306945, agent episode reward: [-9.46145844138596, -9.089033161683492], time: 25.041
Running avgs for agent 0: q_loss: 4408.734375, q_loss2: -3417.584716796875, p_loss: 0.4475543200969696, mean_rew: -0.2974964776543306, var_rew: 0.08691192418336868, lamda: 1.6120514869689941
Running avgs for agent 1: q_loss: 7.223608493804932, q_loss2: -5.462057113647461, p_loss: 1.0065780878067017, mean_rew: -0.3905390836891144, var_rew: 0.013104028068482876, lamda: 1.6260508298873901
steps: 674975, episodes: 27000, mean episode reward: -18.94947878482799, agent episode reward: [-9.043729615608989, -9.905749169219], time: 25.534
Running avgs for agent 0: q_loss: 4989.58349609375, q_loss2: -3826.41162109375, p_loss: 0.38492852449417114, mean_rew: -0.30170321705243136, var_rew: -0.050792012363672256, lamda: 1.6370556354522705
Running avgs for agent 1: q_loss: 8.602468490600586, q_loss2: -6.977461814880371, p_loss: 1.2107194662094116, mean_rew: -0.39191225287722886, var_rew: 0.02060207724571228, lamda: 1.6510549783706665
steps: 699975, episodes: 28000, mean episode reward: -17.806609598952175, agent episode reward: [-7.786407712764616, -10.020201886187559], time: 25.623
Running avgs for agent 0: q_loss: 5841.23583984375, q_loss2: -4429.2978515625, p_loss: 0.21140563488006592, mean_rew: -0.30193313765477575, var_rew: -0.0042669144459068775, lamda: 1.6620599031448364
Running avgs for agent 1: q_loss: 8.483181953430176, q_loss2: -7.168035507202148, p_loss: 1.4112772941589355, mean_rew: -0.3921681842128387, var_rew: 0.03726956248283386, lamda: 1.6760591268539429
steps: 724975, episodes: 29000, mean episode reward: -18.234113357065056, agent episode reward: [-8.05694668366206, -10.177166673403], time: 25.13
Running avgs for agent 0: q_loss: 7156.28271484375, q_loss2: -5239.31591796875, p_loss: 0.17728662490844727, mean_rew: -0.3008054258656194, var_rew: -0.25016114115715027, lamda: 1.6870639324188232
Running avgs for agent 1: q_loss: 9.232796669006348, q_loss2: -7.660655975341797, p_loss: 1.5892847776412964, mean_rew: -0.3919636043437933, var_rew: 0.05725393071770668, lamda: 1.7010632753372192
steps: 749975, episodes: 30000, mean episode reward: -19.68292413927203, agent episode reward: [-9.649915453143652, -10.033008686128372], time: 25.061
Running avgs for agent 0: q_loss: 9412.7919921875, q_loss2: -7056.419921875, p_loss: -0.005683078896254301, mean_rew: -0.3038386620026471, var_rew: 0.08880725502967834, lamda: 1.7120680809020996
Running avgs for agent 1: q_loss: 8.321964263916016, q_loss2: -6.700317859649658, p_loss: 1.760071039199829, mean_rew: -0.3916668415938687, var_rew: 0.03778307884931564, lamda: 1.7260674238204956
steps: 774975, episodes: 31000, mean episode reward: -18.3587197990364, agent episode reward: [-8.16389277653681, -10.194827022499592], time: 25.094
Running avgs for agent 0: q_loss: 16748.328125, q_loss2: -13104.4892578125, p_loss: -0.0036338644567877054, mean_rew: -0.3063728681932487, var_rew: -0.0187489315867424, lamda: 1.7370723485946655
Running avgs for agent 1: q_loss: 8.84504508972168, q_loss2: -6.871118068695068, p_loss: 1.904389500617981, mean_rew: -0.3920906320970677, var_rew: 0.044009458273649216, lamda: 1.751071572303772
steps: 799975, episodes: 32000, mean episode reward: -22.115189708906758, agent episode reward: [-12.023886830845962, -10.091302878060793], time: 25.059
Running avgs for agent 0: q_loss: 12785.0537109375, q_loss2: -9317.0146484375, p_loss: -0.03492503613233566, mean_rew: -0.3067008225910241, var_rew: -0.24255485832691193, lamda: 1.7620763778686523
Running avgs for agent 1: q_loss: 7.545004844665527, q_loss2: -6.558642387390137, p_loss: 2.017991781234741, mean_rew: -0.39399290178420837, var_rew: 0.0357029102742672, lamda: 1.7760757207870483
steps: 824975, episodes: 33000, mean episode reward: -25.59347432874524, agent episode reward: [-15.606937195137485, -9.986537133607753], time: 25.164
Running avgs for agent 0: q_loss: 20596.951171875, q_loss2: -15316.1201171875, p_loss: -0.14226430654525757, mean_rew: -0.31607953321300114, var_rew: 0.0317913293838501, lamda: 1.7870805263519287
Running avgs for agent 1: q_loss: 7.19455099105835, q_loss2: -5.343138694763184, p_loss: 2.1151282787323, mean_rew: -0.3931679805312097, var_rew: 0.03674517571926117, lamda: 1.8010798692703247
steps: 849975, episodes: 34000, mean episode reward: -24.156733993444615, agent episode reward: [-14.51396917791699, -9.642764815527622], time: 25.016
Running avgs for agent 0: q_loss: 25421.439453125, q_loss2: -19779.1875, p_loss: 0.036796387284994125, mean_rew: -0.32429375414358425, var_rew: 0.002724194433540106, lamda: 1.8120847940444946
Running avgs for agent 1: q_loss: 10.505562782287598, q_loss2: -7.957877159118652, p_loss: 2.199528455734253, mean_rew: -0.39387223670920696, var_rew: 0.0247659794986248, lamda: 1.826084017753601
steps: 874975, episodes: 35000, mean episode reward: -21.242224744473948, agent episode reward: [-11.781281666869887, -9.460943077604059], time: 25.076
Running avgs for agent 0: q_loss: 28557.8671875, q_loss2: -20973.05859375, p_loss: -0.03536267206072807, mean_rew: -0.33004736431743514, var_rew: -0.19404366612434387, lamda: 1.8370888233184814
Running avgs for agent 1: q_loss: 8.166851043701172, q_loss2: -6.259182453155518, p_loss: 2.2455947399139404, mean_rew: -0.3932575015273532, var_rew: 0.020759986713528633, lamda: 1.8510881662368774
steps: 899975, episodes: 36000, mean episode reward: -20.699163772168234, agent episode reward: [-11.452906860941589, -9.246256911226647], time: 25.027
Running avgs for agent 0: q_loss: 30380.328125, q_loss2: -22163.9609375, p_loss: -0.06778595596551895, mean_rew: -0.33459641058105716, var_rew: -0.24043008685112, lamda: 1.8620929718017578
Running avgs for agent 1: q_loss: 10.802111625671387, q_loss2: -8.665365219116211, p_loss: 2.2744433879852295, mean_rew: -0.39286531774524597, var_rew: 0.023465972393751144, lamda: 1.8760923147201538
steps: 924975, episodes: 37000, mean episode reward: -20.012086993999493, agent episode reward: [-10.901380190319253, -9.110706803680237], time: 25.24
Running avgs for agent 0: q_loss: 35953.8359375, q_loss2: -25740.646484375, p_loss: -0.27483275532722473, mean_rew: -0.33489966949797817, var_rew: -0.3342471122741699, lamda: 1.8870972394943237
Running avgs for agent 1: q_loss: 7.19870138168335, q_loss2: -5.787306308746338, p_loss: 2.309100866317749, mean_rew: -0.3907256898758912, var_rew: 0.036106642335653305, lamda: 1.9010964632034302
steps: 949975, episodes: 38000, mean episode reward: -23.751504434052933, agent episode reward: [-13.988546410406062, -9.762958023646865], time: 25.031
Running avgs for agent 0: q_loss: 42250.12109375, q_loss2: -30097.33984375, p_loss: -0.587742030620575, mean_rew: -0.33925819387136663, var_rew: -0.25691723823547363, lamda: 1.9121012687683105
Running avgs for agent 1: q_loss: 7.255957126617432, q_loss2: -6.188736438751221, p_loss: 2.3347949981689453, mean_rew: -0.39196605168665033, var_rew: 0.04014425352215767, lamda: 1.9261006116867065
steps: 974975, episodes: 39000, mean episode reward: -25.828058823420825, agent episode reward: [-16.621279230318216, -9.206779593102612], time: 25.322
Running avgs for agent 0: q_loss: 72235.2734375, q_loss2: -53887.359375, p_loss: -0.8443377614021301, mean_rew: -0.3509988012863721, var_rew: -0.23250360786914825, lamda: 1.937105417251587
Running avgs for agent 1: q_loss: 14.052290916442871, q_loss2: -11.3074312210083, p_loss: 2.3336825370788574, mean_rew: -0.39166219545722775, var_rew: 0.04220210760831833, lamda: 1.951104760169983
steps: 999975, episodes: 40000, mean episode reward: -22.87950079153804, agent episode reward: [-13.58306117089754, -9.296439620640497], time: 25.354
Running avgs for agent 0: q_loss: 101659.5625, q_loss2: -76819.1328125, p_loss: -0.8113481402397156, mean_rew: -0.3559808694709758, var_rew: 0.37142497301101685, lamda: 1.9621095657348633
Running avgs for agent 1: q_loss: 8.698814392089844, q_loss2: -6.947269916534424, p_loss: 2.3300135135650635, mean_rew: -0.391765055577773, var_rew: 0.046971265226602554, lamda: 1.9761089086532593
steps: 1024975, episodes: 41000, mean episode reward: -22.448877776412978, agent episode reward: [-13.49017375019928, -8.958704026213699], time: 25.145
Running avgs for agent 0: q_loss: 108097.890625, q_loss2: -80023.5390625, p_loss: -0.6978678107261658, mean_rew: -0.365305607371052, var_rew: 0.010033279657363892, lamda: 1.9871137142181396
Running avgs for agent 1: q_loss: 7.456955909729004, q_loss2: -6.192142486572266, p_loss: 2.2981958389282227, mean_rew: -0.3811715309517516, var_rew: 0.044877707958221436, lamda: 2.0011086463928223
steps: 1049975, episodes: 42000, mean episode reward: -23.263661139659636, agent episode reward: [-14.843146277969112, -8.420514861690526], time: 25.471
Running avgs for agent 0: q_loss: 18493.771484375, q_loss2: -9640.98828125, p_loss: -0.6606553792953491, mean_rew: -0.37763062766862476, var_rew: 0.1807839721441269, lamda: 2.012103319168091
Running avgs for agent 1: q_loss: 5.190957069396973, q_loss2: -4.573594570159912, p_loss: 2.2768893241882324, mean_rew: -0.3653854144278284, var_rew: 0.024627692997455597, lamda: 2.0260860919952393
steps: 1074975, episodes: 43000, mean episode reward: -22.138935449826423, agent episode reward: [-14.284938227936806, -7.853997221889618], time: 25.31
Running avgs for agent 0: q_loss: 815.1276245117188, q_loss2: -527.6021728515625, p_loss: -0.4668767750263214, mean_rew: -0.3826603179384684, var_rew: 0.19195014238357544, lamda: 2.0370776653289795
Running avgs for agent 1: q_loss: 2.805429697036743, q_loss2: -2.5406405925750732, p_loss: 2.3067660331726074, mean_rew: -0.3623841155799712, var_rew: 0.04857763648033142, lamda: 2.051060438156128
steps: 1099975, episodes: 44000, mean episode reward: -19.107375673852072, agent episode reward: [-11.49471720632194, -7.612658467530127], time: 25.258
Running avgs for agent 0: q_loss: 317.44256591796875, q_loss2: -209.95265197753906, p_loss: -0.41228383779525757, mean_rew: -0.3873997086563645, var_rew: 0.046476952731609344, lamda: 2.062052011489868
Running avgs for agent 1: q_loss: 3.1726560592651367, q_loss2: -2.9123897552490234, p_loss: 2.3281314373016357, mean_rew: -0.3636360603343974, var_rew: 0.041276466101408005, lamda: 2.0760347843170166
^[[Dsteps: 1124975, episodes: 45000, mean episode reward: -17.97776069994165, agent episode reward: [-10.554690100412705, -7.423070599528948], time: 25.257
Running avgs for agent 0: q_loss: 236.72747802734375, q_loss2: -189.7284393310547, p_loss: -0.362699955701828, mean_rew: -0.3888938144406536, var_rew: 0.08555671572685242, lamda: 2.087026357650757
Running avgs for agent 1: q_loss: 3.274583578109741, q_loss2: -3.0280723571777344, p_loss: 2.3216938972473145, mean_rew: -0.3629910514026483, var_rew: 0.04513109475374222, lamda: 2.1010093688964844
steps: 1149975, episodes: 46000, mean episode reward: -17.569955662916414, agent episode reward: [-10.302422685921577, -7.267532976994838], time: 25.179
Running avgs for agent 0: q_loss: 165.28311157226562, q_loss2: -127.299560546875, p_loss: -0.3258567452430725, mean_rew: -0.3921848614026757, var_rew: 0.04905067756772041, lamda: 2.1120007038116455
Running avgs for agent 1: q_loss: 2.8577651977539062, q_loss2: -2.65999174118042, p_loss: 2.284146547317505, mean_rew: -0.36197076146095863, var_rew: 0.04007761552929878, lamda: 2.125983476638794
steps: 1174975, episodes: 47000, mean episode reward: -18.842005655559923, agent episode reward: [-11.439349021840407, -7.402656633719515], time: 25.412
Running avgs for agent 0: q_loss: 171.74339294433594, q_loss2: -139.98553466796875, p_loss: -0.35715582966804504, mean_rew: -0.39507872622324014, var_rew: 0.0851883664727211, lamda: 2.136975049972534
Running avgs for agent 1: q_loss: 3.412458658218384, q_loss2: -3.1593141555786133, p_loss: 2.234388828277588, mean_rew: -0.36154097910059274, var_rew: 0.03475658595561981, lamda: 2.1509578227996826
steps: 1199975, episodes: 48000, mean episode reward: -19.29274743827196, agent episode reward: [-12.207307798915393, -7.085439639356564], time: 25.199
Running avgs for agent 0: q_loss: 117.54188537597656, q_loss2: -98.08744049072266, p_loss: -0.2990769147872925, mean_rew: -0.3989921996889858, var_rew: 0.07033637911081314, lamda: 2.161949396133423
Running avgs for agent 1: q_loss: 3.4221765995025635, q_loss2: -3.140719175338745, p_loss: 2.1866581439971924, mean_rew: -0.360294726740506, var_rew: 0.03311612457036972, lamda: 2.1759321689605713
steps: 1224975, episodes: 49000, mean episode reward: -20.164965125372177, agent episode reward: [-12.917701926504163, -7.247263198868016], time: 24.907
Running avgs for agent 0: q_loss: 141.863037109375, q_loss2: -114.04452514648438, p_loss: -0.22923707962036133, mean_rew: -0.40486327017649065, var_rew: 0.02450922317802906, lamda: 2.1869237422943115
Running avgs for agent 1: q_loss: 3.4529638290405273, q_loss2: -3.1894497871398926, p_loss: 2.136462926864624, mean_rew: -0.35862494897167874, var_rew: 0.03880273550748825, lamda: 2.200906276702881
steps: 1249975, episodes: 50000, mean episode reward: -20.624691770380025, agent episode reward: [-13.479283148465772, -7.145408621914256], time: 24.97
Running avgs for agent 0: q_loss: 106.00089263916016, q_loss2: -88.28755950927734, p_loss: -0.19660452008247375, mean_rew: -0.4133212522862172, var_rew: 0.02302238531410694, lamda: 2.2118980884552
Running avgs for agent 1: q_loss: 3.0778393745422363, q_loss2: -2.8239550590515137, p_loss: 2.077401876449585, mean_rew: -0.3561230341790848, var_rew: 0.04536546394228935, lamda: 2.2258808612823486
steps: 1274975, episodes: 51000, mean episode reward: -20.77771878744536, agent episode reward: [-13.83539781921193, -6.94232096823343], time: 24.906
Running avgs for agent 0: q_loss: 110.20635986328125, q_loss2: -85.81868743896484, p_loss: -0.13702844083309174, mean_rew: -0.4201846772820805, var_rew: 0.06823529303073883, lamda: 2.236872673034668
Running avgs for agent 1: q_loss: 3.0424067974090576, q_loss2: -2.8545374870300293, p_loss: 2.039217472076416, mean_rew: -0.35476337596292273, var_rew: 0.03960224241018295, lamda: 2.2508552074432373
steps: 1299975, episodes: 52000, mean episode reward: -21.062283144621922, agent episode reward: [-14.371962458073117, -6.6903206865488025], time: 24.957
Running avgs for agent 0: q_loss: 95.89691925048828, q_loss2: -74.29129028320312, p_loss: 0.020033003762364388, mean_rew: -0.4251666019180703, var_rew: 0.10784538835287094, lamda: 2.2618467807769775
Running avgs for agent 1: q_loss: 2.96392822265625, q_loss2: -2.6964807510375977, p_loss: 2.023205518722534, mean_rew: -0.3554778330604996, var_rew: 0.0382816307246685, lamda: 2.275829315185547
steps: 1324975, episodes: 53000, mean episode reward: -21.141393033079307, agent episode reward: [-14.334638265399294, -6.806754767680015], time: 25.065
Running avgs for agent 0: q_loss: 94.79728698730469, q_loss2: -77.67709350585938, p_loss: 0.20339742302894592, mean_rew: -0.4340357224738387, var_rew: 0.16880609095096588, lamda: 2.2868213653564453
Running avgs for agent 1: q_loss: 3.430788993835449, q_loss2: -3.1853256225585938, p_loss: 1.992920160293579, mean_rew: -0.35205393365662496, var_rew: 0.03579746186733246, lamda: 2.3008038997650146
steps: 1349975, episodes: 54000, mean episode reward: -22.223400876984563, agent episode reward: [-15.406930474916912, -6.816470402067652], time: 25.004
Running avgs for agent 0: q_loss: 102.33182525634766, q_loss2: -81.49885559082031, p_loss: 0.392585426568985, mean_rew: -0.4403109834372634, var_rew: 0.20406709611415863, lamda: 2.311795711517334
Running avgs for agent 1: q_loss: 3.8479158878326416, q_loss2: -3.534484386444092, p_loss: 1.9808571338653564, mean_rew: -0.3524883253579508, var_rew: 0.035730279982089996, lamda: 2.3257782459259033
steps: 1374975, episodes: 55000, mean episode reward: -23.412388930753252, agent episode reward: [-16.655630104962878, -6.756758825790375], time: 25.017
Running avgs for agent 0: q_loss: 97.4233169555664, q_loss2: -84.0565185546875, p_loss: 0.7353676557540894, mean_rew: -0.4474578417418717, var_rew: 0.18760238587856293, lamda: 2.3367698192596436
Running avgs for agent 1: q_loss: 3.0885229110717773, q_loss2: -2.819830894470215, p_loss: 1.9384541511535645, mean_rew: -0.3480085907804074, var_rew: 0.03910370543599129, lamda: 2.350752353668213
steps: 1399975, episodes: 56000, mean episode reward: -22.179779491527842, agent episode reward: [-15.471467613893036, -6.708311877634805], time: 25.023
Running avgs for agent 0: q_loss: 169.6890869140625, q_loss2: -149.46116638183594, p_loss: 1.1077136993408203, mean_rew: -0.45691695330449705, var_rew: 0.13061852753162384, lamda: 2.3617441654205322
Running avgs for agent 1: q_loss: 3.281848669052124, q_loss2: -3.056062936782837, p_loss: 1.9147043228149414, mean_rew: -0.3466968877249841, var_rew: 0.05263790860772133, lamda: 2.3757269382476807
steps: 1424975, episodes: 57000, mean episode reward: -23.090842743778865, agent episode reward: [-16.286871606811186, -6.803971136967677], time: 25.086
Running avgs for agent 0: q_loss: 121.14114379882812, q_loss2: -98.36690521240234, p_loss: 1.3442944288253784, mean_rew: -0.46515950382700055, var_rew: 0.1619282066822052, lamda: 2.386718511581421
Running avgs for agent 1: q_loss: 4.462380409240723, q_loss2: -4.055942535400391, p_loss: 1.887347936630249, mean_rew: -0.344876588048075, var_rew: 0.04013238847255707, lamda: 2.4007012844085693
steps: 1449975, episodes: 58000, mean episode reward: -22.451456273407015, agent episode reward: [-15.604220042942533, -6.847236230464481], time: 25.326
Running avgs for agent 0: q_loss: 123.28162384033203, q_loss2: -102.56612396240234, p_loss: 1.350091814994812, mean_rew: -0.4692346654328122, var_rew: 0.11305804550647736, lamda: 2.4116928577423096
Running avgs for agent 1: q_loss: 3.8540396690368652, q_loss2: -3.542531728744507, p_loss: 1.8455684185028076, mean_rew: -0.3426082915648498, var_rew: 0.041014060378074646, lamda: 2.425675868988037
steps: 1474975, episodes: 59000, mean episode reward: -21.040192329229374, agent episode reward: [-14.55667733090401, -6.4835149983253615], time: 25.216
Running avgs for agent 0: q_loss: 126.41561126708984, q_loss2: -108.95574951171875, p_loss: 1.2334262132644653, mean_rew: -0.47781332930753784, var_rew: 0.027182886376976967, lamda: 2.4366674423217773
Running avgs for agent 1: q_loss: 4.167294502258301, q_loss2: -3.8406295776367188, p_loss: 1.8107954263687134, mean_rew: -0.3412295453580857, var_rew: 0.04737205430865288, lamda: 2.4506499767303467
steps: 1499975, episodes: 60000, mean episode reward: -19.45745917407464, agent episode reward: [-12.715211709336167, -6.74224746473847], time: 25.117
Running avgs for agent 0: q_loss: 120.00637817382812, q_loss2: -102.6013412475586, p_loss: 1.0921447277069092, mean_rew: -0.4847924488375304, var_rew: -0.03039935603737831, lamda: 2.461641550064087
Running avgs for agent 1: q_loss: 3.3609843254089355, q_loss2: -3.0557754039764404, p_loss: 1.7565175294876099, mean_rew: -0.33651361128003426, var_rew: 0.04883680120110512, lamda: 2.4756243228912354
Traceback (most recent call last):

