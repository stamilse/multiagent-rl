python train.py --scenario simple_push --num-adversaries 5
2018-10-23 10:15:10.104968: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
<tf.Variable 'agent_0/q_func/fully_connected/weights:0' shape=(345, 64) dtype=float32_ref> Tensor("agent_0/gradients/agent_0/q_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(345, 64), dtype=float32)
<tf.Variable 'agent_0/q_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_0/gradients/agent_0/q_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_0/q_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_0/gradients/agent_0/q_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_0/q_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_0/gradients/agent_0/q_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_0/q_func/fully_connected_2/weights:0' shape=(64, 1) dtype=float32_ref> Tensor("agent_0/gradients/agent_0/q_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 1), dtype=float32)
<tf.Variable 'agent_0/q_func/fully_connected_2/biases:0' shape=(1,) dtype=float32_ref> Tensor("agent_0/gradients/agent_0/q_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(1,), dtype=float32)
<tf.Variable 'agent_0/lamda_constraint0:0' shape=(1,) dtype=float32_ref> Tensor("agent_0/gradients_1/agent_0/mul_grad/tuple/control_dependency:0", shape=(1,), dtype=float32)
<tf.Variable 'agent_0/p_func/fully_connected/weights:0' shape=(24, 64) dtype=float32_ref> Tensor("agent_0_1/gradients/agent_0_1/p_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(24, 64), dtype=float32)
<tf.Variable 'agent_0/p_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_0_1/gradients/agent_0_1/p_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_0/p_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_0_1/gradients/agent_0_1/p_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_0/p_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_0_1/gradients/agent_0_1/p_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_0/p_func/fully_connected_2/weights:0' shape=(64, 5) dtype=float32_ref> Tensor("agent_0_1/gradients/agent_0_1/p_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 5), dtype=float32)
<tf.Variable 'agent_0/p_func/fully_connected_2/biases:0' shape=(5,) dtype=float32_ref> Tensor("agent_0_1/gradients/agent_0_1/p_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(5,), dtype=float32)
<tf.Variable 'agent_1/q_func/fully_connected/weights:0' shape=(345, 64) dtype=float32_ref> Tensor("agent_1/gradients/agent_1/q_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(345, 64), dtype=float32)
<tf.Variable 'agent_1/q_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_1/gradients/agent_1/q_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_1/q_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_1/gradients/agent_1/q_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_1/q_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_1/gradients/agent_1/q_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_1/q_func/fully_connected_2/weights:0' shape=(64, 1) dtype=float32_ref> Tensor("agent_1/gradients/agent_1/q_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 1), dtype=float32)
<tf.Variable 'agent_1/q_func/fully_connected_2/biases:0' shape=(1,) dtype=float32_ref> Tensor("agent_1/gradients/agent_1/q_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(1,), dtype=float32)
<tf.Variable 'agent_1/lamda_constraint1:0' shape=(1,) dtype=float32_ref> Tensor("agent_1/gradients_1/agent_1/mul_grad/tuple/control_dependency:0", shape=(1,), dtype=float32)
<tf.Variable 'agent_1/p_func/fully_connected/weights:0' shape=(24, 64) dtype=float32_ref> Tensor("agent_1_1/gradients/agent_1_1/p_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(24, 64), dtype=float32)
<tf.Variable 'agent_1/p_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_1_1/gradients/agent_1_1/p_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_1/p_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_1_1/gradients/agent_1_1/p_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_1/p_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_1_1/gradients/agent_1_1/p_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_1/p_func/fully_connected_2/weights:0' shape=(64, 5) dtype=float32_ref> Tensor("agent_1_1/gradients/agent_1_1/p_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 5), dtype=float32)
<tf.Variable 'agent_1/p_func/fully_connected_2/biases:0' shape=(5,) dtype=float32_ref> Tensor("agent_1_1/gradients/agent_1_1/p_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(5,), dtype=float32)
<tf.Variable 'agent_2/q_func/fully_connected/weights:0' shape=(345, 64) dtype=float32_ref> Tensor("agent_2/gradients/agent_2/q_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(345, 64), dtype=float32)
<tf.Variable 'agent_2/q_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_2/gradients/agent_2/q_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_2/q_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_2/gradients/agent_2/q_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_2/q_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_2/gradients/agent_2/q_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_2/q_func/fully_connected_2/weights:0' shape=(64, 1) dtype=float32_ref> Tensor("agent_2/gradients/agent_2/q_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 1), dtype=float32)
<tf.Variable 'agent_2/q_func/fully_connected_2/biases:0' shape=(1,) dtype=float32_ref> Tensor("agent_2/gradients/agent_2/q_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(1,), dtype=float32)
<tf.Variable 'agent_2/lamda_constraint2:0' shape=(1,) dtype=float32_ref> Tensor("agent_2/gradients_1/agent_2/mul_grad/tuple/control_dependency:0", shape=(1,), dtype=float32)
<tf.Variable 'agent_2/p_func/fully_connected/weights:0' shape=(24, 64) dtype=float32_ref> Tensor("agent_2_1/gradients/agent_2_1/p_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(24, 64), dtype=float32)
<tf.Variable 'agent_2/p_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_2_1/gradients/agent_2_1/p_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_2/p_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_2_1/gradients/agent_2_1/p_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_2/p_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_2_1/gradients/agent_2_1/p_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_2/p_func/fully_connected_2/weights:0' shape=(64, 5) dtype=float32_ref> Tensor("agent_2_1/gradients/agent_2_1/p_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 5), dtype=float32)
<tf.Variable 'agent_2/p_func/fully_connected_2/biases:0' shape=(5,) dtype=float32_ref> Tensor("agent_2_1/gradients/agent_2_1/p_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(5,), dtype=float32)
<tf.Variable 'agent_3/q_func/fully_connected/weights:0' shape=(345, 64) dtype=float32_ref> Tensor("agent_3/gradients/agent_3/q_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(345, 64), dtype=float32)
<tf.Variable 'agent_3/q_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_3/gradients/agent_3/q_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_3/q_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_3/gradients/agent_3/q_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_3/q_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_3/gradients/agent_3/q_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_3/q_func/fully_connected_2/weights:0' shape=(64, 1) dtype=float32_ref> Tensor("agent_3/gradients/agent_3/q_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 1), dtype=float32)
<tf.Variable 'agent_3/q_func/fully_connected_2/biases:0' shape=(1,) dtype=float32_ref> Tensor("agent_3/gradients/agent_3/q_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(1,), dtype=float32)
<tf.Variable 'agent_3/lamda_constraint3:0' shape=(1,) dtype=float32_ref> Tensor("agent_3/gradients_1/agent_3/mul_grad/tuple/control_dependency:0", shape=(1,), dtype=float32)
<tf.Variable 'agent_3/p_func/fully_connected/weights:0' shape=(24, 64) dtype=float32_ref> Tensor("agent_3_1/gradients/agent_3_1/p_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(24, 64), dtype=float32)
<tf.Variable 'agent_3/p_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_3_1/gradients/agent_3_1/p_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_3/p_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_3_1/gradients/agent_3_1/p_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_3/p_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_3_1/gradients/agent_3_1/p_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_3/p_func/fully_connected_2/weights:0' shape=(64, 5) dtype=float32_ref> Tensor("agent_3_1/gradients/agent_3_1/p_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 5), dtype=float32)
<tf.Variable 'agent_3/p_func/fully_connected_2/biases:0' shape=(5,) dtype=float32_ref> Tensor("agent_3_1/gradients/agent_3_1/p_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(5,), dtype=float32)
<tf.Variable 'agent_4/q_func/fully_connected/weights:0' shape=(345, 64) dtype=float32_ref> Tensor("agent_4/gradients/agent_4/q_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(345, 64), dtype=float32)
<tf.Variable 'agent_4/q_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_4/gradients/agent_4/q_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_4/q_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_4/gradients/agent_4/q_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_4/q_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_4/gradients/agent_4/q_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_4/q_func/fully_connected_2/weights:0' shape=(64, 1) dtype=float32_ref> Tensor("agent_4/gradients/agent_4/q_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 1), dtype=float32)
<tf.Variable 'agent_4/q_func/fully_connected_2/biases:0' shape=(1,) dtype=float32_ref> Tensor("agent_4/gradients/agent_4/q_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(1,), dtype=float32)
<tf.Variable 'agent_4/lamda_constraint4:0' shape=(1,) dtype=float32_ref> Tensor("agent_4/gradients_1/agent_4/mul_grad/tuple/control_dependency:0", shape=(1,), dtype=float32)
<tf.Variable 'agent_4/p_func/fully_connected/weights:0' shape=(24, 64) dtype=float32_ref> Tensor("agent_4_1/gradients/agent_4_1/p_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(24, 64), dtype=float32)
<tf.Variable 'agent_4/p_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_4_1/gradients/agent_4_1/p_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_4/p_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_4_1/gradients/agent_4_1/p_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_4/p_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_4_1/gradients/agent_4_1/p_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_4/p_func/fully_connected_2/weights:0' shape=(64, 5) dtype=float32_ref> Tensor("agent_4_1/gradients/agent_4_1/p_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 5), dtype=float32)
<tf.Variable 'agent_4/p_func/fully_connected_2/biases:0' shape=(5,) dtype=float32_ref> Tensor("agent_4_1/gradients/agent_4_1/p_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(5,), dtype=float32)
<tf.Variable 'agent_5/q_func/fully_connected/weights:0' shape=(345, 64) dtype=float32_ref> Tensor("agent_5/gradients/agent_5/q_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(345, 64), dtype=float32)
<tf.Variable 'agent_5/q_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_5/gradients/agent_5/q_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_5/q_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_5/gradients/agent_5/q_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_5/q_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_5/gradients/agent_5/q_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_5/q_func/fully_connected_2/weights:0' shape=(64, 1) dtype=float32_ref> Tensor("agent_5/gradients/agent_5/q_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 1), dtype=float32)
<tf.Variable 'agent_5/q_func/fully_connected_2/biases:0' shape=(1,) dtype=float32_ref> Tensor("agent_5/gradients/agent_5/q_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(1,), dtype=float32)
<tf.Variable 'agent_5/lamda_constraint5:0' shape=(1,) dtype=float32_ref> Tensor("agent_5/gradients_1/agent_5/mul_grad/tuple/control_dependency:0", shape=(1,), dtype=float32)
<tf.Variable 'agent_5/p_func/fully_connected/weights:0' shape=(35, 64) dtype=float32_ref> Tensor("agent_5_1/gradients/agent_5_1/p_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(35, 64), dtype=float32)
<tf.Variable 'agent_5/p_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_5_1/gradients/agent_5_1/p_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_5/p_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_5_1/gradients/agent_5_1/p_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_5/p_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_5_1/gradients/agent_5_1/p_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_5/p_func/fully_connected_2/weights:0' shape=(64, 5) dtype=float32_ref> Tensor("agent_5_1/gradients/agent_5_1/p_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 5), dtype=float32)
<tf.Variable 'agent_5/p_func/fully_connected_2/biases:0' shape=(5,) dtype=float32_ref> Tensor("agent_5_1/gradients/agent_5_1/p_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(5,), dtype=float32)
<tf.Variable 'agent_6/q_func/fully_connected/weights:0' shape=(345, 64) dtype=float32_ref> Tensor("agent_6/gradients/agent_6/q_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(345, 64), dtype=float32)
<tf.Variable 'agent_6/q_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_6/gradients/agent_6/q_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_6/q_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_6/gradients/agent_6/q_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_6/q_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_6/gradients/agent_6/q_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_6/q_func/fully_connected_2/weights:0' shape=(64, 1) dtype=float32_ref> Tensor("agent_6/gradients/agent_6/q_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 1), dtype=float32)
<tf.Variable 'agent_6/q_func/fully_connected_2/biases:0' shape=(1,) dtype=float32_ref> Tensor("agent_6/gradients/agent_6/q_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(1,), dtype=float32)
<tf.Variable 'agent_6/lamda_constraint6:0' shape=(1,) dtype=float32_ref> Tensor("agent_6/gradients_1/agent_6/mul_grad/tuple/control_dependency:0", shape=(1,), dtype=float32)
<tf.Variable 'agent_6/p_func/fully_connected/weights:0' shape=(35, 64) dtype=float32_ref> Tensor("agent_6_1/gradients/agent_6_1/p_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(35, 64), dtype=float32)
<tf.Variable 'agent_6/p_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_6_1/gradients/agent_6_1/p_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_6/p_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_6_1/gradients/agent_6_1/p_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_6/p_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_6_1/gradients/agent_6_1/p_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_6/p_func/fully_connected_2/weights:0' shape=(64, 5) dtype=float32_ref> Tensor("agent_6_1/gradients/agent_6_1/p_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 5), dtype=float32)
<tf.Variable 'agent_6/p_func/fully_connected_2/biases:0' shape=(5,) dtype=float32_ref> Tensor("agent_6_1/gradients/agent_6_1/p_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(5,), dtype=float32)
<tf.Variable 'agent_7/q_func/fully_connected/weights:0' shape=(345, 64) dtype=float32_ref> Tensor("agent_7/gradients/agent_7/q_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(345, 64), dtype=float32)
<tf.Variable 'agent_7/q_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_7/gradients/agent_7/q_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_7/q_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_7/gradients/agent_7/q_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_7/q_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_7/gradients/agent_7/q_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_7/q_func/fully_connected_2/weights:0' shape=(64, 1) dtype=float32_ref> Tensor("agent_7/gradients/agent_7/q_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 1), dtype=float32)
<tf.Variable 'agent_7/q_func/fully_connected_2/biases:0' shape=(1,) dtype=float32_ref> Tensor("agent_7/gradients/agent_7/q_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(1,), dtype=float32)
<tf.Variable 'agent_7/lamda_constraint7:0' shape=(1,) dtype=float32_ref> Tensor("agent_7/gradients_1/agent_7/mul_grad/tuple/control_dependency:0", shape=(1,), dtype=float32)
<tf.Variable 'agent_7/p_func/fully_connected/weights:0' shape=(35, 64) dtype=float32_ref> Tensor("agent_7_1/gradients/agent_7_1/p_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(35, 64), dtype=float32)
<tf.Variable 'agent_7/p_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_7_1/gradients/agent_7_1/p_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_7/p_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_7_1/gradients/agent_7_1/p_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_7/p_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_7_1/gradients/agent_7_1/p_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_7/p_func/fully_connected_2/weights:0' shape=(64, 5) dtype=float32_ref> Tensor("agent_7_1/gradients/agent_7_1/p_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 5), dtype=float32)
<tf.Variable 'agent_7/p_func/fully_connected_2/biases:0' shape=(5,) dtype=float32_ref> Tensor("agent_7_1/gradients/agent_7_1/p_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(5,), dtype=float32)
<tf.Variable 'agent_8/q_func/fully_connected/weights:0' shape=(345, 64) dtype=float32_ref> Tensor("agent_8/gradients/agent_8/q_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(345, 64), dtype=float32)
<tf.Variable 'agent_8/q_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_8/gradients/agent_8/q_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_8/q_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_8/gradients/agent_8/q_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_8/q_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_8/gradients/agent_8/q_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_8/q_func/fully_connected_2/weights:0' shape=(64, 1) dtype=float32_ref> Tensor("agent_8/gradients/agent_8/q_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 1), dtype=float32)
<tf.Variable 'agent_8/q_func/fully_connected_2/biases:0' shape=(1,) dtype=float32_ref> Tensor("agent_8/gradients/agent_8/q_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(1,), dtype=float32)
<tf.Variable 'agent_8/lamda_constraint8:0' shape=(1,) dtype=float32_ref> Tensor("agent_8/gradients_1/agent_8/mul_grad/tuple/control_dependency:0", shape=(1,), dtype=float32)
<tf.Variable 'agent_8/p_func/fully_connected/weights:0' shape=(35, 64) dtype=float32_ref> Tensor("agent_8_1/gradients/agent_8_1/p_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(35, 64), dtype=float32)
<tf.Variable 'agent_8/p_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_8_1/gradients/agent_8_1/p_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_8/p_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_8_1/gradients/agent_8_1/p_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_8/p_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_8_1/gradients/agent_8_1/p_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_8/p_func/fully_connected_2/weights:0' shape=(64, 5) dtype=float32_ref> Tensor("agent_8_1/gradients/agent_8_1/p_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 5), dtype=float32)
<tf.Variable 'agent_8/p_func/fully_connected_2/biases:0' shape=(5,) dtype=float32_ref> Tensor("agent_8_1/gradients/agent_8_1/p_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(5,), dtype=float32)
<tf.Variable 'agent_9/q_func/fully_connected/weights:0' shape=(345, 64) dtype=float32_ref> Tensor("agent_9/gradients/agent_9/q_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(345, 64), dtype=float32)
<tf.Variable 'agent_9/q_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_9/gradients/agent_9/q_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_9/q_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_9/gradients/agent_9/q_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_9/q_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_9/gradients/agent_9/q_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_9/q_func/fully_connected_2/weights:0' shape=(64, 1) dtype=float32_ref> Tensor("agent_9/gradients/agent_9/q_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 1), dtype=float32)
<tf.Variable 'agent_9/q_func/fully_connected_2/biases:0' shape=(1,) dtype=float32_ref> Tensor("agent_9/gradients/agent_9/q_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(1,), dtype=float32)
<tf.Variable 'agent_9/lamda_constraint9:0' shape=(1,) dtype=float32_ref> Tensor("agent_9/gradients_1/agent_9/mul_grad/tuple/control_dependency:0", shape=(1,), dtype=float32)
<tf.Variable 'agent_9/p_func/fully_connected/weights:0' shape=(35, 64) dtype=float32_ref> Tensor("agent_9_1/gradients/agent_9_1/p_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(35, 64), dtype=float32)
<tf.Variable 'agent_9/p_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_9_1/gradients/agent_9_1/p_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_9/p_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_9_1/gradients/agent_9_1/p_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_9/p_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_9_1/gradients/agent_9_1/p_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_9/p_func/fully_connected_2/weights:0' shape=(64, 5) dtype=float32_ref> Tensor("agent_9_1/gradients/agent_9_1/p_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 5), dtype=float32)
<tf.Variable 'agent_9/p_func/fully_connected_2/biases:0' shape=(5,) dtype=float32_ref> Tensor("agent_9_1/gradients/agent_9_1/p_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(5,), dtype=float32)
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -205.97298988683877, agent episode reward: [-12.830430149868848, -14.184148154941544, -13.198508709508584, -14.75410642864016, -13.034753501853618, -27.251932605845376, -27.471337858282077, -29.628301443042947, -26.057135235892343, -27.562335798963268], time: 117.332
/anaconda2/envs/py36/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/anaconda2/envs/py36/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
Running avgs for agent 0: q_loss: nan, q_loss2: nan, p_loss: nan, mean_rew: nan, var_rew: nan
Running avgs for agent 1: q_loss: nan, q_loss2: nan, p_loss: nan, mean_rew: nan, var_rew: nan
Running avgs for agent 2: q_loss: nan, q_loss2: nan, p_loss: nan, mean_rew: nan, var_rew: nan
Running avgs for agent 3: q_loss: nan, q_loss2: nan, p_loss: nan, mean_rew: nan, var_rew: nan
Running avgs for agent 4: q_loss: nan, q_loss2: nan, p_loss: nan, mean_rew: nan, var_rew: nan
Running avgs for agent 5: q_loss: nan, q_loss2: nan, p_loss: nan, mean_rew: nan, var_rew: nan
Running avgs for agent 6: q_loss: nan, q_loss2: nan, p_loss: nan, mean_rew: nan, var_rew: nan
Running avgs for agent 7: q_loss: nan, q_loss2: nan, p_loss: nan, mean_rew: nan, var_rew: nan
Running avgs for agent 8: q_loss: nan, q_loss2: nan, p_loss: nan, mean_rew: nan, var_rew: nan
Running avgs for agent 9: q_loss: nan, q_loss2: nan, p_loss: nan, mean_rew: nan, var_rew: nan
steps: 49975, episodes: 2000, mean episode reward: -147.58935919682105, agent episode reward: [-16.049027588988054, -12.430066877722812, -13.54053353540094, -13.526022955555927, -13.437297570015684, -15.189490775487725, -17.1846371070651, -15.332785726633434, -13.853335757391957, -17.046161302559423], time: 307.556
Running avgs for agent 0: q_loss: 0.8037633895874023, q_loss2: -0.7924553751945496, p_loss: 1.0555564165115356, mean_rew: -0.6056935519047769, var_rew: 0.4897179858938237
Running avgs for agent 1: q_loss: 0.5809636116027832, q_loss2: -0.571171760559082, p_loss: 0.8550761342048645, mean_rew: -0.56925405177942, var_rew: 0.34967753303530186
Running avgs for agent 2: q_loss: 0.6367203593254089, q_loss2: -0.6282773613929749, p_loss: 0.9552852511405945, mean_rew: -0.5628701648544278, var_rew: 0.39122726322949386
Running avgs for agent 3: q_loss: 0.6458581686019897, q_loss2: -0.6357482671737671, p_loss: 1.0798536539077759, mean_rew: -0.6101438471061291, var_rew: 0.3907854852890783
Running avgs for agent 4: q_loss: 0.6710792779922485, q_loss2: -0.6641845703125, p_loss: 0.8586564660072327, mean_rew: -0.5601580399226292, var_rew: 0.40383048454835396
Running avgs for agent 5: q_loss: 0.5313474535942078, q_loss2: -0.5258883237838745, p_loss: 1.3926751613616943, mean_rew: -1.0089081002286722, var_rew: 0.33704852997782075
Running avgs for agent 6: q_loss: 0.49995169043540955, q_loss2: -0.49136850237846375, p_loss: 1.300214409828186, mean_rew: -1.0293414478316225, var_rew: 0.3001857972830101
Running avgs for agent 7: q_loss: 0.6297060251235962, q_loss2: -0.612263560295105, p_loss: 1.441562294960022, mean_rew: -1.0724598297448678, var_rew: 0.40408135664808875
Running avgs for agent 8: q_loss: 0.5222817063331604, q_loss2: -0.517676591873169, p_loss: 1.4596816301345825, mean_rew: -0.9492489673545406, var_rew: 0.3178765426176039
Running avgs for agent 9: q_loss: 0.7250186204910278, q_loss2: -0.695896327495575, p_loss: 1.7025686502456665, mean_rew: -1.0596828223236534, var_rew: 0.4426726845530331
steps: 74975, episodes: 3000, mean episode reward: -77.82568422985985, agent episode reward: [-8.009225435812185, -7.775261975725327, -7.76507108824513, -7.829755003474704, -7.859786171638012, -7.849792705138714, -7.778599085448849, -7.764828055025214, -7.525470610903101, -7.667894098448602], time: 297.108
Running avgs for agent 0: q_loss: 0.9610943794250488, q_loss2: -0.9543079733848572, p_loss: 1.5945552587509155, mean_rew: -0.5325652773774429, var_rew: 0.3812277504061492
Running avgs for agent 1: q_loss: 0.6158271431922913, q_loss2: -0.6089646220207214, p_loss: 1.3646464347839355, mean_rew: -0.49183455231625284, var_rew: 0.2672439954043442
Running avgs for agent 2: q_loss: 0.7493565678596497, q_loss2: -0.7424510717391968, p_loss: 1.476786494255066, mean_rew: -0.4938575756959822, var_rew: 0.3079257305270919
Running avgs for agent 3: q_loss: 0.7042937874794006, q_loss2: -0.6983836889266968, p_loss: 1.536576747894287, mean_rew: -0.5199629843030179, var_rew: 0.3004054075041527
Running avgs for agent 4: q_loss: 0.6996462345123291, q_loss2: -0.6936340928077698, p_loss: 1.404491662979126, mean_rew: -0.49052781014213154, var_rew: 0.29940588285348396
Running avgs for agent 5: q_loss: 0.9817997217178345, q_loss2: -0.9814950227737427, p_loss: 2.0697529315948486, mean_rew: -0.7459085554300013, var_rew: 0.3768231090797474
Running avgs for agent 6: q_loss: 0.8998545408248901, q_loss2: -0.8997129201889038, p_loss: 2.040050983428955, mean_rew: -0.7837383843037703, var_rew: 0.3521393951459162
Running avgs for agent 7: q_loss: 1.1375205516815186, q_loss2: -1.1373521089553833, p_loss: 2.1081626415252686, mean_rew: -0.7888141500184143, var_rew: 0.43683169880834066
Running avgs for agent 8: q_loss: 0.864996075630188, q_loss2: -0.864697277545929, p_loss: 2.0278797149658203, mean_rew: -0.7057469565067117, var_rew: 0.34190749225749384
Running avgs for agent 9: q_loss: 1.2577825784683228, q_loss2: -1.257663607597351, p_loss: 2.287506103515625, mean_rew: -0.7827522002980061, var_rew: 0.47173328173907536
steps: 99975, episodes: 4000, mean episode reward: -73.48511123905121, agent episode reward: [-7.021632840443643, -7.139207259364483, -7.099219220456988, -6.969322379333355, -7.146530033600126, -7.75803093028941, -7.668541659997786, -7.57413065171465, -7.603388954189547, -7.5051073096612235], time: 292.773
Running avgs for agent 0: q_loss: 1.055068016052246, q_loss2: -1.0453044176101685, p_loss: 1.9489619731903076, mean_rew: -0.4631770702804911, var_rew: 0.31449710414663984
Running avgs for agent 1: q_loss: 0.7311264276504517, q_loss2: -0.7212287783622742, p_loss: 1.7560783624649048, mean_rew: -0.43727920454482705, var_rew: 0.23447393325198143
Running avgs for agent 2: q_loss: 0.8325275778770447, q_loss2: -0.823323130607605, p_loss: 1.833522915840149, mean_rew: -0.43686044160776205, var_rew: 0.25816957859845574
Running avgs for agent 3: q_loss: 0.8235124349594116, q_loss2: -0.81361323595047, p_loss: 1.8940131664276123, mean_rew: -0.4537410620005411, var_rew: 0.2584603426602562
Running avgs for agent 4: q_loss: 0.7966148257255554, q_loss2: -0.786858081817627, p_loss: 1.79715895652771, mean_rew: -0.4362095114618966, var_rew: 0.25498199807098276
Running avgs for agent 5: q_loss: 1.3103076219558716, q_loss2: -1.3095589876174927, p_loss: 2.4294018745422363, mean_rew: -0.6230235112941702, var_rew: 0.3547364622930523
Running avgs for agent 6: q_loss: 1.2605504989624023, q_loss2: -1.2599613666534424, p_loss: 2.3987205028533936, mean_rew: -0.6432070616063656, var_rew: 0.341244883891832
Running avgs for agent 7: q_loss: 1.4893351793289185, q_loss2: -1.4884432554244995, p_loss: 2.4927480220794678, mean_rew: -0.6505469736411115, var_rew: 0.40395511114862903
Running avgs for agent 8: q_loss: 1.158962607383728, q_loss2: -1.1578118801116943, p_loss: 2.3263611793518066, mean_rew: -0.5902475610959063, var_rew: 0.32167276110338416
Running avgs for agent 9: q_loss: 1.5863066911697388, q_loss2: -1.5856670141220093, p_loss: 2.628899335861206, mean_rew: -0.6436530014125069, var_rew: 0.42564435011375623
steps: 124975, episodes: 5000, mean episode reward: -73.35026760339673, agent episode reward: [-7.117039059807817, -6.967732006576163, -6.830862475500536, -7.043780235930688, -6.845547762363358, -7.784677146526751, -7.546460067662022, -7.759379775748668, -7.803490215434964, -7.65129885784576], time: 295.895
Running avgs for agent 0: q_loss: 1.1406487226486206, q_loss2: -1.1280962228775024, p_loss: 2.154146909713745, mean_rew: -0.42284221389051474, var_rew: 0.2785134362527809
Running avgs for agent 1: q_loss: 0.8203983306884766, q_loss2: -0.8076677322387695, p_loss: 1.959700584411621, mean_rew: -0.4028202059054116, var_rew: 0.21331211101199238
Running avgs for agent 2: q_loss: 0.9159760475158691, q_loss2: -0.9050562977790833, p_loss: 2.013807535171509, mean_rew: -0.4009790374494165, var_rew: 0.23244791309927842
Running avgs for agent 3: q_loss: 0.9149165153503418, q_loss2: -0.9037954211235046, p_loss: 2.083920478820801, mean_rew: -0.4156925006371057, var_rew: 0.234549770011484
Running avgs for agent 4: q_loss: 0.8843768835067749, q_loss2: -0.8744879364967346, p_loss: 2.006631851196289, mean_rew: -0.40224499515597456, var_rew: 0.23213875504503012
Running avgs for agent 5: q_loss: 1.5129233598709106, q_loss2: -1.5114458799362183, p_loss: 2.5618605613708496, mean_rew: -0.5522428551747044, var_rew: 0.32477933444655
Running avgs for agent 6: q_loss: 1.5064703226089478, q_loss2: -1.5050760507583618, p_loss: 2.5379390716552734, mean_rew: -0.5696465783189806, var_rew: 0.3201418140206057
Running avgs for agent 7: q_loss: 1.7117094993591309, q_loss2: -1.710385799407959, p_loss: 2.6423661708831787, mean_rew: -0.5729178880801653, var_rew: 0.3684588367842122
Running avgs for agent 8: q_loss: 1.344468593597412, q_loss2: -1.342832088470459, p_loss: 2.4269351959228516, mean_rew: -0.5268178603363356, var_rew: 0.29705720002090286
Running avgs for agent 9: q_loss: 1.8065146207809448, q_loss2: -1.8049989938735962, p_loss: 2.703446388244629, mean_rew: -0.5672596146062764, var_rew: 0.38569062881836746
steps: 149975, episodes: 6000, mean episode reward: -72.18454579567383, agent episode reward: [-6.945261220519358, -6.565416946842811, -6.70530390838289, -6.7263630828251575, -6.612144165262862, -7.63627015851973, -7.72445536296733, -7.850487931951941, -7.62670484209772, -7.792138176304021], time: 305.097
Running avgs for agent 0: q_loss: 1.2094532251358032, q_loss2: -1.1968392133712769, p_loss: 2.26904034614563, mean_rew: -0.3984294919552574, var_rew: 0.25407532318481635
Running avgs for agent 1: q_loss: 0.90984046459198, q_loss2: -0.8966892957687378, p_loss: 2.049422025680542, mean_rew: -0.37782754447658057, var_rew: 0.20086747151428647
Running avgs for agent 2: q_loss: 1.002819538116455, q_loss2: -0.9904037714004517, p_loss: 2.093533754348755, mean_rew: -0.3761135862713703, var_rew: 0.21730144513639152
Running avgs for agent 3: q_loss: 0.9749540090560913, q_loss2: -0.9647408127784729, p_loss: 2.1636617183685303, mean_rew: -0.38899953664381326, var_rew: 0.21538895391759547
Running avgs for agent 4: q_loss: 0.9550341963768005, q_loss2: -0.9434280395507812, p_loss: 2.092693328857422, mean_rew: -0.37761500052464925, var_rew: 0.2136443033031861
Running avgs for agent 5: q_loss: 1.6731593608856201, q_loss2: -1.6706328392028809, p_loss: 2.607144355773926, mean_rew: -0.5068812363698525, var_rew: 0.3017194984269736
Running avgs for agent 6: q_loss: 1.7030845880508423, q_loss2: -1.7012158632278442, p_loss: 2.587984323501587, mean_rew: -0.5226858003788741, var_rew: 0.30182070213190537
Running avgs for agent 7: q_loss: 1.861159086227417, q_loss2: -1.8590750694274902, p_loss: 2.682450771331787, mean_rew: -0.5225478917662608, var_rew: 0.33707726158484963
Running avgs for agent 8: q_loss: 1.4873274564743042, q_loss2: -1.4857289791107178, p_loss: 2.4605181217193604, mean_rew: -0.48545946599722467, var_rew: 0.2772792717458666
Running avgs for agent 9: q_loss: 1.9426442384719849, q_loss2: -1.9410336017608643, p_loss: 2.707925319671631, mean_rew: -0.5187884805873941, var_rew: 0.35127840007469285
steps: 174975, episodes: 7000, mean episode reward: -73.10234962375621, agent episode reward: [-6.8917539829480745, -6.55207206513186, -6.621913060739522, -6.7647221611460315, -6.952796263029573, -7.77667645659276, -7.860332769027132, -8.063641963424137, -7.742601974313513, -7.875838927403611], time: 299.742
Running avgs for agent 0: q_loss: 1.2393991947174072, q_loss2: -1.2263659238815308, p_loss: 2.2936604022979736, mean_rew: -0.3762574448903219, var_rew: 0.23093482071247187
Running avgs for agent 1: q_loss: 0.9622912406921387, q_loss2: -0.952227771282196, p_loss: 2.0730106830596924, mean_rew: -0.3599077245895979, var_rew: 0.18881553201261653
Running avgs for agent 2: q_loss: 1.0594449043273926, q_loss2: -1.0479835271835327, p_loss: 2.1326558589935303, mean_rew: -0.360255911976075, var_rew: 0.20278430895800795
Running avgs for agent 3: q_loss: 1.0358250141143799, q_loss2: -1.0257599353790283, p_loss: 2.201244831085205, mean_rew: -0.36954651485094453, var_rew: 0.20175636929623836
Running avgs for agent 4: q_loss: 1.0320178270339966, q_loss2: -1.02101469039917, p_loss: 2.1407530307769775, mean_rew: -0.3614993395745786, var_rew: 0.20255500828516979
Running avgs for agent 5: q_loss: 1.7934306859970093, q_loss2: -1.7917293310165405, p_loss: 2.632260322570801, mean_rew: -0.47754643105700617, var_rew: 0.28326182868161276
Running avgs for agent 6: q_loss: 1.8554657697677612, q_loss2: -1.853580355644226, p_loss: 2.598067283630371, mean_rew: -0.4888547067059148, var_rew: 0.28534866011570237
Running avgs for agent 7: q_loss: 2.014890432357788, q_loss2: -2.012887477874756, p_loss: 2.716507911682129, mean_rew: -0.4927199242104802, var_rew: 0.3171719308608433
Running avgs for agent 8: q_loss: 1.6686522960662842, q_loss2: -1.6663872003555298, p_loss: 2.4994120597839355, mean_rew: -0.46111003883773527, var_rew: 0.267703927757511
Running avgs for agent 9: q_loss: 2.0613062381744385, q_loss2: -2.0585267543792725, p_loss: 2.731830596923828, mean_rew: -0.4874540040504327, var_rew: 0.32552605711219157
steps: 199975, episodes: 8000, mean episode reward: -72.51167575839939, agent episode reward: [-6.962261458744053, -6.396376476307627, -6.67606753220061, -6.751445734352881, -6.818327676594033, -7.738911239459862, -7.4546901694983525, -8.0001418491259, -7.61995198081353, -8.093501641302545], time: 297.409
Running avgs for agent 0: q_loss: 1.2783657312393188, q_loss2: -1.2666305303573608, p_loss: 2.357609272003174, mean_rew: -0.3652050329262062, var_rew: 0.21566657663701408
Running avgs for agent 1: q_loss: 1.0457887649536133, q_loss2: -1.0344364643096924, p_loss: 2.0851876735687256, mean_rew: -0.346868907692459, var_rew: 0.18146759616975452
Running avgs for agent 2: q_loss: 1.1150544881820679, q_loss2: -1.1051169633865356, p_loss: 2.1522631645202637, mean_rew: -0.34761669141257745, var_rew: 0.1923071476465001
Running avgs for agent 3: q_loss: 1.1221215724945068, q_loss2: -1.111480474472046, p_loss: 2.225771903991699, mean_rew: -0.3582291718034296, var_rew: 0.19377105576869724
Running avgs for agent 4: q_loss: 1.084937572479248, q_loss2: -1.0746837854385376, p_loss: 2.1484854221343994, mean_rew: -0.3492533195233939, var_rew: 0.19110889084520727
Running avgs for agent 5: q_loss: 1.926977276802063, q_loss2: -1.924243688583374, p_loss: 2.649009943008423, mean_rew: -0.4554763746808145, var_rew: 0.26977159691447145
Running avgs for agent 6: q_loss: 2.0049068927764893, q_loss2: -2.0029897689819336, p_loss: 2.6240131855010986, mean_rew: -0.46549878314535215, var_rew: 0.2732439140658269
Running avgs for agent 7: q_loss: 2.1098809242248535, q_loss2: -2.106895685195923, p_loss: 2.7297050952911377, mean_rew: -0.4707625224793021, var_rew: 0.2961386035243492
Running avgs for agent 8: q_loss: 1.7770576477050781, q_loss2: -1.774222731590271, p_loss: 2.4989540576934814, mean_rew: -0.4381744186193301, var_rew: 0.25328251483067676
Running avgs for agent 9: q_loss: 2.1861279010772705, q_loss2: -2.1834356784820557, p_loss: 2.749870538711548, mean_rew: -0.4667338982794447, var_rew: 0.3077849438907336
steps: 224975, episodes: 9000, mean episode reward: -73.29066856322203, agent episode reward: [-7.0626358154937945, -6.801334394174431, -6.675446653965049, -7.068837530953883, -7.087406702843655, -7.4561518389138675, -7.687756226615835, -8.158416953951239, -7.408788031064332, -7.883894415245946], time: 297.558
Running avgs for agent 0: q_loss: 1.3479139804840088, q_loss2: -1.3358756303787231, p_loss: 2.4011147022247314, mean_rew: -0.35565653498138194, var_rew: 0.20620893600362475
Running avgs for agent 1: q_loss: 1.1103647947311401, q_loss2: -1.098778247833252, p_loss: 2.1029255390167236, mean_rew: -0.3365460879868791, var_rew: 0.17460864034352366
Running avgs for agent 2: q_loss: 1.1796560287475586, q_loss2: -1.168271541595459, p_loss: 2.1647119522094727, mean_rew: -0.3382244823925572, var_rew: 0.18376262709745325
Running avgs for agent 3: q_loss: 1.1918730735778809, q_loss2: -1.1820554733276367, p_loss: 2.2424516677856445, mean_rew: -0.3484687587924781, var_rew: 0.18643077072723582
Running avgs for agent 4: q_loss: 1.154848575592041, q_loss2: -1.1458256244659424, p_loss: 2.17056941986084, mean_rew: -0.34009868360647205, var_rew: 0.18440588989863008
Running avgs for agent 5: q_loss: 2.0493690967559814, q_loss2: -2.0474963188171387, p_loss: 2.6587538719177246, mean_rew: -0.43710566551788466, var_rew: 0.25927585746852616
Running avgs for agent 6: q_loss: 2.0969014167785645, q_loss2: -2.094374418258667, p_loss: 2.62508225440979, mean_rew: -0.4431059403344134, var_rew: 0.2581555935629611
Running avgs for agent 7: q_loss: 2.189035177230835, q_loss2: -2.187661647796631, p_loss: 2.7529258728027344, mean_rew: -0.4519692779274268, var_rew: 0.2796124070850975
Running avgs for agent 8: q_loss: 1.9008899927139282, q_loss2: -1.8977299928665161, p_loss: 2.51419734954834, mean_rew: -0.42075801943675817, var_rew: 0.24378058123154045
Running avgs for agent 9: q_loss: 2.2858400344848633, q_loss2: -2.283012866973877, p_loss: 2.7521581649780273, mean_rew: -0.4491205458765845, var_rew: 0.2914454981230206
steps: 249975, episodes: 10000, mean episode reward: -72.63852088000792, agent episode reward: [-6.465608061408226, -6.687693033096923, -6.596794137852584, -6.789942121268979, -6.938323735609718, -7.746678687849044, -7.597578437582254, -8.420432841452891, -7.612011514166119, -7.7834583097211745], time: 328.761
Running avgs for agent 0: q_loss: 1.4039335250854492, q_loss2: -1.3932093381881714, p_loss: 2.4402663707733154, mean_rew: -0.3472925432072284, var_rew: 0.1980614167859619
Running avgs for agent 1: q_loss: 1.1791203022003174, q_loss2: -1.1709294319152832, p_loss: 2.1294922828674316, mean_rew: -0.329818983873438, var_rew: 0.17027203092428914
Running avgs for agent 2: q_loss: 1.2533527612686157, q_loss2: -1.2446503639221191, p_loss: 2.1791858673095703, mean_rew: -0.33064285700658763, var_rew: 0.179273065927974
Running avgs for agent 3: q_loss: 1.2723523378372192, q_loss2: -1.2640554904937744, p_loss: 2.2761192321777344, mean_rew: -0.3417795355395685, var_rew: 0.18154544036691506
Running avgs for agent 4: q_loss: 1.2058494091033936, q_loss2: -1.1977314949035645, p_loss: 2.1991870403289795, mean_rew: -0.33157073449447194, var_rew: 0.1768267115871258
Running avgs for agent 5: q_loss: 2.170341730117798, q_loss2: -2.1681785583496094, p_loss: 2.669062614440918, mean_rew: -0.4220820890421255, var_rew: 0.2501641570237642
Running avgs for agent 6: q_loss: 2.234520673751831, q_loss2: -2.2331972122192383, p_loss: 2.6584632396698, mean_rew: -0.42985706518489947, var_rew: 0.2512849138440916
Running avgs for agent 7: q_loss: 2.3133301734924316, q_loss2: -2.311501979827881, p_loss: 2.8074097633361816, mean_rew: -0.4410583387374633, var_rew: 0.2694441612706991
Running avgs for agent 8: q_loss: 2.007680892944336, q_loss2: -2.005650520324707, p_loss: 2.5319221019744873, mean_rew: -0.40940006190266587, var_rew: 0.23537855748523617
Running avgs for agent 9: q_loss: 2.3547475337982178, q_loss2: -2.3511078357696533, p_loss: 2.7512152194976807, mean_rew: -0.43324421485705794, var_rew: 0.2748146640151896
steps: 274975, episodes: 11000, mean episode reward: -71.8036439130935, agent episode reward: [-6.399889804599641, -6.507732761002955, -6.457504474374475, -6.821850807842364, -6.35444877554519, -7.877081413299451, -7.696869737509691, -8.177064168743664, -7.568106059372996, -7.943095910803069], time: 340.003
Running avgs for agent 0: q_loss: 1.4618600606918335, q_loss2: -1.4529178142547607, p_loss: 2.4572856426239014, mean_rew: -0.3379402897293336, var_rew: 0.19095144189987598
Running avgs for agent 1: q_loss: 1.2411792278289795, q_loss2: -1.232943058013916, p_loss: 2.1573691368103027, mean_rew: -0.3238154900962664, var_rew: 0.16488324623932682
Running avgs for agent 2: q_loss: 1.321778416633606, q_loss2: -1.313226342201233, p_loss: 2.1972174644470215, mean_rew: -0.3252362689272827, var_rew: 0.1740404084936046
Running avgs for agent 3: q_loss: 1.3123514652252197, q_loss2: -1.3035873174667358, p_loss: 2.28495454788208, mean_rew: -0.33451802726716795, var_rew: 0.173282051216917
Running avgs for agent 4: q_loss: 1.3040075302124023, q_loss2: -1.2950832843780518, p_loss: 2.2665088176727295, mean_rew: -0.3297842871010872, var_rew: 0.1743632028810778
Running avgs for agent 5: q_loss: 2.2923662662506104, q_loss2: -2.289994239807129, p_loss: 2.687633514404297, mean_rew: -0.4124588684450115, var_rew: 0.24292471264197588
Running avgs for agent 6: q_loss: 2.3590424060821533, q_loss2: -2.3575263023376465, p_loss: 2.684211492538452, mean_rew: -0.4194059569682288, var_rew: 0.24380270386204678
Running avgs for agent 7: q_loss: 2.4068315029144287, q_loss2: -2.4026496410369873, p_loss: 2.8262791633605957, mean_rew: -0.4281867804549612, var_rew: 0.25751527163985577
Running avgs for agent 8: q_loss: 2.1478819847106934, q_loss2: -2.1458771228790283, p_loss: 2.567119598388672, mean_rew: -0.4003192368943868, var_rew: 0.2305874153809896
Running avgs for agent 9: q_loss: 2.4697299003601074, q_loss2: -2.4670825004577637, p_loss: 2.758983850479126, mean_rew: -0.4215031279787815, var_rew: 0.2662445339252661
steps: 299975, episodes: 12000, mean episode reward: -72.0879461802062, agent episode reward: [-6.5816760190265535, -6.403490670272493, -6.618322866549805, -6.949705050305088, -6.58199699350857, -7.671359681039028, -7.7525695886145565, -8.080914827366792, -7.780400061842154, -7.667510421681171], time: 324.302
Running avgs for agent 0: q_loss: 1.5105587244033813, q_loss2: -1.5026633739471436, p_loss: 2.472193717956543, mean_rew: -0.3318634412206867, var_rew: 0.18429483152743104
Running avgs for agent 1: q_loss: 1.3084779977798462, q_loss2: -1.3019534349441528, p_loss: 2.189854383468628, mean_rew: -0.31892924973506703, var_rew: 0.16134642073722927
Running avgs for agent 2: q_loss: 1.3747873306274414, q_loss2: -1.3670598268508911, p_loss: 2.198528528213501, mean_rew: -0.31979839421251416, var_rew: 0.16829001345700653
Running avgs for agent 3: q_loss: 1.378467321395874, q_loss2: -1.3696110248565674, p_loss: 2.3167035579681396, mean_rew: -0.3289136688891381, var_rew: 0.1682747278347809
Running avgs for agent 4: q_loss: 1.3441858291625977, q_loss2: -1.3380858898162842, p_loss: 2.290069818496704, mean_rew: -0.3220387935788713, var_rew: 0.16798609671186598
Running avgs for agent 5: q_loss: 2.413823127746582, q_loss2: -2.4118754863739014, p_loss: 2.705728054046631, mean_rew: -0.4042287635450229, var_rew: 0.23677528018210275
Running avgs for agent 6: q_loss: 2.4767541885375977, q_loss2: -2.4753847122192383, p_loss: 2.703606605529785, mean_rew: -0.40888151923054356, var_rew: 0.2370492679907181
Running avgs for agent 7: q_loss: 2.5505294799804688, q_loss2: -2.548218250274658, p_loss: 2.8557682037353516, mean_rew: -0.42268381065933297, var_rew: 0.2529714240190297
Running avgs for agent 8: q_loss: 2.2230403423309326, q_loss2: -2.2207798957824707, p_loss: 2.578564405441284, mean_rew: -0.38922432819920505, var_rew: 0.22151324477632042
Running avgs for agent 9: q_loss: 2.5833353996276855, q_loss2: -2.5802230834960938, p_loss: 2.7685794830322266, mean_rew: -0.41254114078309095, var_rew: 0.257839043973187
steps: 324975, episodes: 13000, mean episode reward: -72.00760204182748, agent episode reward: [-6.522619659195954, -6.457591892019922, -6.412187705172053, -6.705299460275509, -6.941981167999047, -7.852582124970013, -7.793354216834011, -7.890039364310333, -7.8174707962043275, -7.614475654846321], time: 354.873
Running avgs for agent 0: q_loss: 1.5602482557296753, q_loss2: -1.5522185564041138, p_loss: 2.4689602851867676, mean_rew: -0.3248997259093492, var_rew: 0.17789616072175438
Running avgs for agent 1: q_loss: 1.39321768283844, q_loss2: -1.3849496841430664, p_loss: 2.2113595008850098, mean_rew: -0.3135495844501156, var_rew: 0.1584012361346228
Running avgs for agent 2: q_loss: 1.4494495391845703, q_loss2: -1.4405121803283691, p_loss: 2.196669340133667, mean_rew: -0.3131009069159472, var_rew: 0.16453349594504663
Running avgs for agent 3: q_loss: 1.4673694372177124, q_loss2: -1.458547592163086, p_loss: 2.3434746265411377, mean_rew: -0.3255628365609432, var_rew: 0.16587141988540094
Running avgs for agent 4: q_loss: 1.4397485256195068, q_loss2: -1.4325335025787354, p_loss: 2.3171494007110596, mean_rew: -0.31886987264650474, var_rew: 0.1659109072061819
Running avgs for agent 5: q_loss: 2.4998621940612793, q_loss2: -2.498401403427124, p_loss: 2.7245607376098633, mean_rew: -0.3962712191655374, var_rew: 0.2290195064755593
Running avgs for agent 6: q_loss: 2.5943453311920166, q_loss2: -2.5931313037872314, p_loss: 2.720872163772583, mean_rew: -0.4009043339962342, var_rew: 0.23132987873990038
Running avgs for agent 7: q_loss: 2.614513635635376, q_loss2: -2.6128921508789062, p_loss: 2.8313136100769043, mean_rew: -0.4114650513745895, var_rew: 0.24262311452033783
Running avgs for agent 8: q_loss: 2.346198558807373, q_loss2: -2.344310998916626, p_loss: 2.6110122203826904, mean_rew: -0.3836998653194382, var_rew: 0.21740877406228876
Running avgs for agent 9: q_loss: 2.728339672088623, q_loss2: -2.725707530975342, p_loss: 2.7920005321502686, mean_rew: -0.4050090029899368, var_rew: 0.25326903181550803
steps: 349975, episodes: 14000, mean episode reward: -72.32990278656877, agent episode reward: [-6.701506525951576, -6.743653836370888, -6.534875328494094, -6.698996529450391, -6.640223533268551, -7.894679527917715, -7.672105464871462, -7.823007126858831, -7.808867638554404, -7.811987274830854], time: 316.818
Running avgs for agent 0: q_loss: 1.6505167484283447, q_loss2: -1.641811490058899, p_loss: 2.4801480770111084, mean_rew: -0.3207709635614756, var_rew: 0.17508480382082633
Running avgs for agent 1: q_loss: 1.4546376466751099, q_loss2: -1.4479621648788452, p_loss: 2.2233431339263916, mean_rew: -0.3090578986027777, var_rew: 0.15524498982447038
Running avgs for agent 2: q_loss: 1.514393925666809, q_loss2: -1.5081521272659302, p_loss: 2.1931183338165283, mean_rew: -0.3086310662885042, var_rew: 0.1614980037518074
Running avgs for agent 3: q_loss: 1.5269683599472046, q_loss2: -1.519918441772461, p_loss: 2.3389484882354736, mean_rew: -0.32009638484924385, var_rew: 0.16230885412245608
Running avgs for agent 4: q_loss: 1.5043425559997559, q_loss2: -1.497020959854126, p_loss: 2.3359532356262207, mean_rew: -0.31489672977228583, var_rew: 0.16200153319936947
Running avgs for agent 5: q_loss: 2.5938384532928467, q_loss2: -2.5902440547943115, p_loss: 2.745265245437622, mean_rew: -0.3893125399973968, var_rew: 0.2220682128140799
Running avgs for agent 6: q_loss: 2.6802964210510254, q_loss2: -2.6791622638702393, p_loss: 2.7163243293762207, mean_rew: -0.39218425736031454, var_rew: 0.22435453251608606
Running avgs for agent 7: q_loss: 2.7304325103759766, q_loss2: -2.727670192718506, p_loss: 2.8247079849243164, mean_rew: -0.4050292854011618, var_rew: 0.2366975685490304
Running avgs for agent 8: q_loss: 2.5134122371673584, q_loss2: -2.5101842880249023, p_loss: 2.654249429702759, mean_rew: -0.38064918683982296, var_rew: 0.21622310276361492
Running avgs for agent 9: q_loss: 2.819603204727173, q_loss2: -2.8157331943511963, p_loss: 2.7922489643096924, mean_rew: -0.39763930782091095, var_rew: 0.24491056332825525
steps: 374975, episodes: 15000, mean episode reward: -71.18341907331758, agent episode reward: [-6.625588835085381, -6.326763874562236, -6.628748501978501, -6.483977203686248, -6.564646394142819, -7.804578295412956, -7.549635726270575, -7.816683976094573, -7.86547777236733, -7.517318493716955], time: 322.988
Running avgs for agent 0: q_loss: 1.7098815441131592, q_loss2: -1.6999709606170654, p_loss: 2.475184679031372, mean_rew: -0.3168181973303548, var_rew: 0.1706900060431467
Running avgs for agent 1: q_loss: 1.5577830076217651, q_loss2: -1.5521291494369507, p_loss: 2.250431537628174, mean_rew: -0.30716659587371303, var_rew: 0.15496795379984474
Running avgs for agent 2: q_loss: 1.6150752305984497, q_loss2: -1.6066043376922607, p_loss: 2.1927545070648193, mean_rew: -0.3058808399235833, var_rew: 0.1599081525138284
Running avgs for agent 3: q_loss: 1.5870405435562134, q_loss2: -1.5804646015167236, p_loss: 2.3466968536376953, mean_rew: -0.31590928356468806, var_rew: 0.15865429262726247
Running avgs for agent 4: q_loss: 1.5846660137176514, q_loss2: -1.578124761581421, p_loss: 2.349454164505005, mean_rew: -0.31120457978628824, var_rew: 0.15966001226397455
Running avgs for agent 5: q_loss: 2.7341580390930176, q_loss2: -2.7325356006622314, p_loss: 2.781400680541992, mean_rew: -0.38595797771405677, var_rew: 0.2200365986020446
Running avgs for agent 6: q_loss: 2.842655897140503, q_loss2: -2.8407227993011475, p_loss: 2.742368459701538, mean_rew: -0.38809141533616115, var_rew: 0.22252888891801267
Running avgs for agent 7: q_loss: 2.8561410903930664, q_loss2: -2.8545055389404297, p_loss: 2.8289995193481445, mean_rew: -0.3995629674768418, var_rew: 0.23268229796732134
Running avgs for agent 8: q_loss: 2.61548113822937, q_loss2: -2.613116979598999, p_loss: 2.6711597442626953, mean_rew: -0.3743329397377493, var_rew: 0.2114378827302238
Running avgs for agent 9: q_loss: 2.919362783432007, q_loss2: -2.9166738986968994, p_loss: 2.795279026031494, mean_rew: -0.3915635043466966, var_rew: 0.2387243644923361
steps: 399975, episodes: 16000, mean episode reward: -72.13206129464848, agent episode reward: [-6.58702571023166, -6.424829026880878, -6.096531108897387, -6.843328460791207, -6.962199156344857, -8.232936271462593, -7.731268733133779, -7.910506348163114, -7.652497154527006, -7.690939324215984], time: 352.092
Running avgs for agent 0: q_loss: 1.8331249952316284, q_loss2: -1.8240000009536743, p_loss: 2.480757236480713, mean_rew: -0.3155261642535762, var_rew: 0.17102948769705892
Running avgs for agent 1: q_loss: 1.6315116882324219, q_loss2: -1.6244025230407715, p_loss: 2.2552499771118164, mean_rew: -0.3022475394635537, var_rew: 0.15213384491013612
Running avgs for agent 2: q_loss: 1.7000951766967773, q_loss2: -1.6941542625427246, p_loss: 2.201259136199951, mean_rew: -0.30454965166440456, var_rew: 0.15853532869070436
Running avgs for agent 3: q_loss: 1.6900798082351685, q_loss2: -1.6834040880203247, p_loss: 2.357459545135498, mean_rew: -0.3135277023790209, var_rew: 0.15807861561340097
Running avgs for agent 4: q_loss: 1.6273443698883057, q_loss2: -1.6211472749710083, p_loss: 2.3585686683654785, mean_rew: -0.3076645193356513, var_rew: 0.15511069059727597
Running avgs for agent 5: q_loss: 2.8123838901519775, q_loss2: -2.810703992843628, p_loss: 2.788783311843872, mean_rew: -0.3802219501818436, var_rew: 0.21391976284704292
Running avgs for agent 6: q_loss: 2.963491201400757, q_loss2: -2.9617671966552734, p_loss: 2.755293846130371, mean_rew: -0.3835647397230499, var_rew: 0.21870579626169004
Running avgs for agent 7: q_loss: 2.9936251640319824, q_loss2: -2.9916646480560303, p_loss: 2.846862316131592, mean_rew: -0.3945491856567782, var_rew: 0.2293473446898912
Running avgs for agent 8: q_loss: 2.743880033493042, q_loss2: -2.7425527572631836, p_loss: 2.6951327323913574, mean_rew: -0.371400254241388, var_rew: 0.20884565272031047
Running avgs for agent 9: q_loss: 3.026029586791992, q_loss2: -3.022491455078125, p_loss: 2.7883076667785645, mean_rew: -0.38338453923726273, var_rew: 0.2330621422728122
^[[Bsteps: 424975, episodes: 17000, mean episode reward: -70.78415265524134, agent episode reward: [-6.750536817346243, -6.270417024382422, -6.034744460066483, -6.466712338007617, -6.64078027292867, -7.9309614991343675, -7.684364445232728, -7.892221490264777, -7.68468224691787, -7.428732060960181], time: 345.196
Running avgs for agent 0: q_loss: 1.8755966424942017, q_loss2: -1.8652619123458862, p_loss: 2.4714114665985107, mean_rew: -0.31065813919302593, var_rew: 0.16553788787720408
Running avgs for agent 1: q_loss: 1.730392575263977, q_loss2: -1.72505784034729, p_loss: 2.265441656112671, mean_rew: -0.30019676678988333, var_rew: 0.15174190196425646
Running avgs for agent 2: q_loss: 1.775776982307434, q_loss2: -1.770060658454895, p_loss: 2.1843013763427734, mean_rew: -0.2992711859031515, var_rew: 0.15608661099872606
Running avgs for agent 3: q_loss: 1.7520062923431396, q_loss2: -1.7463068962097168, p_loss: 2.341521978378296, mean_rew: -0.30926474167646534, var_rew: 0.15510119052133686
Running avgs for agent 4: q_loss: 1.6987553834915161, q_loss2: -1.6914361715316772, p_loss: 2.378499984741211, mean_rew: -0.3052728300429412, var_rew: 0.15266579485319898
Running avgs for agent 5: q_loss: 2.930344343185425, q_loss2: -2.928986072540283, p_loss: 2.817232608795166, mean_rew: -0.37601396391866804, var_rew: 0.21075089175740072
Running avgs for agent 6: q_loss: 3.0768182277679443, q_loss2: -3.0759341716766357, p_loss: 2.7698493003845215, mean_rew: -0.37910392784924274, var_rew: 0.21501617021097635
Running avgs for agent 7: q_loss: 3.060194492340088, q_loss2: -3.0590195655822754, p_loss: 2.8585567474365234, mean_rew: -0.3891824101412824, var_rew: 0.2225864549565571
Running avgs for agent 8: q_loss: 2.866830587387085, q_loss2: -2.863767147064209, p_loss: 2.709268093109131, mean_rew: -0.3661346639996053, var_rew: 0.20549371438836064
Running avgs for agent 9: q_loss: 3.0929484367370605, q_loss2: -3.091135263442993, p_loss: 2.7979934215545654, mean_rew: -0.3798308640728879, var_rew: 0.22641571155995616
steps: 449975, episodes: 18000, mean episode reward: -71.70513916936234, agent episode reward: [-6.67452992784181, -6.230076378416033, -6.15174786976197, -6.391036628159691, -6.749910400319713, -8.326250889313457, -7.506198190852263, -8.124867161978298, -7.578115495330441, -7.972406227388661], time: 325.303
Running avgs for agent 0: q_loss: 1.9619823694229126, q_loss2: -1.9555869102478027, p_loss: 2.474558115005493, mean_rew: -0.3077329737382943, var_rew: 0.16452730097084725
Running avgs for agent 1: q_loss: 1.8400992155075073, q_loss2: -1.8327089548110962, p_loss: 2.270087242126465, mean_rew: -0.29755384119084266, var_rew: 0.15104756060383892
Running avgs for agent 2: q_loss: 1.8578808307647705, q_loss2: -1.8504091501235962, p_loss: 2.167428493499756, mean_rew: -0.2956950452035351, var_rew: 0.15397435068348975
Running avgs for agent 3: q_loss: 1.8473782539367676, q_loss2: -1.841162085533142, p_loss: 2.340700149536133, mean_rew: -0.3072217991433283, var_rew: 0.1539638459128729
Running avgs for agent 4: q_loss: 1.787353515625, q_loss2: -1.7819406986236572, p_loss: 2.3949897289276123, mean_rew: -0.30401072730457984, var_rew: 0.15173695789831454
Running avgs for agent 5: q_loss: 3.0571675300598145, q_loss2: -3.055865526199341, p_loss: 2.849860906600952, mean_rew: -0.37516949239828046, var_rew: 0.20824590084358435
Running avgs for agent 6: q_loss: 3.207167863845825, q_loss2: -3.2056970596313477, p_loss: 2.7890536785125732, mean_rew: -0.3749613114808217, var_rew: 0.21214123808886381
Running avgs for agent 7: q_loss: 3.202509880065918, q_loss2: -3.2003018856048584, p_loss: 2.8766539096832275, mean_rew: -0.3842215465669419, var_rew: 0.2199434748059118
Running avgs for agent 8: q_loss: 2.9769153594970703, q_loss2: -2.9756557941436768, p_loss: 2.732334852218628, mean_rew: -0.3634026218753509, var_rew: 0.2026679935829046
Running avgs for agent 9: q_loss: 3.2363438606262207, q_loss2: -3.2327518463134766, p_loss: 2.8104746341705322, mean_rew: -0.3761288356457916, var_rew: 0.22376641442004971
steps: 474975, episodes: 19000, mean episode reward: -70.95846868653696, agent episode reward: [-6.640669253238945, -6.0302646121633785, -6.141429232380737, -6.1477324612505875, -6.573818311651263, -8.523874559347547, -7.53604542733188, -7.921394512964534, -7.575087846591411, -7.868152469616675], time: 334.445
Running avgs for agent 0: q_loss: 2.0390286445617676, q_loss2: -2.0316474437713623, p_loss: 2.4872872829437256, mean_rew: -0.3076186520387381, var_rew: 0.16185695349310816
Running avgs for agent 1: q_loss: 1.866461157798767, q_loss2: -1.862821340560913, p_loss: 2.2602739334106445, mean_rew: -0.2932456126406337, var_rew: 0.14718895238872723
Running avgs for agent 2: q_loss: 1.945214867591858, q_loss2: -1.9404572248458862, p_loss: 2.1639785766601562, mean_rew: -0.29387180773775595, var_rew: 0.15298339022754628
Running avgs for agent 3: q_loss: 1.9120593070983887, q_loss2: -1.9067575931549072, p_loss: 2.3389029502868652, mean_rew: -0.3039197081654205, var_rew: 0.1515660164400043
Running avgs for agent 4: q_loss: 1.8424392938613892, q_loss2: -1.8385937213897705, p_loss: 2.406646490097046, mean_rew: -0.30151702820997095, var_rew: 0.14912939511799667
Running avgs for agent 5: q_loss: 3.182924270629883, q_loss2: -3.180032253265381, p_loss: 2.8760263919830322, mean_rew: -0.3730092532890731, var_rew: 0.2055406377361279
Running avgs for agent 6: q_loss: 3.3285412788391113, q_loss2: -3.327580451965332, p_loss: 2.80531644821167, mean_rew: -0.37048425734357776, var_rew: 0.2094074502779709
Running avgs for agent 7: q_loss: 3.300990343093872, q_loss2: -3.299348831176758, p_loss: 2.8870699405670166, mean_rew: -0.38034827113165487, var_rew: 0.2158976116399852
Running avgs for agent 8: q_loss: 3.113642692565918, q_loss2: -3.111508846282959, p_loss: 2.7516672611236572, mean_rew: -0.36110127176530876, var_rew: 0.20072476379899723
Running avgs for agent 9: q_loss: 3.3782966136932373, q_loss2: -3.374448299407959, p_loss: 2.8230366706848145, mean_rew: -0.3726780066747261, var_rew: 0.22144397530208174
steps: 499975, episodes: 20000, mean episode reward: -71.41538607106074, agent episode reward: [-6.461554845370781, -6.251337178472149, -6.143061194295935, -6.183521649889686, -6.4922853626591355, -9.186554875217627, -7.493578900192113, -8.233345886547822, -7.63954748859039, -7.330598689825098], time: 330.412
Running avgs for agent 0: q_loss: 2.099675178527832, q_loss2: -2.091261148452759, p_loss: 2.48195743560791, mean_rew: -0.30388300836052123, var_rew: 0.15849691171539343
Running avgs for agent 1: q_loss: 1.9832412004470825, q_loss2: -1.9793437719345093, p_loss: 2.2716431617736816, mean_rew: -0.29317845256149044, var_rew: 0.14754201868350963
Running avgs for agent 2: q_loss: 1.9947757720947266, q_loss2: -1.988036870956421, p_loss: 2.1389236450195312, mean_rew: -0.2906686654753203, var_rew: 0.1493971866415397
Running avgs for agent 3: q_loss: 1.9570695161819458, q_loss2: -1.9525123834609985, p_loss: 2.328202247619629, mean_rew: -0.3008167569788637, var_rew: 0.14825896923683674
Running avgs for agent 4: q_loss: 1.920711636543274, q_loss2: -1.915299654006958, p_loss: 2.4255058765411377, mean_rew: -0.2997492431516769, var_rew: 0.1473203016022441
Running avgs for agent 5: q_loss: 3.297097682952881, q_loss2: -3.295405864715576, p_loss: 2.909590244293213, mean_rew: -0.3735123628747396, var_rew: 0.20300362552805643
Running avgs for agent 6: q_loss: 3.418888807296753, q_loss2: -3.4176979064941406, p_loss: 2.8129074573516846, mean_rew: -0.3658458142201937, var_rew: 0.20530962298999317
Running avgs for agent 7: q_loss: 3.3474278450012207, q_loss2: -3.3463382720947266, p_loss: 2.8971681594848633, mean_rew: -0.37680418702440793, var_rew: 0.20956635002925508
Running avgs for agent 8: q_loss: 3.2389066219329834, q_loss2: -3.23655366897583, p_loss: 2.7638022899627686, mean_rew: -0.3570764352332862, var_rew: 0.19861735329837757
Running avgs for agent 9: q_loss: 3.519474744796753, q_loss2: -3.515897035598755, p_loss: 2.8366496562957764, mean_rew: -0.37031776876396794, var_rew: 0.21928104675349902
steps: 524975, episodes: 21000, mean episode reward: -72.84233438742125, agent episode reward: [-6.956664794003833, -6.345261429192301, -6.400742461226094, -6.126548087602842, -6.769951449907444, -9.444459449959115, -7.619733382958582, -8.152732886039166, -7.4329670770155305, -7.593273369516326], time: 333.079
Running avgs for agent 0: q_loss: 2.1598246097564697, q_loss2: -2.1559362411499023, p_loss: 2.4847302436828613, mean_rew: -0.30292875095961713, var_rew: 0.15653606844121076
Running avgs for agent 1: q_loss: 2.0296220779418945, q_loss2: -2.023865222930908, p_loss: 2.2471351623535156, mean_rew: -0.2877279869641429, var_rew: 0.14424497170582107
Running avgs for agent 2: q_loss: 2.1033411026000977, q_loss2: -2.097959518432617, p_loss: 2.1184957027435303, mean_rew: -0.2889104340074655, var_rew: 0.14956370759018073
Running avgs for agent 3: q_loss: 2.1175546646118164, q_loss2: -2.1117913722991943, p_loss: 2.326061487197876, mean_rew: -0.2996197766626343, var_rew: 0.1505697485317789
Running avgs for agent 4: q_loss: 2.0149831771850586, q_loss2: -2.0055723190307617, p_loss: 2.4482808113098145, mean_rew: -0.2979953934584551, var_rew: 0.1459701115177774
Running avgs for agent 5: q_loss: 3.369596004486084, q_loss2: -3.367949962615967, p_loss: 2.93314790725708, mean_rew: -0.372120816764939, var_rew: 0.19864626181390788
Running avgs for agent 6: q_loss: 3.570143699645996, q_loss2: -3.5688860416412354, p_loss: 2.829911947250366, mean_rew: -0.36394063140559046, var_rew: 0.2042623203451148
Running avgs for agent 7: q_loss: 3.5255885124206543, q_loss2: -3.5239861011505127, p_loss: 2.9281132221221924, mean_rew: -0.37621926224373836, var_rew: 0.20966710100795938
Running avgs for agent 8: q_loss: 3.3583507537841797, q_loss2: -3.356818437576294, p_loss: 2.7780230045318604, mean_rew: -0.35388328326275137, var_rew: 0.19655034015868153
Running avgs for agent 9: q_loss: 3.650026321411133, q_loss2: -3.6487557888031006, p_loss: 2.8424086570739746, mean_rew: -0.36651543093777694, var_rew: 0.217120870520503
steps: 549975, episodes: 22000, mean episode reward: -71.61774520408495, agent episode reward: [-6.738247213097244, -6.163313964556256, -6.154314929165805, -6.151971661120781, -6.460993825547266, -9.186355262895233, -7.554903343543679, -7.878985094232734, -7.739137238798849, -7.589522671127117], time: 304.865
Running avgs for agent 0: q_loss: 2.239799737930298, q_loss2: -2.2348618507385254, p_loss: 2.4876596927642822, mean_rew: -0.30257331054029063, var_rew: 0.1546258320526486
Running avgs for agent 1: q_loss: 2.1626670360565186, q_loss2: -2.1590635776519775, p_loss: 2.2538208961486816, mean_rew: -0.2887598181655203, var_rew: 0.14569523503638457
Running avgs for agent 2: q_loss: 2.177222728729248, q_loss2: -2.172832489013672, p_loss: 2.0934691429138184, mean_rew: -0.28695696646950336, var_rew: 0.14799274801472886
Running avgs for agent 3: q_loss: 2.1507608890533447, q_loss2: -2.1452813148498535, p_loss: 2.2948200702667236, mean_rew: -0.2953037314166192, var_rew: 0.1469170450011208
Running avgs for agent 4: q_loss: 2.0460116863250732, q_loss2: -2.0415267944335938, p_loss: 2.4540891647338867, mean_rew: -0.29636241783735606, var_rew: 0.14322771552829597
Running avgs for agent 5: q_loss: 3.4231128692626953, q_loss2: -3.421825885772705, p_loss: 2.9641573429107666, mean_rew: -0.3705997523907504, var_rew: 0.1939734165969455
Running avgs for agent 6: q_loss: 3.704446792602539, q_loss2: -3.7033851146698, p_loss: 2.8406171798706055, mean_rew: -0.36140862235473964, var_rew: 0.20255381114726698
Running avgs for agent 7: q_loss: 3.6746647357940674, q_loss2: -3.6723780632019043, p_loss: 2.9423484802246094, mean_rew: -0.37448604036829874, var_rew: 0.2082196880651483
Running avgs for agent 8: q_loss: 3.4654853343963623, q_loss2: -3.464231491088867, p_loss: 2.7836215496063232, mean_rew: -0.35175279021017514, var_rew: 0.1940429254657633
Running avgs for agent 9: q_loss: 3.7611656188964844, q_loss2: -3.7592432498931885, p_loss: 2.8528454303741455, mean_rew: -0.3643048056973296, var_rew: 0.21368784083379977
steps: 574975, episodes: 23000, mean episode reward: -72.21116709126566, agent episode reward: [-6.917613300920825, -6.188045975650013, -6.493147370350445, -6.171222918547389, -6.259668854774414, -9.391620518431617, -7.417748299318773, -7.943468883986181, -7.558445851988554, -7.870185117297464], time: 310.871
Running avgs for agent 0: q_loss: 2.3222758769989014, q_loss2: -2.313581943511963, p_loss: 2.4868602752685547, mean_rew: -0.30074498225416335, var_rew: 0.1526897740329366
Running avgs for agent 1: q_loss: 2.226804494857788, q_loss2: -2.221721887588501, p_loss: 2.2332005500793457, mean_rew: -0.2853532894064157, var_rew: 0.14352052902347567
Running avgs for agent 2: q_loss: 2.279487371444702, q_loss2: -2.2743446826934814, p_loss: 2.0795345306396484, mean_rew: -0.2857891934127388, var_rew: 0.14749122232682169
Running avgs for agent 3: q_loss: 2.2999887466430664, q_loss2: -2.2953333854675293, p_loss: 2.2896878719329834, mean_rew: -0.29562717506763975, var_rew: 0.14873611828646793
Running avgs for agent 4: q_loss: 2.109398126602173, q_loss2: -2.1053245067596436, p_loss: 2.451984167098999, mean_rew: -0.29414259710124435, var_rew: 0.14160974403947496
Running avgs for agent 5: q_loss: 3.524412155151367, q_loss2: -3.522831678390503, p_loss: 2.9967141151428223, mean_rew: -0.3704023091055311, var_rew: 0.1915410426398796
Running avgs for agent 6: q_loss: 3.8056483268737793, q_loss2: -3.8046276569366455, p_loss: 2.837252378463745, mean_rew: -0.3584603148410104, var_rew: 0.19962537714118367
Running avgs for agent 7: q_loss: 3.7406089305877686, q_loss2: -3.739490509033203, p_loss: 2.9395930767059326, mean_rew: -0.37057449951264715, var_rew: 0.20382998303042493
Running avgs for agent 8: q_loss: 3.6421151161193848, q_loss2: -3.640475034713745, p_loss: 2.795307159423828, mean_rew: -0.3505981265224143, var_rew: 0.19436440649886516
Running avgs for agent 9: q_loss: 3.9560964107513428, q_loss2: -3.952943801879883, p_loss: 2.864962577819824, mean_rew: -0.36250420352237794, var_rew: 0.21382500464842585
steps: 599975, episodes: 24000, mean episode reward: -71.07825973150736, agent episode reward: [-7.001017854432235, -6.215199398603552, -6.450296826541158, -6.112044684048478, -6.252024600439716, -8.197724477778221, -7.842690161808883, -7.894005472898119, -7.431775242399168, -7.681481012557835], time: 309.432
Running avgs for agent 0: q_loss: 2.3565282821655273, q_loss2: -2.351900339126587, p_loss: 2.4802956581115723, mean_rew: -0.2981615676799787, var_rew: 0.14984442546217527
Running avgs for agent 1: q_loss: 2.2907705307006836, q_loss2: -2.2863807678222656, p_loss: 2.2307519912719727, mean_rew: -0.28412172204951536, var_rew: 0.14181708635836499
Running avgs for agent 2: q_loss: 2.31427001953125, q_loss2: -2.3105015754699707, p_loss: 2.050818920135498, mean_rew: -0.28448208906445355, var_rew: 0.14444302181830457
Running avgs for agent 3: q_loss: 2.3546290397644043, q_loss2: -2.3496103286743164, p_loss: 2.269564390182495, mean_rew: -0.2927263064869039, var_rew: 0.14621633380244015
Running avgs for agent 4: q_loss: 2.2213101387023926, q_loss2: -2.2158687114715576, p_loss: 2.464340925216675, mean_rew: -0.2930554109995363, var_rew: 0.14172845390202976
Running avgs for agent 5: q_loss: 3.6903679370880127, q_loss2: -3.6878719329833984, p_loss: 3.0263125896453857, mean_rew: -0.3714923067110908, var_rew: 0.1916178044651585
Running avgs for agent 6: q_loss: 3.985753059387207, q_loss2: -3.984628677368164, p_loss: 2.844434976577759, mean_rew: -0.3571536380348321, var_rew: 0.19985367857597347
Running avgs for agent 7: q_loss: 3.864445686340332, q_loss2: -3.863029718399048, p_loss: 2.942384719848633, mean_rew: -0.36788906028952595, var_rew: 0.20180400989081976
Running avgs for agent 8: q_loss: 3.734847068786621, q_loss2: -3.7335140705108643, p_loss: 2.790592908859253, mean_rew: -0.3477189224685276, var_rew: 0.1915757373295952
Running avgs for agent 9: q_loss: 4.023525238037109, q_loss2: -4.0214362144470215, p_loss: 2.8597779273986816, mean_rew: -0.3589128164279115, var_rew: 0.20933620818660018
steps: 624975, episodes: 25000, mean episode reward: -71.54808825571342, agent episode reward: [-7.070134259460512, -6.423606847310275, -6.3220058876807315, -6.395219175803026, -6.231950703803854, -8.050989329535556, -7.689317781655617, -7.9638426638869575, -7.607167534369798, -7.793854072207094], time: 371.672
Running avgs for agent 0: q_loss: 2.443737506866455, q_loss2: -2.4378037452697754, p_loss: 2.4903738498687744, mean_rew: -0.2987996950575172, var_rew: 0.14875434400021123
Running avgs for agent 1: q_loss: 2.4475345611572266, q_loss2: -2.441882610321045, p_loss: 2.2423834800720215, mean_rew: -0.28324112711325644, var_rew: 0.14351360205677913
Running avgs for agent 2: q_loss: 2.417571783065796, q_loss2: -2.4134457111358643, p_loss: 2.0328001976013184, mean_rew: -0.2827172352331386, var_rew: 0.14424733467954917
Running avgs for agent 3: q_loss: 2.4541025161743164, q_loss2: -2.4491493701934814, p_loss: 2.256831645965576, mean_rew: -0.2908525319380576, var_rew: 0.14571094273398053
Running avgs for agent 4: q_loss: 2.2898294925689697, q_loss2: -2.283996343612671, p_loss: 2.43704891204834, mean_rew: -0.2907845708279633, var_rew: 0.14024483767014714
Running avgs for agent 5: q_loss: 3.7934389114379883, q_loss2: -3.792015790939331, p_loss: 3.0217792987823486, mean_rew: -0.3695893382345718, var_rew: 0.1895717642248698
Running avgs for agent 6: q_loss: 4.094454765319824, q_loss2: -4.092972755432129, p_loss: 2.8339335918426514, mean_rew: -0.3543272783044842, var_rew: 0.1974084357731964
Running avgs for agent 7: q_loss: 3.9839510917663574, q_loss2: -3.9821090698242188, p_loss: 2.941575288772583, mean_rew: -0.3653380851490227, var_rew: 0.19973288653995058
Running avgs for agent 8: q_loss: 3.8973941802978516, q_loss2: -3.894883394241333, p_loss: 2.798121452331543, mean_rew: -0.3471754136750908, var_rew: 0.19125528456857824
Running avgs for agent 9: q_loss: 4.098104476928711, q_loss2: -4.096403121948242, p_loss: 2.8541855812072754, mean_rew: -0.3564697470625163, var_rew: 0.20532174171988768
steps: 649975, episodes: 26000, mean episode reward: -71.826911368617, agent episode reward: [-6.934823628661727, -6.264745538858217, -6.537780990895241, -6.2712238055316885, -6.206199155734836, -8.198271538746152, -7.842782841079508, -7.928889632446811, -7.611506722460338, -8.030687514202482], time: 507.196
Running avgs for agent 0: q_loss: 2.529787063598633, q_loss2: -2.525770425796509, p_loss: 2.4911317825317383, mean_rew: -0.2974742785820897, var_rew: 0.14804756510371492
Running avgs for agent 1: q_loss: 2.509953022003174, q_loss2: -2.5055949687957764, p_loss: 2.2343342304229736, mean_rew: -0.2824012571538294, var_rew: 0.14186560210468513
Running avgs for agent 2: q_loss: 2.503589630126953, q_loss2: -2.500201463699341, p_loss: 2.0327272415161133, mean_rew: -0.2834778691536743, var_rew: 0.14346686840251174
Running avgs for agent 3: q_loss: 2.5001275539398193, q_loss2: -2.4945898056030273, p_loss: 2.234816312789917, mean_rew: -0.2874101467322837, var_rew: 0.14320990809338155
Running avgs for agent 4: q_loss: 2.3369057178497314, q_loss2: -2.332810640335083, p_loss: 2.380078077316284, mean_rew: -0.28900594423953496, var_rew: 0.13831355540446663
Running avgs for agent 5: q_loss: 3.9253289699554443, q_loss2: -3.923352003097534, p_loss: 3.002382755279541, mean_rew: -0.3664286721527756, var_rew: 0.18848607536526418
Running avgs for agent 6: q_loss: 4.205406188964844, q_loss2: -4.204406261444092, p_loss: 2.817751407623291, mean_rew: -0.3522978263529554, var_rew: 0.19534356233232775
Running avgs for agent 7: q_loss: 4.1360578536987305, q_loss2: -4.1349873542785645, p_loss: 2.9367072582244873, mean_rew: -0.36490703356596094, var_rew: 0.19911556413179526
Running avgs for agent 8: q_loss: 4.046997547149658, q_loss2: -4.045106410980225, p_loss: 2.8022799491882324, mean_rew: -0.34527312105369107, var_rew: 0.19072655926936835
Running avgs for agent 9: q_loss: 4.2304487228393555, q_loss2: -4.227252006530762, p_loss: 2.8575594425201416, mean_rew: -0.354330505180514, var_rew: 0.2034849177963697
steps: 674975, episodes: 27000, mean episode reward: -70.71918322228832, agent episode reward: [-6.831484255456913, -6.217819544868853, -6.153716278881383, -6.133667035209743, -6.076992736067188, -7.970997795399668, -7.779750675526755, -7.942808778380334, -7.5879191753177055, -8.02402694717978], time: 812.9
Running avgs for agent 0: q_loss: 2.5779073238372803, q_loss2: -2.5730819702148438, p_loss: 2.495551824569702, mean_rew: -0.2968320598052225, var_rew: 0.14561332634908752
Running avgs for agent 1: q_loss: 2.594156265258789, q_loss2: -2.5908803939819336, p_loss: 2.23648738861084, mean_rew: -0.28134720587142636, var_rew: 0.1411360640240448
Running avgs for agent 2: q_loss: 2.589552164077759, q_loss2: -2.5851035118103027, p_loss: 2.0247621536254883, mean_rew: -0.28203780115631827, var_rew: 0.14253492316198266
Running avgs for agent 3: q_loss: 2.5966153144836426, q_loss2: -2.5931315422058105, p_loss: 2.22526216506958, mean_rew: -0.28729506728540266, var_rew: 0.14301254999701865
Running avgs for agent 4: q_loss: 2.4427568912506104, q_loss2: -2.4374678134918213, p_loss: 2.337394952774048, mean_rew: -0.28694546769618895, var_rew: 0.13833596630685427
Running avgs for agent 5: q_loss: 4.041973114013672, q_loss2: -4.0404462814331055, p_loss: 2.990612268447876, mean_rew: -0.3655333315521233, var_rew: 0.18703732654496538
Running avgs for agent 6: q_loss: 4.334388732910156, q_loss2: -4.333652973175049, p_loss: 2.8081603050231934, mean_rew: -0.35030219963402726, var_rew: 0.19402535328341766
Running avgs for agent 7: q_loss: 4.285813808441162, q_loss2: -4.2837629318237305, p_loss: 2.930190324783325, mean_rew: -0.3644616839127821, var_rew: 0.19812105241079622
Running avgs for agent 8: q_loss: 4.081316947937012, q_loss2: -4.080216407775879, p_loss: 2.783249616622925, mean_rew: -0.3422417834961976, var_rew: 0.18642832171582427
Running avgs for agent 9: q_loss: 4.418255805969238, q_loss2: -4.415541172027588, p_loss: 2.8537790775299072, mean_rew: -0.3550292346060487, var_rew: 0.2039572476042245
steps: 699975, episodes: 28000, mean episode reward: -71.28650751666305, agent episode reward: [-7.060743442117461, -5.957147416437189, -6.5472231337227225, -6.1911052553746515, -6.186432330839963, -8.157170467400167, -7.653882434889274, -8.114705486290887, -7.583345874857134, -7.834751674733598], time: 1092.317
Running avgs for agent 0: q_loss: 2.610344171524048, q_loss2: -2.6020381450653076, p_loss: 2.4785075187683105, mean_rew: -0.2951858381001809, var_rew: 0.14247580753497033
Running avgs for agent 1: q_loss: 2.6535961627960205, q_loss2: -2.6502573490142822, p_loss: 2.243898391723633, mean_rew: -0.28092399048446, var_rew: 0.13945955421254883
Running avgs for agent 2: q_loss: 2.650312900543213, q_loss2: -2.6461498737335205, p_loss: 2.0099072456359863, mean_rew: -0.28035503820531965, var_rew: 0.14090026089056074
Running avgs for agent 3: q_loss: 2.666062116622925, q_loss2: -2.66243314743042, p_loss: 2.209688425064087, mean_rew: -0.28491159368025243, var_rew: 0.14163395152228617
Running avgs for agent 4: q_loss: 2.5840470790863037, q_loss2: -2.576141119003296, p_loss: 2.318574905395508, mean_rew: -0.28752166160271303, var_rew: 0.1393942206885385
Running avgs for agent 5: q_loss: 4.209495544433594, q_loss2: -4.2079925537109375, p_loss: 2.9877870082855225, mean_rew: -0.36420747857324076, var_rew: 0.1872656572990589
Running avgs for agent 6: q_loss: 4.506148338317871, q_loss2: -4.505253314971924, p_loss: 2.811981439590454, mean_rew: -0.35029952290884947, var_rew: 0.19404109676874867
Running avgs for agent 7: q_loss: 4.378434658050537, q_loss2: -4.3767900466918945, p_loss: 2.9110846519470215, mean_rew: -0.36196851780515976, var_rew: 0.1955250478587908
Running avgs for agent 8: q_loss: 4.288763999938965, q_loss2: -4.287530422210693, p_loss: 2.795123338699341, mean_rew: -0.34297495462828276, var_rew: 0.18790778378515144
Running avgs for agent 9: q_loss: 4.518306732177734, q_loss2: -4.516948223114014, p_loss: 2.851789712905884, mean_rew: -0.3531778734940707, var_rew: 0.20145026879403902
steps: 724975, episodes: 29000, mean episode reward: -72.0146062942079, agent episode reward: [-7.152588590727505, -6.0914938160440215, -6.762703509143522, -6.243999172029565, -6.218395452826068, -8.097041386999193, -7.822644122448555, -8.098147045360015, -7.649370778898679, -7.878222419730781], time: 2010.04
Running avgs for agent 0: q_loss: 2.6968681812286377, q_loss2: -2.6887505054473877, p_loss: 2.4877567291259766, mean_rew: -0.2946891914871803, var_rew: 0.14193361958810158
Running avgs for agent 1: q_loss: 2.7619447708129883, q_loss2: -2.757798194885254, p_loss: 2.2572617530822754, mean_rew: -0.2774674529310348, var_rew: 0.13941460028812344
Running avgs for agent 2: q_loss: 2.763753652572632, q_loss2: -2.7589008808135986, p_loss: 2.0006771087646484, mean_rew: -0.27962800852490316, var_rew: 0.14101620139548693
Running avgs for agent 3: q_loss: 2.8268496990203857, q_loss2: -2.822385787963867, p_loss: 2.2070472240448, mean_rew: -0.28454341494621677, var_rew: 0.1432728946496706
Running avgs for agent 4: q_loss: 2.614276170730591, q_loss2: -2.6083052158355713, p_loss: 2.281818389892578, mean_rew: -0.2847708266492154, var_rew: 0.13702754507614942
Running avgs for agent 5: q_loss: 4.250722885131836, q_loss2: -4.248456954956055, p_loss: 2.978367567062378, mean_rew: -0.36105044579685236, var_rew: 0.18338564332818302
Running avgs for agent 6: q_loss: 4.599393367767334, q_loss2: -4.59842586517334, p_loss: 2.8003740310668945, mean_rew: -0.3471767163436358, var_rew: 0.19163557797345146
Running avgs for agent 7: q_loss: 4.500316143035889, q_loss2: -4.499172687530518, p_loss: 2.9128637313842773, mean_rew: -0.36043953663883743, var_rew: 0.19407594806643838
Running avgs for agent 8: q_loss: 4.398620128631592, q_loss2: -4.395577907562256, p_loss: 2.7875454425811768, mean_rew: -0.33995545406899713, var_rew: 0.18596123982721577
Running avgs for agent 9: q_loss: 4.661751747131348, q_loss2: -4.660035133361816, p_loss: 2.844609260559082, mean_rew: -0.3510854949371251, var_rew: 0.200328045811925
steps: 749975, episodes: 30000, mean episode reward: -71.20042844367825, agent episode reward: [-6.800959538690369, -6.182593343181512, -6.3254779331564865, -5.984519999159302, -6.341227488272434, -8.494743418368204, -7.792609787087688, -7.879057384317155, -7.762908972526883, -7.636330578918214], time: 866.563
Running avgs for agent 0: q_loss: 2.789979934692383, q_loss2: -2.78426456451416, p_loss: 2.5036258697509766, mean_rew: -0.29486320779064223, var_rew: 0.14176747414215005
Running avgs for agent 1: q_loss: 2.8383054733276367, q_loss2: -2.834099769592285, p_loss: 2.248997449874878, mean_rew: -0.27672241847848733, var_rew: 0.13849303800988752
Running avgs for agent 2: q_loss: 2.854769468307495, q_loss2: -2.850250482559204, p_loss: 1.9776592254638672, mean_rew: -0.27905204697225194, var_rew: 0.1404333164604434
Running avgs for agent 3: q_loss: 2.9007315635681152, q_loss2: -2.8954336643218994, p_loss: 2.194399356842041, mean_rew: -0.28314102048311784, var_rew: 0.14194066373799552
Running avgs for agent 4: q_loss: 2.6842713356018066, q_loss2: -2.68072509765625, p_loss: 2.2409181594848633, mean_rew: -0.283484282949165, var_rew: 0.13627129511944214
Running avgs for agent 5: q_loss: 4.493522644042969, q_loss2: -4.492433547973633, p_loss: 3.0052318572998047, mean_rew: -0.36326739887199794, var_rew: 0.18612394721184966
Running avgs for agent 6: q_loss: 4.7418622970581055, q_loss2: -4.740994930267334, p_loss: 2.8098154067993164, mean_rew: -0.3465364822112225, var_rew: 0.19090029814539053
Running avgs for agent 7: q_loss: 4.655450344085693, q_loss2: -4.653352737426758, p_loss: 2.911750316619873, mean_rew: -0.3594446216344254, var_rew: 0.1935344511864782
Running avgs for agent 8: q_loss: 4.564292907714844, q_loss2: -4.563359260559082, p_loss: 2.787997007369995, mean_rew: -0.3399188567122316, var_rew: 0.1863065459446752
Running avgs for agent 9: q_loss: 4.753182888031006, q_loss2: -4.751303672790527, p_loss: 2.8229165077209473, mean_rew: -0.3493241451795052, var_rew: 0.19763526169987466
steps: 774975, episodes: 31000, mean episode reward: -72.53533410369478, agent episode reward: [-6.597900937172054, -6.261468802198319, -6.873626123376519, -6.264612557843393, -6.44284192977159, -9.171449276120489, -7.504999268552705, -8.163710270645897, -7.486547946604072, -7.7681769914097405], time: 1039.736
Running avgs for agent 0: q_loss: 2.925947904586792, q_loss2: -2.9211983680725098, p_loss: 2.510374069213867, mean_rew: -0.2949757633502966, var_rew: 0.14284981386538792
Running avgs for agent 1: q_loss: 2.988518476486206, q_loss2: -2.984213352203369, p_loss: 2.238346576690674, mean_rew: -0.2764669231511078, var_rew: 0.13979236276226975
Running avgs for agent 2: q_loss: 2.9274070262908936, q_loss2: -2.920835494995117, p_loss: 1.954559326171875, mean_rew: -0.27912062653319913, var_rew: 0.1391812684202955
Running avgs for agent 3: q_loss: 2.9784138202667236, q_loss2: -2.973271369934082, p_loss: 2.181467294692993, mean_rew: -0.2818552513396378, var_rew: 0.1409698739379377
Running avgs for agent 4: q_loss: 2.7604427337646484, q_loss2: -2.7543270587921143, p_loss: 2.2090566158294678, mean_rew: -0.2817430335894371, var_rew: 0.13521702195372512
Running avgs for agent 5: q_loss: 4.558993339538574, q_loss2: -4.557950496673584, p_loss: 2.9967739582061768, mean_rew: -0.36218528750319695, var_rew: 0.18325972677360303
Running avgs for agent 6: q_loss: 4.851367950439453, q_loss2: -4.850200176239014, p_loss: 2.814771890640259, mean_rew: -0.34406484045865426, var_rew: 0.18918863036855876
Running avgs for agent 7: q_loss: 4.7576799392700195, q_loss2: -4.755984783172607, p_loss: 2.8908920288085938, mean_rew: -0.35717944112974503, var_rew: 0.19159747176501357
Running avgs for agent 8: q_loss: 4.671908378601074, q_loss2: -4.670892715454102, p_loss: 2.7821450233459473, mean_rew: -0.3379433397109453, var_rew: 0.18467398715936906
Running avgs for agent 9: q_loss: 4.8872528076171875, q_loss2: -4.885425567626953, p_loss: 2.788008213043213, mean_rew: -0.3481809646198998, var_rew: 0.19646206325130225
steps: 799975, episodes: 32000, mean episode reward: -73.3519703502164, agent episode reward: [-6.437492449176369, -6.403079275195114, -6.798752791566772, -6.239724121767512, -6.479621314104339, -9.228433421057206, -7.920755969136379, -8.134091110910726, -7.769279686957479, -7.940740210344505], time: 1109.074
Running avgs for agent 0: q_loss: 2.9417724609375, q_loss2: -2.9356679916381836, p_loss: 2.5092992782592773, mean_rew: -0.29232246780194454, var_rew: 0.1398799490736912
Running avgs for agent 1: q_loss: 2.984600782394409, q_loss2: -2.9814653396606445, p_loss: 2.21008038520813, mean_rew: -0.2741407099867758, var_rew: 0.13663673592608372
Running avgs for agent 2: q_loss: 3.050631523132324, q_loss2: -3.0467193126678467, p_loss: 1.9411696195602417, mean_rew: -0.2792095178769121, var_rew: 0.13989302551542943
Running avgs for agent 3: q_loss: 3.085275650024414, q_loss2: -3.0813674926757812, p_loss: 2.1766130924224854, mean_rew: -0.28144203826064573, var_rew: 0.1410570747833788
Running avgs for agent 4: q_loss: 2.8449180126190186, q_loss2: -2.839566707611084, p_loss: 2.194516181945801, mean_rew: -0.28087606852912506, var_rew: 0.13476665420918285
Running avgs for agent 5: q_loss: 4.605474948883057, q_loss2: -4.603748321533203, p_loss: 2.9754931926727295, mean_rew: -0.3592681306539305, var_rew: 0.17994909265900322
Running avgs for agent 6: q_loss: 5.054593563079834, q_loss2: -5.05357551574707, p_loss: 2.8340532779693604, mean_rew: -0.3456011259294694, var_rew: 0.19021452703691538
Running avgs for agent 7: q_loss: 4.901782512664795, q_loss2: -4.899721145629883, p_loss: 2.8828861713409424, mean_rew: -0.35634327777233266, var_rew: 0.19077808620358166
Running avgs for agent 8: q_loss: 4.815269947052002, q_loss2: -4.813071250915527, p_loss: 2.782975196838379, mean_rew: -0.3365987608598842, var_rew: 0.18402932771813407
Running avgs for agent 9: q_loss: 4.997292518615723, q_loss2: -4.995386600494385, p_loss: 2.7655715942382812, mean_rew: -0.34591347640443576, var_rew: 0.19466013932817125
steps: 824975, episodes: 33000, mean episode reward: -71.09308855224523, agent episode reward: [-6.3881543403317025, -6.187936511355656, -6.648378168820972, -6.168126832385438, -6.3923865352228155, -8.023718665590025, -7.505237036374178, -8.162011039716566, -7.625232653058559, -7.991906769389309], time: 2104.806
Running avgs for agent 0: q_loss: 3.039661169052124, q_loss2: -3.0349032878875732, p_loss: 2.5284314155578613, mean_rew: -0.2925120972335192, var_rew: 0.1397498669783757
Running avgs for agent 1: q_loss: 3.088843822479248, q_loss2: -3.085373878479004, p_loss: 2.2044572830200195, mean_rew: -0.27435348669990706, var_rew: 0.13664186840097398
Running avgs for agent 2: q_loss: 3.1390180587768555, q_loss2: -3.1356732845306396, p_loss: 1.9187443256378174, mean_rew: -0.2797651221733255, var_rew: 0.13941257412318944
Running avgs for agent 3: q_loss: 3.1412618160247803, q_loss2: -3.1348252296447754, p_loss: 2.163038969039917, mean_rew: -0.27980589880772094, var_rew: 0.13924862263984833
Running avgs for agent 4: q_loss: 2.940265655517578, q_loss2: -2.936657667160034, p_loss: 2.1850054264068604, mean_rew: -0.2810580516816598, var_rew: 0.1347649275106175
Running avgs for agent 5: q_loss: 4.748532772064209, q_loss2: -4.7470703125, p_loss: 2.942775249481201, mean_rew: -0.3594712878826648, var_rew: 0.1797462614888637
Running avgs for agent 6: q_loss: 5.1948747634887695, q_loss2: -5.193930149078369, p_loss: 2.8323843479156494, mean_rew: -0.34350696433543865, var_rew: 0.18941925766573092
Running avgs for agent 7: q_loss: 5.026439666748047, q_loss2: -5.025092601776123, p_loss: 2.883164167404175, mean_rew: -0.35665137382529866, var_rew: 0.1896877378445868
Running avgs for agent 8: q_loss: 4.894071578979492, q_loss2: -4.892916202545166, p_loss: 2.7740185260772705, mean_rew: -0.3347537519376008, var_rew: 0.1819030087262189
Running avgs for agent 9: q_loss: 5.171000003814697, q_loss2: -5.168857574462891, p_loss: 2.7494940757751465, mean_rew: -0.3461332346658636, var_rew: 0.19469719150186335


