python train_vanila.py --scenario simple_push --num-adversaries 5
2018-10-23 10:16:18.578466: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
<tf.Variable 'agent_0/q_func/fully_connected/weights:0' shape=(345, 64) dtype=float32_ref> Tensor("agent_0/gradients/agent_0/q_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(345, 64), dtype=float32)
<tf.Variable 'agent_0/q_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_0/gradients/agent_0/q_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_0/q_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_0/gradients/agent_0/q_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_0/q_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_0/gradients/agent_0/q_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_0/q_func/fully_connected_2/weights:0' shape=(64, 1) dtype=float32_ref> Tensor("agent_0/gradients/agent_0/q_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 1), dtype=float32)
<tf.Variable 'agent_0/q_func/fully_connected_2/biases:0' shape=(1,) dtype=float32_ref> Tensor("agent_0/gradients/agent_0/q_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(1,), dtype=float32)
<tf.Variable 'agent_0/p_func/fully_connected/weights:0' shape=(24, 64) dtype=float32_ref> Tensor("agent_0_1/gradients/agent_0_1/p_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(24, 64), dtype=float32)
<tf.Variable 'agent_0/p_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_0_1/gradients/agent_0_1/p_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_0/p_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_0_1/gradients/agent_0_1/p_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_0/p_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_0_1/gradients/agent_0_1/p_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_0/p_func/fully_connected_2/weights:0' shape=(64, 5) dtype=float32_ref> Tensor("agent_0_1/gradients/agent_0_1/p_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 5), dtype=float32)
<tf.Variable 'agent_0/p_func/fully_connected_2/biases:0' shape=(5,) dtype=float32_ref> Tensor("agent_0_1/gradients/agent_0_1/p_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(5,), dtype=float32)
<tf.Variable 'agent_1/q_func/fully_connected/weights:0' shape=(345, 64) dtype=float32_ref> Tensor("agent_1/gradients/agent_1/q_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(345, 64), dtype=float32)
<tf.Variable 'agent_1/q_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_1/gradients/agent_1/q_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_1/q_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_1/gradients/agent_1/q_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_1/q_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_1/gradients/agent_1/q_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_1/q_func/fully_connected_2/weights:0' shape=(64, 1) dtype=float32_ref> Tensor("agent_1/gradients/agent_1/q_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 1), dtype=float32)
<tf.Variable 'agent_1/q_func/fully_connected_2/biases:0' shape=(1,) dtype=float32_ref> Tensor("agent_1/gradients/agent_1/q_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(1,), dtype=float32)
<tf.Variable 'agent_1/p_func/fully_connected/weights:0' shape=(24, 64) dtype=float32_ref> Tensor("agent_1_1/gradients/agent_1_1/p_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(24, 64), dtype=float32)
<tf.Variable 'agent_1/p_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_1_1/gradients/agent_1_1/p_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_1/p_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_1_1/gradients/agent_1_1/p_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_1/p_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_1_1/gradients/agent_1_1/p_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_1/p_func/fully_connected_2/weights:0' shape=(64, 5) dtype=float32_ref> Tensor("agent_1_1/gradients/agent_1_1/p_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 5), dtype=float32)
<tf.Variable 'agent_1/p_func/fully_connected_2/biases:0' shape=(5,) dtype=float32_ref> Tensor("agent_1_1/gradients/agent_1_1/p_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(5,), dtype=float32)
<tf.Variable 'agent_2/q_func/fully_connected/weights:0' shape=(345, 64) dtype=float32_ref> Tensor("agent_2/gradients/agent_2/q_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(345, 64), dtype=float32)
<tf.Variable 'agent_2/q_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_2/gradients/agent_2/q_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_2/q_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_2/gradients/agent_2/q_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_2/q_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_2/gradients/agent_2/q_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_2/q_func/fully_connected_2/weights:0' shape=(64, 1) dtype=float32_ref> Tensor("agent_2/gradients/agent_2/q_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 1), dtype=float32)
<tf.Variable 'agent_2/q_func/fully_connected_2/biases:0' shape=(1,) dtype=float32_ref> Tensor("agent_2/gradients/agent_2/q_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(1,), dtype=float32)
<tf.Variable 'agent_2/p_func/fully_connected/weights:0' shape=(24, 64) dtype=float32_ref> Tensor("agent_2_1/gradients/agent_2_1/p_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(24, 64), dtype=float32)
<tf.Variable 'agent_2/p_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_2_1/gradients/agent_2_1/p_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_2/p_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_2_1/gradients/agent_2_1/p_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_2/p_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_2_1/gradients/agent_2_1/p_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_2/p_func/fully_connected_2/weights:0' shape=(64, 5) dtype=float32_ref> Tensor("agent_2_1/gradients/agent_2_1/p_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 5), dtype=float32)
<tf.Variable 'agent_2/p_func/fully_connected_2/biases:0' shape=(5,) dtype=float32_ref> Tensor("agent_2_1/gradients/agent_2_1/p_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(5,), dtype=float32)
<tf.Variable 'agent_3/q_func/fully_connected/weights:0' shape=(345, 64) dtype=float32_ref> Tensor("agent_3/gradients/agent_3/q_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(345, 64), dtype=float32)
<tf.Variable 'agent_3/q_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_3/gradients/agent_3/q_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_3/q_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_3/gradients/agent_3/q_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_3/q_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_3/gradients/agent_3/q_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_3/q_func/fully_connected_2/weights:0' shape=(64, 1) dtype=float32_ref> Tensor("agent_3/gradients/agent_3/q_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 1), dtype=float32)
<tf.Variable 'agent_3/q_func/fully_connected_2/biases:0' shape=(1,) dtype=float32_ref> Tensor("agent_3/gradients/agent_3/q_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(1,), dtype=float32)
<tf.Variable 'agent_3/p_func/fully_connected/weights:0' shape=(24, 64) dtype=float32_ref> Tensor("agent_3_1/gradients/agent_3_1/p_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(24, 64), dtype=float32)
<tf.Variable 'agent_3/p_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_3_1/gradients/agent_3_1/p_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_3/p_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_3_1/gradients/agent_3_1/p_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_3/p_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_3_1/gradients/agent_3_1/p_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_3/p_func/fully_connected_2/weights:0' shape=(64, 5) dtype=float32_ref> Tensor("agent_3_1/gradients/agent_3_1/p_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 5), dtype=float32)
<tf.Variable 'agent_3/p_func/fully_connected_2/biases:0' shape=(5,) dtype=float32_ref> Tensor("agent_3_1/gradients/agent_3_1/p_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(5,), dtype=float32)
<tf.Variable 'agent_4/q_func/fully_connected/weights:0' shape=(345, 64) dtype=float32_ref> Tensor("agent_4/gradients/agent_4/q_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(345, 64), dtype=float32)
<tf.Variable 'agent_4/q_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_4/gradients/agent_4/q_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_4/q_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_4/gradients/agent_4/q_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_4/q_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_4/gradients/agent_4/q_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_4/q_func/fully_connected_2/weights:0' shape=(64, 1) dtype=float32_ref> Tensor("agent_4/gradients/agent_4/q_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 1), dtype=float32)
<tf.Variable 'agent_4/q_func/fully_connected_2/biases:0' shape=(1,) dtype=float32_ref> Tensor("agent_4/gradients/agent_4/q_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(1,), dtype=float32)
<tf.Variable 'agent_4/p_func/fully_connected/weights:0' shape=(24, 64) dtype=float32_ref> Tensor("agent_4_1/gradients/agent_4_1/p_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(24, 64), dtype=float32)
<tf.Variable 'agent_4/p_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_4_1/gradients/agent_4_1/p_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_4/p_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_4_1/gradients/agent_4_1/p_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_4/p_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_4_1/gradients/agent_4_1/p_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_4/p_func/fully_connected_2/weights:0' shape=(64, 5) dtype=float32_ref> Tensor("agent_4_1/gradients/agent_4_1/p_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 5), dtype=float32)
<tf.Variable 'agent_4/p_func/fully_connected_2/biases:0' shape=(5,) dtype=float32_ref> Tensor("agent_4_1/gradients/agent_4_1/p_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(5,), dtype=float32)
<tf.Variable 'agent_5/q_func/fully_connected/weights:0' shape=(345, 64) dtype=float32_ref> Tensor("agent_5/gradients/agent_5/q_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(345, 64), dtype=float32)
<tf.Variable 'agent_5/q_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_5/gradients/agent_5/q_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_5/q_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_5/gradients/agent_5/q_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_5/q_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_5/gradients/agent_5/q_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_5/q_func/fully_connected_2/weights:0' shape=(64, 1) dtype=float32_ref> Tensor("agent_5/gradients/agent_5/q_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 1), dtype=float32)
<tf.Variable 'agent_5/q_func/fully_connected_2/biases:0' shape=(1,) dtype=float32_ref> Tensor("agent_5/gradients/agent_5/q_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(1,), dtype=float32)
<tf.Variable 'agent_5/p_func/fully_connected/weights:0' shape=(35, 64) dtype=float32_ref> Tensor("agent_5_1/gradients/agent_5_1/p_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(35, 64), dtype=float32)
<tf.Variable 'agent_5/p_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_5_1/gradients/agent_5_1/p_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_5/p_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_5_1/gradients/agent_5_1/p_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_5/p_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_5_1/gradients/agent_5_1/p_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_5/p_func/fully_connected_2/weights:0' shape=(64, 5) dtype=float32_ref> Tensor("agent_5_1/gradients/agent_5_1/p_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 5), dtype=float32)
<tf.Variable 'agent_5/p_func/fully_connected_2/biases:0' shape=(5,) dtype=float32_ref> Tensor("agent_5_1/gradients/agent_5_1/p_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(5,), dtype=float32)
<tf.Variable 'agent_6/q_func/fully_connected/weights:0' shape=(345, 64) dtype=float32_ref> Tensor("agent_6/gradients/agent_6/q_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(345, 64), dtype=float32)
<tf.Variable 'agent_6/q_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_6/gradients/agent_6/q_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_6/q_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_6/gradients/agent_6/q_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_6/q_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_6/gradients/agent_6/q_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_6/q_func/fully_connected_2/weights:0' shape=(64, 1) dtype=float32_ref> Tensor("agent_6/gradients/agent_6/q_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 1), dtype=float32)
<tf.Variable 'agent_6/q_func/fully_connected_2/biases:0' shape=(1,) dtype=float32_ref> Tensor("agent_6/gradients/agent_6/q_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(1,), dtype=float32)
<tf.Variable 'agent_6/p_func/fully_connected/weights:0' shape=(35, 64) dtype=float32_ref> Tensor("agent_6_1/gradients/agent_6_1/p_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(35, 64), dtype=float32)
<tf.Variable 'agent_6/p_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_6_1/gradients/agent_6_1/p_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_6/p_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_6_1/gradients/agent_6_1/p_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_6/p_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_6_1/gradients/agent_6_1/p_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_6/p_func/fully_connected_2/weights:0' shape=(64, 5) dtype=float32_ref> Tensor("agent_6_1/gradients/agent_6_1/p_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 5), dtype=float32)
<tf.Variable 'agent_6/p_func/fully_connected_2/biases:0' shape=(5,) dtype=float32_ref> Tensor("agent_6_1/gradients/agent_6_1/p_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(5,), dtype=float32)
<tf.Variable 'agent_7/q_func/fully_connected/weights:0' shape=(345, 64) dtype=float32_ref> Tensor("agent_7/gradients/agent_7/q_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(345, 64), dtype=float32)
<tf.Variable 'agent_7/q_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_7/gradients/agent_7/q_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_7/q_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_7/gradients/agent_7/q_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_7/q_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_7/gradients/agent_7/q_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_7/q_func/fully_connected_2/weights:0' shape=(64, 1) dtype=float32_ref> Tensor("agent_7/gradients/agent_7/q_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 1), dtype=float32)
<tf.Variable 'agent_7/q_func/fully_connected_2/biases:0' shape=(1,) dtype=float32_ref> Tensor("agent_7/gradients/agent_7/q_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(1,), dtype=float32)
<tf.Variable 'agent_7/p_func/fully_connected/weights:0' shape=(35, 64) dtype=float32_ref> Tensor("agent_7_1/gradients/agent_7_1/p_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(35, 64), dtype=float32)
<tf.Variable 'agent_7/p_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_7_1/gradients/agent_7_1/p_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_7/p_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_7_1/gradients/agent_7_1/p_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_7/p_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_7_1/gradients/agent_7_1/p_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_7/p_func/fully_connected_2/weights:0' shape=(64, 5) dtype=float32_ref> Tensor("agent_7_1/gradients/agent_7_1/p_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 5), dtype=float32)
<tf.Variable 'agent_7/p_func/fully_connected_2/biases:0' shape=(5,) dtype=float32_ref> Tensor("agent_7_1/gradients/agent_7_1/p_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(5,), dtype=float32)
<tf.Variable 'agent_8/q_func/fully_connected/weights:0' shape=(345, 64) dtype=float32_ref> Tensor("agent_8/gradients/agent_8/q_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(345, 64), dtype=float32)
<tf.Variable 'agent_8/q_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_8/gradients/agent_8/q_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_8/q_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_8/gradients/agent_8/q_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_8/q_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_8/gradients/agent_8/q_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_8/q_func/fully_connected_2/weights:0' shape=(64, 1) dtype=float32_ref> Tensor("agent_8/gradients/agent_8/q_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 1), dtype=float32)
<tf.Variable 'agent_8/q_func/fully_connected_2/biases:0' shape=(1,) dtype=float32_ref> Tensor("agent_8/gradients/agent_8/q_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(1,), dtype=float32)
<tf.Variable 'agent_8/p_func/fully_connected/weights:0' shape=(35, 64) dtype=float32_ref> Tensor("agent_8_1/gradients/agent_8_1/p_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(35, 64), dtype=float32)
<tf.Variable 'agent_8/p_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_8_1/gradients/agent_8_1/p_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_8/p_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_8_1/gradients/agent_8_1/p_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_8/p_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_8_1/gradients/agent_8_1/p_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_8/p_func/fully_connected_2/weights:0' shape=(64, 5) dtype=float32_ref> Tensor("agent_8_1/gradients/agent_8_1/p_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 5), dtype=float32)
<tf.Variable 'agent_8/p_func/fully_connected_2/biases:0' shape=(5,) dtype=float32_ref> Tensor("agent_8_1/gradients/agent_8_1/p_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(5,), dtype=float32)
<tf.Variable 'agent_9/q_func/fully_connected/weights:0' shape=(345, 64) dtype=float32_ref> Tensor("agent_9/gradients/agent_9/q_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(345, 64), dtype=float32)
<tf.Variable 'agent_9/q_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_9/gradients/agent_9/q_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_9/q_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_9/gradients/agent_9/q_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_9/q_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_9/gradients/agent_9/q_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_9/q_func/fully_connected_2/weights:0' shape=(64, 1) dtype=float32_ref> Tensor("agent_9/gradients/agent_9/q_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 1), dtype=float32)
<tf.Variable 'agent_9/q_func/fully_connected_2/biases:0' shape=(1,) dtype=float32_ref> Tensor("agent_9/gradients/agent_9/q_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(1,), dtype=float32)
<tf.Variable 'agent_9/p_func/fully_connected/weights:0' shape=(35, 64) dtype=float32_ref> Tensor("agent_9_1/gradients/agent_9_1/p_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(35, 64), dtype=float32)
<tf.Variable 'agent_9/p_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_9_1/gradients/agent_9_1/p_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_9/p_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_9_1/gradients/agent_9_1/p_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_9/p_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_9_1/gradients/agent_9_1/p_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_9/p_func/fully_connected_2/weights:0' shape=(64, 5) dtype=float32_ref> Tensor("agent_9_1/gradients/agent_9_1/p_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 5), dtype=float32)
<tf.Variable 'agent_9/p_func/fully_connected_2/biases:0' shape=(5,) dtype=float32_ref> Tensor("agent_9_1/gradients/agent_9_1/p_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(5,), dtype=float32)
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -211.80948664100993, agent episode reward: [-14.369962488241596, -14.570586958263048, -16.501828251578534, -16.973295259774837, -14.544761526696814, -25.8193410337152, -27.9939601893173, -25.051701518354033, -27.470566631612968, -28.513482783455608], time: 118.471
/anaconda2/envs/py36/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/anaconda2/envs/py36/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, var_rew: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, var_rew: nan
Running avgs for agent 2: q_loss: nan, p_loss: nan, mean_rew: nan, var_rew: nan
Running avgs for agent 3: q_loss: nan, p_loss: nan, mean_rew: nan, var_rew: nan
Running avgs for agent 4: q_loss: nan, p_loss: nan, mean_rew: nan, var_rew: nan
Running avgs for agent 5: q_loss: nan, p_loss: nan, mean_rew: nan, var_rew: nan
Running avgs for agent 6: q_loss: nan, p_loss: nan, mean_rew: nan, var_rew: nan
Running avgs for agent 7: q_loss: nan, p_loss: nan, mean_rew: nan, var_rew: nan
Running avgs for agent 8: q_loss: nan, p_loss: nan, mean_rew: nan, var_rew: nan
Running avgs for agent 9: q_loss: nan, p_loss: nan, mean_rew: nan, var_rew: nan
steps: 49975, episodes: 2000, mean episode reward: -143.09908740812529, agent episode reward: [-11.835695972914802, -11.281962669098354, -11.30620459725061, -11.62443021100458, -16.44188589360777, -16.49310271975083, -15.53445348961653, -18.979313000669862, -13.701360431749025, -15.90067842246294], time: 297.674
Running avgs for agent 0: q_loss: 0.07866751402616501, p_loss: 0.5245928168296814, mean_rew: -0.5705306683262062, var_rew: 0.38140739454399153
Running avgs for agent 1: q_loss: 0.08335316181182861, p_loss: 0.8153512477874756, mean_rew: -0.5582140912353837, var_rew: 0.3290638508021483
Running avgs for agent 2: q_loss: 0.08780931681394577, p_loss: 0.9517333507537842, mean_rew: -0.6121120233802699, var_rew: 0.4570301620953466
Running avgs for agent 3: q_loss: 0.10069169849157333, p_loss: 0.6060928702354431, mean_rew: -0.6289820561197907, var_rew: 0.44109138579652274
Running avgs for agent 4: q_loss: 0.07235008478164673, p_loss: 0.8899815082550049, mean_rew: -0.6598509375080259, var_rew: 0.46665083327627227
Running avgs for agent 5: q_loss: 0.045984573662281036, p_loss: 1.300724744796753, mean_rew: -0.9930162733936394, var_rew: 0.34401211318992114
Running avgs for agent 6: q_loss: 0.04414907842874527, p_loss: 1.7202385663986206, mean_rew: -1.0391323940687947, var_rew: 0.38939902352534356
Running avgs for agent 7: q_loss: 0.036871425807476044, p_loss: 1.560723900794983, mean_rew: -1.0317740585446131, var_rew: 0.48383363130976426
Running avgs for agent 8: q_loss: 0.03829565271735191, p_loss: 1.1190440654754639, mean_rew: -0.9877750829550028, var_rew: 0.338651969946759
Running avgs for agent 9: q_loss: 0.0554950088262558, p_loss: 1.5659902095794678, mean_rew: -1.056513150856248, var_rew: 0.37382374819122544
steps: 74975, episodes: 3000, mean episode reward: -78.72164129409444, agent episode reward: [-7.6838747561539185, -7.9932342619504215, -8.101214537860564, -8.04455292399595, -7.711359634735671, -7.959370127377619, -7.759452925905131, -7.741430499093401, -7.834980323376932, -7.892171303644831], time: 284.658
Running avgs for agent 0: q_loss: 0.043372992426157, p_loss: 1.1505054235458374, mean_rew: -0.48383729695634087, var_rew: 0.2876414747825012
Running avgs for agent 1: q_loss: 0.04529097303748131, p_loss: 1.3866057395935059, mean_rew: -0.48335186021191473, var_rew: 0.2579971318266263
Running avgs for agent 2: q_loss: 0.054744452238082886, p_loss: 1.4736011028289795, mean_rew: -0.5153598686048827, var_rew: 0.3348088351684861
Running avgs for agent 3: q_loss: 0.044405244290828705, p_loss: 1.3176995515823364, mean_rew: -0.5281076206929618, var_rew: 0.32922952566665853
Running avgs for agent 4: q_loss: 0.050975557416677475, p_loss: 1.5647348165512085, mean_rew: -0.5614852161156066, var_rew: 0.3884195683787674
Running avgs for agent 5: q_loss: 0.011558589525520802, p_loss: 1.987453579902649, mean_rew: -0.7492560445412869, var_rew: 0.38651204294639485
Running avgs for agent 6: q_loss: 0.02434644289314747, p_loss: 2.330644130706787, mean_rew: -0.7645509084623183, var_rew: 0.4238721453657388
Running avgs for agent 7: q_loss: 0.017322944477200508, p_loss: 2.20839786529541, mean_rew: -0.7736040625619321, var_rew: 0.5036397504898166
Running avgs for agent 8: q_loss: 0.009780104272067547, p_loss: 1.8400781154632568, mean_rew: -0.72971264945928, var_rew: 0.37335436603668654
Running avgs for agent 9: q_loss: 0.010837449692189693, p_loss: 2.2370245456695557, mean_rew: -0.7796195303614122, var_rew: 0.42197941927874455
steps: 99975, episodes: 4000, mean episode reward: -73.47580160403503, agent episode reward: [-7.138718035273978, -6.86109832193012, -7.2494366777019374, -7.1399871690240095, -6.9561589396421954, -7.920606003725589, -7.6570692652485866, -7.602056932491324, -7.537178884545763, -7.41349137445154], time: 283.295
Running avgs for agent 0: q_loss: 0.0616612434387207, p_loss: 1.5960566997528076, mean_rew: -0.42920632410924586, var_rew: 0.24953829260620558
Running avgs for agent 1: q_loss: 0.06351063400506973, p_loss: 1.7599760293960571, mean_rew: -0.4285334430876582, var_rew: 0.22657517545412878
Running avgs for agent 2: q_loss: 0.06873153895139694, p_loss: 1.8406139612197876, mean_rew: -0.45480942592720236, var_rew: 0.2861486329761799
Running avgs for agent 3: q_loss: 0.06436555087566376, p_loss: 1.798019289970398, mean_rew: -0.4619642797868387, var_rew: 0.2786281175608866
Running avgs for agent 4: q_loss: 0.06618901342153549, p_loss: 1.9784457683563232, mean_rew: -0.48426266395465084, var_rew: 0.3229021546856272
Running avgs for agent 5: q_loss: 0.01503487303853035, p_loss: 2.3476345539093018, mean_rew: -0.6234592630824886, var_rew: 0.36210559246954144
Running avgs for agent 6: q_loss: 0.012845691293478012, p_loss: 2.588822364807129, mean_rew: -0.6336131554925389, var_rew: 0.3915778611946321
Running avgs for agent 7: q_loss: 0.01200647559016943, p_loss: 2.5132813453674316, mean_rew: -0.6387422461231241, var_rew: 0.45047406554963165
Running avgs for agent 8: q_loss: 0.01504510547965765, p_loss: 2.2235443592071533, mean_rew: -0.6065076395209827, var_rew: 0.347884783875792
Running avgs for agent 9: q_loss: 0.014532580971717834, p_loss: 2.574815511703491, mean_rew: -0.6426259860737263, var_rew: 0.39117137934226986
steps: 124975, episodes: 5000, mean episode reward: -73.14115178068896, agent episode reward: [-6.989726236124198, -6.561171649098092, -6.883546209671881, -6.857219429128729, -6.778840778499067, -7.937692347391259, -7.812749819654066, -7.907032228131469, -7.776911112065818, -7.636261970924374], time: 286.879
Running avgs for agent 0: q_loss: 0.06785644590854645, p_loss: 1.8418022394180298, mean_rew: -0.3958978334627033, var_rew: 0.22458698296613333
Running avgs for agent 1: q_loss: 0.07340899109840393, p_loss: 1.928220272064209, mean_rew: -0.3919217110113822, var_rew: 0.20704704815324104
Running avgs for agent 2: q_loss: 0.07641824334859848, p_loss: 2.0242722034454346, mean_rew: -0.4156192440149166, var_rew: 0.25435423943164626
Running avgs for agent 3: q_loss: 0.06791616976261139, p_loss: 2.062148332595825, mean_rew: -0.420970245750781, var_rew: 0.2519128146579027
Running avgs for agent 4: q_loss: 0.07483365386724472, p_loss: 2.1984174251556396, mean_rew: -0.43664499176008814, var_rew: 0.2821322069384604
Running avgs for agent 5: q_loss: 0.015926772728562355, p_loss: 2.4765219688415527, mean_rew: -0.5532392792386167, var_rew: 0.32967037336565685
Running avgs for agent 6: q_loss: 0.025395197793841362, p_loss: 2.675065279006958, mean_rew: -0.5610542219641648, var_rew: 0.3562634794224791
Running avgs for agent 7: q_loss: 0.02498064748942852, p_loss: 2.6157264709472656, mean_rew: -0.563348527995981, var_rew: 0.3991559185334468
Running avgs for agent 8: q_loss: 0.017506107687950134, p_loss: 2.373535394668579, mean_rew: -0.5387195037885336, var_rew: 0.3197211568509375
Running avgs for agent 9: q_loss: 0.026240093633532524, p_loss: 2.679581642150879, mean_rew: -0.5673437424263967, var_rew: 0.35788322179814197
steps: 149975, episodes: 6000, mean episode reward: -72.71562693621095, agent episode reward: [-6.982806315281005, -6.893686558258221, -6.956438765062143, -7.037192939944511, -6.665752921602247, -7.772867519029667, -7.6026387515093425, -7.606411136481069, -7.69862471063234, -7.499207318410401], time: 295.931
Running avgs for agent 0: q_loss: 0.07453422993421555, p_loss: 1.9807056188583374, mean_rew: -0.3729155011231008, var_rew: 0.20506398770422532
Running avgs for agent 1: q_loss: 0.07067491859197617, p_loss: 2.029747724533081, mean_rew: -0.3707339483806458, var_rew: 0.19258704334243304
Running avgs for agent 2: q_loss: 0.07926124334335327, p_loss: 2.1014935970306396, mean_rew: -0.3894753451762814, var_rew: 0.23041374933197994
Running avgs for agent 3: q_loss: 0.07484985888004303, p_loss: 2.1974785327911377, mean_rew: -0.3958829051215896, var_rew: 0.22900977475035147
Running avgs for agent 4: q_loss: 0.08501606434583664, p_loss: 2.2930192947387695, mean_rew: -0.40634633418709437, var_rew: 0.25337248823951364
Running avgs for agent 5: q_loss: 0.02504655346274376, p_loss: 2.534369707107544, mean_rew: -0.5121319156997738, var_rew: 0.30779974145863026
Running avgs for agent 6: q_loss: 0.019994862377643585, p_loss: 2.6788175106048584, mean_rew: -0.5133393459802731, var_rew: 0.32921868914977065
Running avgs for agent 7: q_loss: 0.025794005021452904, p_loss: 2.6482903957366943, mean_rew: -0.5191699071235019, var_rew: 0.3645465307961628
Running avgs for agent 8: q_loss: 0.026731209829449654, p_loss: 2.4333226680755615, mean_rew: -0.49677956604556966, var_rew: 0.29787066753943486
Running avgs for agent 9: q_loss: 0.02040191926062107, p_loss: 2.684974193572998, mean_rew: -0.5174533537197256, var_rew: 0.32991728724469044
steps: 174975, episodes: 7000, mean episode reward: -72.19875025522477, agent episode reward: [-7.37947846705916, -7.0108211873434465, -6.496453883537907, -7.297665850464093, -6.542992897434549, -7.651374428462768, -7.433147767441112, -7.525129739330946, -7.27996757390471, -7.581718460246068], time: 289.842
Running avgs for agent 0: q_loss: 0.06822475790977478, p_loss: 2.1132633686065674, mean_rew: -0.3633115384296538, var_rew: 0.19316315872527037
Running avgs for agent 1: q_loss: 0.07504994422197342, p_loss: 2.0811331272125244, mean_rew: -0.35597790575096644, var_rew: 0.18168470441635418
Running avgs for agent 2: q_loss: 0.07918749749660492, p_loss: 2.158536195755005, mean_rew: -0.37122095289468543, var_rew: 0.21641882937905366
Running avgs for agent 3: q_loss: 0.07119034230709076, p_loss: 2.2711281776428223, mean_rew: -0.377557521442786, var_rew: 0.21166782887149652
Running avgs for agent 4: q_loss: 0.07752641290426254, p_loss: 2.3478829860687256, mean_rew: -0.38423472468985553, var_rew: 0.23485722742937434
Running avgs for agent 5: q_loss: 0.023053554818034172, p_loss: 2.5461456775665283, mean_rew: -0.47868832189098776, var_rew: 0.2910999962320754
Running avgs for agent 6: q_loss: 0.026866501197218895, p_loss: 2.6803970336914062, mean_rew: -0.4820469613354855, var_rew: 0.3082881189244741
Running avgs for agent 7: q_loss: 0.021943163126707077, p_loss: 2.6632938385009766, mean_rew: -0.48577701302027954, var_rew: 0.33912814468699354
Running avgs for agent 8: q_loss: 0.01928129605948925, p_loss: 2.449821949005127, mean_rew: -0.46836259531689983, var_rew: 0.2818047997230838
Running avgs for agent 9: q_loss: 0.021511135622859, p_loss: 2.6771974563598633, mean_rew: -0.4835507230222354, var_rew: 0.30657703181257123
steps: 199975, episodes: 8000, mean episode reward: -74.04526869626633, agent episode reward: [-7.3326314267396135, -7.163476125415422, -6.811082091369972, -7.381520418297574, -6.926000139569713, -7.473570458220116, -7.709219134311919, -7.922036856418535, -7.535748986476763, -7.789983059446696], time: 288.303
Running avgs for agent 0: q_loss: 0.07896919548511505, p_loss: 2.1943671703338623, mean_rew: -0.35340597453147826, var_rew: 0.1828775640966216
Running avgs for agent 1: q_loss: 0.061648521572351456, p_loss: 2.134805202484131, mean_rew: -0.34663979654590615, var_rew: 0.17306734961968964
Running avgs for agent 2: q_loss: 0.07160355895757675, p_loss: 2.2242696285247803, mean_rew: -0.35781881559559914, var_rew: 0.2064539245682707
Running avgs for agent 3: q_loss: 0.06116465851664543, p_loss: 2.3458950519561768, mean_rew: -0.3676548440509968, var_rew: 0.20020006426016687
Running avgs for agent 4: q_loss: 0.07603675872087479, p_loss: 2.367044448852539, mean_rew: -0.3681871100810825, var_rew: 0.22188975738232433
Running avgs for agent 5: q_loss: 0.025649840012192726, p_loss: 2.5583434104919434, mean_rew: -0.4555171447343314, var_rew: 0.2777007552749263
Running avgs for agent 6: q_loss: 0.025153953582048416, p_loss: 2.663846492767334, mean_rew: -0.45609255388020564, var_rew: 0.28967335636524366
Running avgs for agent 7: q_loss: 0.0218706913292408, p_loss: 2.6906991004943848, mean_rew: -0.4619286529395668, var_rew: 0.316984552023986
Running avgs for agent 8: q_loss: 0.023540373891592026, p_loss: 2.458930253982544, mean_rew: -0.4441581640095493, var_rew: 0.2672087279482953
Running avgs for agent 9: q_loss: 0.022045785561203957, p_loss: 2.69303297996521, mean_rew: -0.4623150660073045, var_rew: 0.292468673717351
steps: 224975, episodes: 9000, mean episode reward: -74.40945148272175, agent episode reward: [-7.05560243890361, -7.16303944736246, -6.9506695335329605, -7.472334245095833, -7.034189430092481, -7.526647820147597, -7.860204249296582, -7.900347187651632, -7.5152653615853655, -7.931151769053225], time: 288.172
Running avgs for agent 0: q_loss: 0.05578335374593735, p_loss: 2.27139949798584, mean_rew: -0.34529334390052907, var_rew: 0.17493773616265076
Running avgs for agent 1: q_loss: 0.06017916277050972, p_loss: 2.1847012042999268, mean_rew: -0.3372186705641477, var_rew: 0.16597577180200618
Running avgs for agent 2: q_loss: 0.07634837180376053, p_loss: 2.2422587871551514, mean_rew: -0.34613597426561726, var_rew: 0.19571887983308556
Running avgs for agent 3: q_loss: 0.06280025839805603, p_loss: 2.395576238632202, mean_rew: -0.3572606431436, var_rew: 0.18896584114308898
Running avgs for agent 4: q_loss: 0.07262177765369415, p_loss: 2.3956000804901123, mean_rew: -0.35881204633249614, var_rew: 0.21201584442999172
Running avgs for agent 5: q_loss: 0.01927354745566845, p_loss: 2.572514057159424, mean_rew: -0.4363744303161604, var_rew: 0.2647704805340004
Running avgs for agent 6: q_loss: 0.0240198764950037, p_loss: 2.658195734024048, mean_rew: -0.4394374536975706, var_rew: 0.27704956895960414
Running avgs for agent 7: q_loss: 0.031020019203424454, p_loss: 2.758758068084717, mean_rew: -0.4471671048425823, var_rew: 0.30118130934788223
Running avgs for agent 8: q_loss: 0.023600246757268906, p_loss: 2.48576021194458, mean_rew: -0.4291803466105821, var_rew: 0.25539085538260353
Running avgs for agent 9: q_loss: 0.024943968281149864, p_loss: 2.6954479217529297, mean_rew: -0.44400570752397334, var_rew: 0.2786132433927262
steps: 249975, episodes: 10000, mean episode reward: -72.4317416676689, agent episode reward: [-6.653842004842314, -6.668270801752888, -6.464188570371442, -7.206123930642616, -6.627985243940971, -7.688181310835472, -7.852220301071615, -7.944836436349625, -7.544531338472159, -7.781561729389801], time: 318.78
Running avgs for agent 0: q_loss: 0.06296203285455704, p_loss: 2.3278794288635254, mean_rew: -0.33941854665555704, var_rew: 0.16978147776746533
Running avgs for agent 1: q_loss: 0.06346122175455093, p_loss: 2.2518913745880127, mean_rew: -0.33411157110631995, var_rew: 0.16355743020823732
Running avgs for agent 2: q_loss: 0.06377127766609192, p_loss: 2.258607864379883, mean_rew: -0.3389602515361584, var_rew: 0.1901432848850205
Running avgs for agent 3: q_loss: 0.06375914812088013, p_loss: 2.4639523029327393, mean_rew: -0.35226591916856526, var_rew: 0.18221075932955993
Running avgs for agent 4: q_loss: 0.06405068933963776, p_loss: 2.4188649654388428, mean_rew: -0.34941563219935423, var_rew: 0.20105021734981476
Running avgs for agent 5: q_loss: 0.025060836225748062, p_loss: 2.5929951667785645, mean_rew: -0.4228541553211266, var_rew: 0.2558927882649136
Running avgs for agent 6: q_loss: 0.022542018443346024, p_loss: 2.654909610748291, mean_rew: -0.4242943747530911, var_rew: 0.2647508048778564
Running avgs for agent 7: q_loss: 0.01910572126507759, p_loss: 2.7950735092163086, mean_rew: -0.4314384882904956, var_rew: 0.286750268503286
Running avgs for agent 8: q_loss: 0.01875978522002697, p_loss: 2.5081112384796143, mean_rew: -0.4128619980021606, var_rew: 0.24545104995633987
Running avgs for agent 9: q_loss: 0.02248869091272354, p_loss: 2.716244697570801, mean_rew: -0.4316855437263805, var_rew: 0.2681457475164814
steps: 274975, episodes: 11000, mean episode reward: -72.05586485296048, agent episode reward: [-6.372506278233883, -6.587834091217969, -6.4917424799797425, -7.107581090157857, -6.239464951748434, -7.636345709888158, -7.794255387635086, -7.902938954968407, -8.17475795056402, -7.748437958566921], time: 329.769
Running avgs for agent 0: q_loss: 0.05501227453351021, p_loss: 2.320610284805298, mean_rew: -0.3297202568953418, var_rew: 0.16348742305361708
Running avgs for agent 1: q_loss: 0.05434652417898178, p_loss: 2.2907612323760986, mean_rew: -0.3262315385871316, var_rew: 0.15836650890766224
Running avgs for agent 2: q_loss: 0.06827182322740555, p_loss: 2.262542247772217, mean_rew: -0.3316930695952685, var_rew: 0.1830574516215144
Running avgs for agent 3: q_loss: 0.058779891580343246, p_loss: 2.5054080486297607, mean_rew: -0.345257734249188, var_rew: 0.1754261619793241
Running avgs for agent 4: q_loss: 0.05734045431017876, p_loss: 2.42472767829895, mean_rew: -0.33932919177540033, var_rew: 0.19326395311798095
Running avgs for agent 5: q_loss: 0.018648894503712654, p_loss: 2.6130664348602295, mean_rew: -0.41214991667608897, var_rew: 0.24890868741171301
Running avgs for agent 6: q_loss: 0.026153214275836945, p_loss: 2.6813905239105225, mean_rew: -0.4183975819771789, var_rew: 0.2595864015516246
Running avgs for agent 7: q_loss: 0.02288448065519333, p_loss: 2.8307878971099854, mean_rew: -0.42065153322228604, var_rew: 0.27506479669914097
Running avgs for agent 8: q_loss: 0.019901318475604057, p_loss: 2.5510761737823486, mean_rew: -0.40322952966983144, var_rew: 0.2385460006542265
Running avgs for agent 9: q_loss: 0.01662234030663967, p_loss: 2.738416910171509, mean_rew: -0.42018207393012486, var_rew: 0.2593049782075508
steps: 299975, episodes: 12000, mean episode reward: -72.48915528715166, agent episode reward: [-6.415641256694392, -6.824375190845216, -6.287367828717846, -7.613741157504279, -6.439741867476171, -7.413165118134298, -7.810615650767148, -7.806674556993288, -8.100732275427076, -7.77710038459195], time: 315.459
Running avgs for agent 0: q_loss: 0.05240735039114952, p_loss: 2.3313403129577637, mean_rew: -0.32422810443704, var_rew: 0.16024403403770343
Running avgs for agent 1: q_loss: 0.04836210235953331, p_loss: 2.3325014114379883, mean_rew: -0.32181607612688967, var_rew: 0.15551513437663966
Running avgs for agent 2: q_loss: 0.05948060005903244, p_loss: 2.2598536014556885, mean_rew: -0.325595712947108, var_rew: 0.17846583832170998
Running avgs for agent 3: q_loss: 0.05102558806538582, p_loss: 2.542649984359741, mean_rew: -0.33937123352556237, var_rew: 0.17002516903444076
Running avgs for agent 4: q_loss: 0.06370106339454651, p_loss: 2.4262919425964355, mean_rew: -0.333233701396839, var_rew: 0.18932064942089227
Running avgs for agent 5: q_loss: 0.024975404143333435, p_loss: 2.630993366241455, mean_rew: -0.403071807520026, var_rew: 0.2416290256750841
Running avgs for agent 6: q_loss: 0.02538975700736046, p_loss: 2.685861110687256, mean_rew: -0.40810690365872465, var_rew: 0.249455750019489
Running avgs for agent 7: q_loss: 0.025211483240127563, p_loss: 2.859344720840454, mean_rew: -0.4117844422310668, var_rew: 0.26450094239387173
Running avgs for agent 8: q_loss: 0.018269184976816177, p_loss: 2.593461036682129, mean_rew: -0.3977336863590237, var_rew: 0.23248800646292372
Running avgs for agent 9: q_loss: 0.023539679124951363, p_loss: 2.7518985271453857, mean_rew: -0.4089928864252988, var_rew: 0.24777522760021597
steps: 324975, episodes: 13000, mean episode reward: -72.80965913999242, agent episode reward: [-6.353534659767822, -7.057584781295689, -6.403975777001696, -7.062846448540782, -6.349414928704345, -7.691712684023688, -7.7173272038130545, -8.158706221951768, -8.09180358683319, -7.9227528480603855], time: 337.643
Running avgs for agent 0: q_loss: 0.058131199330091476, p_loss: 2.3360953330993652, mean_rew: -0.3194202307592056, var_rew: 0.1583769701861829
Running avgs for agent 1: q_loss: 0.05241034924983978, p_loss: 2.357090473175049, mean_rew: -0.31881658564095916, var_rew: 0.15169554079282796
Running avgs for agent 2: q_loss: 0.056570686399936676, p_loss: 2.2577342987060547, mean_rew: -0.32070927692034185, var_rew: 0.17647097511930052
Running avgs for agent 3: q_loss: 0.05201251804828644, p_loss: 2.5832176208496094, mean_rew: -0.3375047284547088, var_rew: 0.16551756783066532
Running avgs for agent 4: q_loss: 0.060711201280355453, p_loss: 2.419287919998169, mean_rew: -0.32757640548558437, var_rew: 0.18572244879660105
Running avgs for agent 5: q_loss: 0.02701236866414547, p_loss: 2.6585166454315186, mean_rew: -0.3956464217821861, var_rew: 0.23762694638411455
Running avgs for agent 6: q_loss: 0.0143563998863101, p_loss: 2.6960084438323975, mean_rew: -0.3976814662071737, var_rew: 0.2396205185188488
Running avgs for agent 7: q_loss: 0.028286859393119812, p_loss: 2.8837616443634033, mean_rew: -0.40455294368710953, var_rew: 0.2587938741877291
Running avgs for agent 8: q_loss: 0.021596329286694527, p_loss: 2.627244710922241, mean_rew: -0.391385934632705, var_rew: 0.22662603635980216
Running avgs for agent 9: q_loss: 0.024255018681287766, p_loss: 2.784843921661377, mean_rew: -0.40077710089120194, var_rew: 0.24115687436262848
steps: 349975, episodes: 14000, mean episode reward: -70.89114152093549, agent episode reward: [-6.107451792111958, -6.413369125540704, -6.304134904276036, -6.446050327204297, -6.153148881392417, -7.512411166477316, -8.265384892069639, -7.928362495262617, -7.942335277243862, -7.818492659356629], time: 313.093
Running avgs for agent 0: q_loss: 0.05013516917824745, p_loss: 2.318927526473999, mean_rew: -0.31365749238674373, var_rew: 0.15524601990412434
Running avgs for agent 1: q_loss: 0.047997549176216125, p_loss: 2.3583056926727295, mean_rew: -0.31442674942422505, var_rew: 0.14878382563440093
Running avgs for agent 2: q_loss: 0.050104424357414246, p_loss: 2.248338460922241, mean_rew: -0.31425509685325703, var_rew: 0.17257993233761
Running avgs for agent 3: q_loss: 0.05358922854065895, p_loss: 2.598863363265991, mean_rew: -0.3320384622143207, var_rew: 0.16346542577846482
Running avgs for agent 4: q_loss: 0.051675159484148026, p_loss: 2.3921689987182617, mean_rew: -0.32146678173578963, var_rew: 0.18028405973019965
Running avgs for agent 5: q_loss: 0.018534736707806587, p_loss: 2.6617300510406494, mean_rew: -0.38738466160284907, var_rew: 0.2299724813548972
Running avgs for agent 6: q_loss: 0.02612026408314705, p_loss: 2.7220189571380615, mean_rew: -0.3934453394150086, var_rew: 0.2357959020188008
Running avgs for agent 7: q_loss: 0.018186470493674278, p_loss: 2.896113634109497, mean_rew: -0.39732298130306937, var_rew: 0.24813055628723646
Running avgs for agent 8: q_loss: 0.01967327482998371, p_loss: 2.6552364826202393, mean_rew: -0.38563378810315097, var_rew: 0.22120721426806403
Running avgs for agent 9: q_loss: 0.03002909943461418, p_loss: 2.8113672733306885, mean_rew: -0.39463234971179423, var_rew: 0.2356247760315702
steps: 374975, episodes: 15000, mean episode reward: -70.41555465980694, agent episode reward: [-6.109949255343064, -6.333935389741314, -6.358126265647035, -6.268968911793477, -5.867379349180859, -7.5980782344960085, -8.143772867025532, -8.040114082229904, -7.997275674220175, -7.6979546301295825], time: 307.977
Running avgs for agent 0: q_loss: 0.048941515386104584, p_loss: 2.2849204540252686, mean_rew: -0.3060993228372795, var_rew: 0.15301003731664103
Running avgs for agent 1: q_loss: 0.04259182885289192, p_loss: 2.3505637645721436, mean_rew: -0.31053098873713303, var_rew: 0.14734549691725193
Running avgs for agent 2: q_loss: 0.07522203028202057, p_loss: 2.238567590713501, mean_rew: -0.31044722611776215, var_rew: 0.16930432719143831
Running avgs for agent 3: q_loss: 0.04452960938215256, p_loss: 2.5751280784606934, mean_rew: -0.3270477998052496, var_rew: 0.1596779050636986
Running avgs for agent 4: q_loss: 0.05976118519902229, p_loss: 2.357179641723633, mean_rew: -0.314910135623309, var_rew: 0.1763034905139495
Running avgs for agent 5: q_loss: 0.02056209370493889, p_loss: 2.666480541229248, mean_rew: -0.38026211181378994, var_rew: 0.22326163587575973
Running avgs for agent 6: q_loss: 0.01883680559694767, p_loss: 2.7341582775115967, mean_rew: -0.38805488664691595, var_rew: 0.22972956914696085
Running avgs for agent 7: q_loss: 0.029029972851276398, p_loss: 2.9062464237213135, mean_rew: -0.39246444316491896, var_rew: 0.2413232399402986
Running avgs for agent 8: q_loss: 0.017775798216462135, p_loss: 2.685018539428711, mean_rew: -0.38273743063645455, var_rew: 0.21842430664836998
Running avgs for agent 9: q_loss: 0.015972748398780823, p_loss: 2.8178210258483887, mean_rew: -0.3879378639826963, var_rew: 0.22862095326435994
steps: 399975, episodes: 16000, mean episode reward: -71.28783950058713, agent episode reward: [-6.119226527103967, -6.337450031022596, -5.934909826311152, -6.214463456030825, -5.9023786938132, -7.512635556282108, -8.490658621027976, -8.809435602230433, -7.7230494383327075, -8.243631748432158], time: 333.73
Running avgs for agent 0: q_loss: 0.05250627174973488, p_loss: 2.2711992263793945, mean_rew: -0.30507124982489664, var_rew: 0.15257966794059022
Running avgs for agent 1: q_loss: 0.051117997616529465, p_loss: 2.3318028450012207, mean_rew: -0.30496219587111534, var_rew: 0.1437611214750511
Running avgs for agent 2: q_loss: 0.0527227409183979, p_loss: 2.2175023555755615, mean_rew: -0.3063298531972051, var_rew: 0.16624881854818072
Running avgs for agent 3: q_loss: 0.05036015063524246, p_loss: 2.5463690757751465, mean_rew: -0.3231762087517817, var_rew: 0.15817219417158368
Running avgs for agent 4: q_loss: 0.05966717749834061, p_loss: 2.321256637573242, mean_rew: -0.30845068086230104, var_rew: 0.17210745237142047
Running avgs for agent 5: q_loss: 0.023356791585683823, p_loss: 2.6937339305877686, mean_rew: -0.3771697323506081, var_rew: 0.22209115286613518
Running avgs for agent 6: q_loss: 0.023018665611743927, p_loss: 2.7501490116119385, mean_rew: -0.38520154781920146, var_rew: 0.225176283170503
Running avgs for agent 7: q_loss: 0.025243176147341728, p_loss: 2.892951726913452, mean_rew: -0.3886772754519096, var_rew: 0.23582445044981393
Running avgs for agent 8: q_loss: 0.025257248431444168, p_loss: 2.7059926986694336, mean_rew: -0.3779839832046029, var_rew: 0.2147243547563413
Running avgs for agent 9: q_loss: 0.019428981468081474, p_loss: 2.838261604309082, mean_rew: -0.3845777353112941, var_rew: 0.22574262062956496
^[steps: 424975, episodes: 17000, mean episode reward: -71.03061378536344, agent episode reward: [-5.921281822513336, -6.04153378093324, -5.736196142853624, -5.964497643991056, -5.7106137958178715, -7.361591977414343, -8.51347462323996, -9.457960626770701, -8.019933182262998, -8.303530189566318], time: 346.713
Running avgs for agent 0: q_loss: 0.04717620462179184, p_loss: 2.2356674671173096, mean_rew: -0.3009334816220652, var_rew: 0.15010020715565586
Running avgs for agent 1: q_loss: 0.04326311871409416, p_loss: 2.3336181640625, mean_rew: -0.30168124357523707, var_rew: 0.14308212960180844
Running avgs for agent 2: q_loss: 0.05295990779995918, p_loss: 2.1980960369110107, mean_rew: -0.30220433752392806, var_rew: 0.16372195878188456
Running avgs for agent 3: q_loss: 0.051880184561014175, p_loss: 2.499098300933838, mean_rew: -0.31716971145425144, var_rew: 0.15520001394664756
Running avgs for agent 4: q_loss: 0.04846635088324547, p_loss: 2.2931509017944336, mean_rew: -0.305961204940251, var_rew: 0.16958008643172912
Running avgs for agent 5: q_loss: 0.021260447800159454, p_loss: 2.7047953605651855, mean_rew: -0.37337125722477577, var_rew: 0.21854949468283974
Running avgs for agent 6: q_loss: 0.018733052536845207, p_loss: 2.762465238571167, mean_rew: -0.38185942211476165, var_rew: 0.2195804951594922
Running avgs for agent 7: q_loss: 0.02456601709127426, p_loss: 2.8871099948883057, mean_rew: -0.3864209073322886, var_rew: 0.2294985844364212
Running avgs for agent 8: q_loss: 0.018591467291116714, p_loss: 2.721010446548462, mean_rew: -0.37347743935101996, var_rew: 0.21075346075001705
Running avgs for agent 9: q_loss: 0.020548947155475616, p_loss: 2.8461341857910156, mean_rew: -0.38138906608164064, var_rew: 0.22173534690982433
steps: 449975, episodes: 18000, mean episode reward: -71.10169154923014, agent episode reward: [-5.662754130521245, -5.947680347855605, -5.719964857630031, -6.025264345345874, -5.762379328792006, -7.538422831602882, -8.903800963112863, -9.168310730324642, -7.79254168532177, -8.580572328723225], time: 316.061
Running avgs for agent 0: q_loss: 0.04973912984132767, p_loss: 2.215155839920044, mean_rew: -0.29725368284988624, var_rew: 0.14788381046050386
Running avgs for agent 1: q_loss: 0.04504634067416191, p_loss: 2.33542537689209, mean_rew: -0.3002575975430749, var_rew: 0.14376105946753903
Running avgs for agent 2: q_loss: 0.05118934065103531, p_loss: 2.1805200576782227, mean_rew: -0.29788067054619477, var_rew: 0.16116473380051724
Running avgs for agent 3: q_loss: 0.04363000765442848, p_loss: 2.4636480808258057, mean_rew: -0.31266076607311816, var_rew: 0.15264199965181816
Running avgs for agent 4: q_loss: 0.06331919133663177, p_loss: 2.251682758331299, mean_rew: -0.3000366806937462, var_rew: 0.16675988860492288
Running avgs for agent 5: q_loss: 0.02197783626616001, p_loss: 2.716372013092041, mean_rew: -0.3674633391880568, var_rew: 0.21490286319559854
Running avgs for agent 6: q_loss: 0.01342807337641716, p_loss: 2.7810943126678467, mean_rew: -0.38041984886554636, var_rew: 0.21596909446001217
Running avgs for agent 7: q_loss: 0.025782449170947075, p_loss: 2.9225029945373535, mean_rew: -0.3859386757711577, var_rew: 0.22403087608191463
Running avgs for agent 8: q_loss: 0.016373198479413986, p_loss: 2.7472503185272217, mean_rew: -0.3715693903693234, var_rew: 0.20755829496489767
Running avgs for agent 9: q_loss: 0.015905730426311493, p_loss: 2.8468940258026123, mean_rew: -0.3790995203663797, var_rew: 0.21656244762111593
steps: 474975, episodes: 19000, mean episode reward: -71.07674590875857, agent episode reward: [-5.611003775025814, -5.529796801075296, -5.684568510759211, -5.696865958009787, -5.604918758627499, -7.553084467914817, -8.961833369627824, -9.556514382021357, -8.104262637908217, -8.77389724778875], time: 323.465
Running avgs for agent 0: q_loss: 0.0444590300321579, p_loss: 2.1956136226654053, mean_rew: -0.2937225422847713, var_rew: 0.14830236323269833
Running avgs for agent 1: q_loss: 0.05050138756632805, p_loss: 2.3218371868133545, mean_rew: -0.29508965769439954, var_rew: 0.14249525341493172
Running avgs for agent 2: q_loss: 0.04959457367658615, p_loss: 2.161794662475586, mean_rew: -0.2928586615488673, var_rew: 0.15865876643971402
Running avgs for agent 3: q_loss: 0.0554133802652359, p_loss: 2.422945499420166, mean_rew: -0.3084563878867039, var_rew: 0.15122845887631817
Running avgs for agent 4: q_loss: 0.053068481385707855, p_loss: 2.2393598556518555, mean_rew: -0.2970141958564179, var_rew: 0.16579232529212962
Running avgs for agent 5: q_loss: 0.017441745847463608, p_loss: 2.725973606109619, mean_rew: -0.3643918425863652, var_rew: 0.21242896166883582
Running avgs for agent 6: q_loss: 0.021369487047195435, p_loss: 2.814751148223877, mean_rew: -0.3807481368205446, var_rew: 0.2137556184661974
Running avgs for agent 7: q_loss: 0.02012111432850361, p_loss: 2.965632915496826, mean_rew: -0.38605583979040753, var_rew: 0.21771138274973906
Running avgs for agent 8: q_loss: 0.017897892743349075, p_loss: 2.769874334335327, mean_rew: -0.3679641237158812, var_rew: 0.20387283724035596
Running avgs for agent 9: q_loss: 0.025930197909474373, p_loss: 2.8660013675689697, mean_rew: -0.37710922665147445, var_rew: 0.21358799479434665
steps: 499975, episodes: 20000, mean episode reward: -72.78573856767258, agent episode reward: [-5.779577290077177, -5.811179871710014, -6.0076449802452005, -5.959698992809261, -5.724740256360653, -7.3268244218070615, -8.904214943924067, -9.93843964933393, -8.144785401776769, -9.188632759628446], time: 321.509
Running avgs for agent 0: q_loss: 0.049594152718782425, p_loss: 2.154431104660034, mean_rew: -0.2897151862233143, var_rew: 0.14748678599829618
Running avgs for agent 1: q_loss: 0.05032157897949219, p_loss: 2.2861709594726562, mean_rew: -0.2922368637476145, var_rew: 0.14244060619414387
Running avgs for agent 2: q_loss: 0.0498826839029789, p_loss: 2.155592441558838, mean_rew: -0.2919869934711341, var_rew: 0.1584906889154042
Running avgs for agent 3: q_loss: 0.04115685820579529, p_loss: 2.3730807304382324, mean_rew: -0.3041270963361394, var_rew: 0.14956589297549333
Running avgs for agent 4: q_loss: 0.04413052275776863, p_loss: 2.219095230102539, mean_rew: -0.29325665832508707, var_rew: 0.16319725062116214
Running avgs for agent 5: q_loss: 0.02156946063041687, p_loss: 2.731748580932617, mean_rew: -0.3605259912813031, var_rew: 0.20904766248945966
Running avgs for agent 6: q_loss: 0.013954211957752705, p_loss: 2.836239814758301, mean_rew: -0.37814937568465923, var_rew: 0.20767469490098603
Running avgs for agent 7: q_loss: 0.020992668345570564, p_loss: 2.970017194747925, mean_rew: -0.38565093876058565, var_rew: 0.21228363074647233
Running avgs for agent 8: q_loss: 0.020967187359929085, p_loss: 2.798954963684082, mean_rew: -0.36574302272818865, var_rew: 0.2022564498873513
Running avgs for agent 9: q_loss: 0.01477805059403181, p_loss: 2.901125192642212, mean_rew: -0.37670040407553596, var_rew: 0.20984172325926148
steps: 524975, episodes: 21000, mean episode reward: -71.1310811634273, agent episode reward: [-5.860324083021636, -5.631673772012674, -5.88493667913701, -5.882223633726619, -5.598341920993369, -7.50185321397862, -7.9839112908824355, -9.054073082645726, -8.334880519528905, -9.398862967500298], time: 323.167
Running avgs for agent 0: q_loss: 0.04556243121623993, p_loss: 2.1293814182281494, mean_rew: -0.28859968884044374, var_rew: 0.14736757907904638
Running avgs for agent 1: q_loss: 0.04459536075592041, p_loss: 2.243887424468994, mean_rew: -0.2899971337513362, var_rew: 0.14148340486936228
Running avgs for agent 2: q_loss: 0.056671153753995895, p_loss: 2.138986349105835, mean_rew: -0.2886669275879252, var_rew: 0.15653973191641363
Running avgs for agent 3: q_loss: 0.04176190868020058, p_loss: 2.3495938777923584, mean_rew: -0.30236692416428046, var_rew: 0.1493540833902445
Running avgs for agent 4: q_loss: 0.04780219867825508, p_loss: 2.205775499343872, mean_rew: -0.29066096171352485, var_rew: 0.1614920772112341
Running avgs for agent 5: q_loss: 0.01718178577721119, p_loss: 2.7351229190826416, mean_rew: -0.3587831384064996, var_rew: 0.2088487739442308
Running avgs for agent 6: q_loss: 0.014279876835644245, p_loss: 2.8651793003082275, mean_rew: -0.37595095436774967, var_rew: 0.20454276322010045
Running avgs for agent 7: q_loss: 0.018102377653121948, p_loss: 2.9636528491973877, mean_rew: -0.38627062522363814, var_rew: 0.20966491777600585
Running avgs for agent 8: q_loss: 0.016415415331721306, p_loss: 2.817121982574463, mean_rew: -0.36346957176588024, var_rew: 0.19849329652525735
Running avgs for agent 9: q_loss: 0.015342406928539276, p_loss: 2.9371354579925537, mean_rew: -0.37637826840223204, var_rew: 0.20586361793778143
steps: 549975, episodes: 22000, mean episode reward: -72.28614553360912, agent episode reward: [-6.104315084455486, -5.961689222360175, -5.9841519705552875, -6.356406648091539, -5.81624091401534, -7.238340543813333, -7.7409015027526475, -9.163751925994134, -8.276014931128204, -9.644332790442972], time: 305.32
Running avgs for agent 0: q_loss: 0.04387343302369118, p_loss: 2.089278221130371, mean_rew: -0.2839723213079061, var_rew: 0.14485378729541185
Running avgs for agent 1: q_loss: 0.0444604717195034, p_loss: 2.19509220123291, mean_rew: -0.28783923281271817, var_rew: 0.1408999482218372
Running avgs for agent 2: q_loss: 0.04703489691019058, p_loss: 2.1219491958618164, mean_rew: -0.2851024979269406, var_rew: 0.1538427838533983
Running avgs for agent 3: q_loss: 0.04243692755699158, p_loss: 2.331451177597046, mean_rew: -0.2990658794391821, var_rew: 0.1468603421450347
Running avgs for agent 4: q_loss: 0.04479839652776718, p_loss: 2.1826329231262207, mean_rew: -0.28658362460898096, var_rew: 0.1583941864139707
Running avgs for agent 5: q_loss: 0.015582878142595291, p_loss: 2.72997784614563, mean_rew: -0.3550347812041645, var_rew: 0.2054853960199505
Running avgs for agent 6: q_loss: 0.018936827778816223, p_loss: 2.8871254920959473, mean_rew: -0.3738085214443822, var_rew: 0.2048437155452579
Running avgs for agent 7: q_loss: 0.02528853341937065, p_loss: 2.9225175380706787, mean_rew: -0.3849402686852295, var_rew: 0.20554664341642936
Running avgs for agent 8: q_loss: 0.01657765544950962, p_loss: 2.845005750656128, mean_rew: -0.36202613968069897, var_rew: 0.19662927194126711
Running avgs for agent 9: q_loss: 0.013076585717499256, p_loss: 2.980959892272949, mean_rew: -0.37618782914170573, var_rew: 0.20168194431934477
steps: 574975, episodes: 23000, mean episode reward: -73.60159408485144, agent episode reward: [-5.974000988058444, -5.831406348104412, -6.232511219855069, -6.692049625258918, -5.9642507301334176, -7.462612437143942, -7.509890318655551, -9.21356551266507, -8.261281473081883, -10.460025431894751], time: 302.27
Running avgs for agent 0: q_loss: 0.0460982583463192, p_loss: 2.0692920684814453, mean_rew: -0.28316087581091576, var_rew: 0.14460921310771446
Running avgs for agent 1: q_loss: 0.04622126370668411, p_loss: 2.1504135131835938, mean_rew: -0.28492587856449686, var_rew: 0.13983293391336274
Running avgs for agent 2: q_loss: 0.0460536889731884, p_loss: 2.1269779205322266, mean_rew: -0.2844578442420682, var_rew: 0.15436289703009043
Running avgs for agent 3: q_loss: 0.04733048751950264, p_loss: 2.3267245292663574, mean_rew: -0.29674556201274177, var_rew: 0.14525939879528804
Running avgs for agent 4: q_loss: 0.04794534295797348, p_loss: 2.1674997806549072, mean_rew: -0.2847710064320629, var_rew: 0.15840091236690854
Running avgs for agent 5: q_loss: 0.026253852993249893, p_loss: 2.7208499908447266, mean_rew: -0.35149292652968506, var_rew: 0.20228456443583748
Running avgs for agent 6: q_loss: 0.01972290128469467, p_loss: 2.8950014114379883, mean_rew: -0.37173038226331656, var_rew: 0.20414428475431015
Running avgs for agent 7: q_loss: 0.028576908633112907, p_loss: 2.891350507736206, mean_rew: -0.3841915144525698, var_rew: 0.20118831761958839
Running avgs for agent 8: q_loss: 0.017672307789325714, p_loss: 2.8754258155822754, mean_rew: -0.36111458870305807, var_rew: 0.1952807120964835
Running avgs for agent 9: q_loss: 0.014757389202713966, p_loss: 3.036508321762085, mean_rew: -0.376932131807072, var_rew: 0.19893819035621532
steps: 599975, episodes: 24000, mean episode reward: -73.33802693616579, agent episode reward: [-6.302825431316868, -5.8839465320335975, -6.028661594382965, -6.718509850380906, -5.815185356703026, -7.509597801279288, -7.376256889888447, -8.708174801847386, -8.362718263915594, -10.632150414417719], time: 301.445
Running avgs for agent 0: q_loss: 0.045007310807704926, p_loss: 2.025552988052368, mean_rew: -0.2808319301328874, var_rew: 0.14222048391556813
Running avgs for agent 1: q_loss: 0.047502852976322174, p_loss: 2.106255292892456, mean_rew: -0.2819765858799793, var_rew: 0.13956872251067712
Running avgs for agent 2: q_loss: 0.050921011716127396, p_loss: 2.121181011199951, mean_rew: -0.2821695053832236, var_rew: 0.1531779274960368
Running avgs for agent 3: q_loss: 0.042365673929452896, p_loss: 2.3308660984039307, mean_rew: -0.29492215624327467, var_rew: 0.14469615606193673
Running avgs for agent 4: q_loss: 0.048480987548828125, p_loss: 2.1550092697143555, mean_rew: -0.2842539127654733, var_rew: 0.15873833526965686
Running avgs for agent 5: q_loss: 0.018680626526474953, p_loss: 2.7375295162200928, mean_rew: -0.3493184774333674, var_rew: 0.20099118896090437
Running avgs for agent 6: q_loss: 0.015845831483602524, p_loss: 2.8748345375061035, mean_rew: -0.36706889893483596, var_rew: 0.20081094956625697
Running avgs for agent 7: q_loss: 0.01732781156897545, p_loss: 2.8673973083496094, mean_rew: -0.382730724145373, var_rew: 0.20124435988025863
Running avgs for agent 8: q_loss: 0.014915264211595058, p_loss: 2.8923468589782715, mean_rew: -0.3593332213216295, var_rew: 0.19298598870398898
Running avgs for agent 9: q_loss: 0.016847306862473488, p_loss: 3.0995428562164307, mean_rew: -0.3790340943886918, var_rew: 0.195358234765953
steps: 624975, episodes: 25000, mean episode reward: -72.73201298623685, agent episode reward: [-5.697927729188547, -5.967672731372576, -5.900747879172128, -6.78096299895487, -5.966220706319009, -7.180461278491308, -7.599652118290138, -8.416659272254645, -8.419715302004166, -10.80199297018946], time: 313.496
Running avgs for agent 0: q_loss: 0.04745524004101753, p_loss: 1.9972670078277588, mean_rew: -0.2792399956744459, var_rew: 0.1424509404384203
Running avgs for agent 1: q_loss: 0.0438016876578331, p_loss: 2.057389259338379, mean_rew: -0.2800495129120135, var_rew: 0.1386345957329745
Running avgs for agent 2: q_loss: 0.043197717517614365, p_loss: 2.1098344326019287, mean_rew: -0.2802491774606513, var_rew: 0.14925035704151215
Running avgs for agent 3: q_loss: 0.04063229262828827, p_loss: 2.341116189956665, mean_rew: -0.29465966204460065, var_rew: 0.1432441845551789
Running avgs for agent 4: q_loss: 0.05011453106999397, p_loss: 2.1304705142974854, mean_rew: -0.2806135167089507, var_rew: 0.15551370545871104
Running avgs for agent 5: q_loss: 0.014198974706232548, p_loss: 2.7358174324035645, mean_rew: -0.3469026344813385, var_rew: 0.1987971689917906
Running avgs for agent 6: q_loss: 0.019081169739365578, p_loss: 2.854275941848755, mean_rew: -0.3650068829228064, var_rew: 0.1982794114921968
Running avgs for agent 7: q_loss: 0.019467951729893684, p_loss: 2.8463191986083984, mean_rew: -0.38079302073588883, var_rew: 0.19827212278646028
Running avgs for agent 8: q_loss: 0.01617758721113205, p_loss: 2.8955752849578857, mean_rew: -0.3574896891646364, var_rew: 0.18921165007015894
Running avgs for agent 9: q_loss: 0.022522948682308197, p_loss: 3.172664165496826, mean_rew: -0.3828629760561329, var_rew: 0.19431207123132585
steps: 649975, episodes: 26000, mean episode reward: -72.66232475688108, agent episode reward: [-6.299265076016667, -5.695305681133578, -5.940244596618173, -7.094613936116808, -5.987634447233617, -7.810127295551963, -7.680321442916053, -8.684342419612538, -8.754856475819448, -8.715613385862223], time: 424.647
Running avgs for agent 0: q_loss: 0.059182144701480865, p_loss: 1.9889132976531982, mean_rew: -0.2776055285970664, var_rew: 0.14184240980494975
Running avgs for agent 1: q_loss: 0.043238043785095215, p_loss: 2.0122950077056885, mean_rew: -0.27806731283203223, var_rew: 0.13853987717008787
Running avgs for agent 2: q_loss: 0.04369285702705383, p_loss: 2.0969512462615967, mean_rew: -0.27931663418723135, var_rew: 0.15084730409889296
Running avgs for agent 3: q_loss: 0.05452961102128029, p_loss: 2.346813440322876, mean_rew: -0.2933546012419758, var_rew: 0.14169691313822552
Running avgs for agent 4: q_loss: 0.04256635531783104, p_loss: 2.0971977710723877, mean_rew: -0.27899994434571496, var_rew: 0.15068431808954935
Running avgs for agent 5: q_loss: 0.02602490782737732, p_loss: 2.7432124614715576, mean_rew: -0.346347448676302, var_rew: 0.19836868970447377
Running avgs for agent 6: q_loss: 0.01831459440290928, p_loss: 2.8574447631835938, mean_rew: -0.36323988198009227, var_rew: 0.1997377516875457
Running avgs for agent 7: q_loss: 0.020097659900784492, p_loss: 2.8318960666656494, mean_rew: -0.3777557676938057, var_rew: 0.1930522849984148
Running avgs for agent 8: q_loss: 0.01681211031973362, p_loss: 2.893331527709961, mean_rew: -0.3574748181589521, var_rew: 0.18886818843628847
Running avgs for agent 9: q_loss: 0.014212029054760933, p_loss: 3.224841356277466, mean_rew: -0.3830456847937994, var_rew: 0.18905832862408256
steps: 674975, episodes: 27000, mean episode reward: -71.97228358965378, agent episode reward: [-6.165576882760877, -6.048053991076922, -6.077348453103263, -7.3221880711965905, -5.941659527491949, -7.383786775663526, -7.857142012567634, -8.801187040652925, -8.713050160464663, -7.662290674675446], time: 624.559
Running avgs for agent 0: q_loss: 0.04580458253622055, p_loss: 1.9531298875808716, mean_rew: -0.2762013660864135, var_rew: 0.1395972585084453
Running avgs for agent 1: q_loss: 0.04923652857542038, p_loss: 1.9665029048919678, mean_rew: -0.2753791310347695, var_rew: 0.13707690521631502
Running avgs for agent 2: q_loss: 0.04922579973936081, p_loss: 2.0758988857269287, mean_rew: -0.27785465791785924, var_rew: 0.1502060763607127
Running avgs for agent 3: q_loss: 0.04048964008688927, p_loss: 2.378605365753174, mean_rew: -0.2940724125451453, var_rew: 0.14179131523920535
Running avgs for agent 4: q_loss: 0.04623908922076225, p_loss: 2.0788156986236572, mean_rew: -0.27681994012389355, var_rew: 0.15114800420109012
Running avgs for agent 5: q_loss: 0.016723163425922394, p_loss: 2.740908145904541, mean_rew: -0.34319063391918303, var_rew: 0.19541236223021524
Running avgs for agent 6: q_loss: 0.01542435772716999, p_loss: 2.840895652770996, mean_rew: -0.3601436609137074, var_rew: 0.19647019933740917
Running avgs for agent 7: q_loss: 0.018269099295139313, p_loss: 2.8573496341705322, mean_rew: -0.3797520415496864, var_rew: 0.19493761476248284
Running avgs for agent 8: q_loss: 0.022307420149445534, p_loss: 2.896451234817505, mean_rew: -0.35725191196913497, var_rew: 0.18640748575722157
Running avgs for agent 9: q_loss: 0.017055774107575417, p_loss: 3.2490394115448, mean_rew: -0.38052957657632835, var_rew: 0.18700622285714122
^[[Bsteps: 699975, episodes: 28000, mean episode reward: -72.92833686690618, agent episode reward: [-6.496949050924322, -6.3683581657884805, -6.280000387579836, -7.583924928857427, -6.214304017712946, -7.702450687637975, -7.8723685294687, -8.522689853612388, -8.318646485463544, -7.568644759860548], time: 959.948
Running avgs for agent 0: q_loss: 0.044814687222242355, p_loss: 1.9263659715652466, mean_rew: -0.2754889980307417, var_rew: 0.13954926366057424
Running avgs for agent 1: q_loss: 0.04276001825928688, p_loss: 1.9466136693954468, mean_rew: -0.27520176202714847, var_rew: 0.1373937958691187
Running avgs for agent 2: q_loss: 0.04510411247611046, p_loss: 2.0382583141326904, mean_rew: -0.27499679203824134, var_rew: 0.14825613844356927
Running avgs for agent 3: q_loss: 0.04101210832595825, p_loss: 2.409775495529175, mean_rew: -0.29399070867648475, var_rew: 0.1399161307050382
Running avgs for agent 4: q_loss: 0.04110861197113991, p_loss: 2.072054862976074, mean_rew: -0.2767328476940416, var_rew: 0.1519414430029706
Running avgs for agent 5: q_loss: 0.01816558465361595, p_loss: 2.7514946460723877, mean_rew: -0.3427437136407816, var_rew: 0.1952127161810447
Running avgs for agent 6: q_loss: 0.024113141000270844, p_loss: 2.835899829864502, mean_rew: -0.358740729651274, var_rew: 0.1955764276970889
Running avgs for agent 7: q_loss: 0.035965804010629654, p_loss: 2.875175714492798, mean_rew: -0.37727661235821597, var_rew: 0.19280293304450175
Running avgs for agent 8: q_loss: 0.014871793799102306, p_loss: 2.886364221572876, mean_rew: -0.3572460206715096, var_rew: 0.1855753778633952
Running avgs for agent 9: q_loss: 0.01453626062721014, p_loss: 3.220900058746338, mean_rew: -0.37700483622779896, var_rew: 0.18613833612060096
steps: 724975, episodes: 29000, mean episode reward: -70.33134886104887, agent episode reward: [-6.259198399881397, -5.9262564258599, -5.9676806661734245, -7.467328207917815, -6.038022038329105, -7.536540304090996, -7.814588100760282, -8.123567197321973, -7.66958914607567, -7.528578374638313], time: 1391.792
Running avgs for agent 0: q_loss: 0.044818684458732605, p_loss: 1.9064329862594604, mean_rew: -0.2748742949514795, var_rew: 0.14031671724146
Running avgs for agent 1: q_loss: 0.04469654709100723, p_loss: 1.9195066690444946, mean_rew: -0.27402262675272426, var_rew: 0.1366546430443575
Running avgs for agent 2: q_loss: 0.0507131963968277, p_loss: 2.022650957107544, mean_rew: -0.27616816945665246, var_rew: 0.1492666478035067
Running avgs for agent 3: q_loss: 0.04807974025607109, p_loss: 2.451953649520874, mean_rew: -0.2942968517676096, var_rew: 0.13831234963829359
Running avgs for agent 4: q_loss: 0.04889753833413124, p_loss: 2.0729916095733643, mean_rew: -0.2767163624681822, var_rew: 0.1508458290569157
Running avgs for agent 5: q_loss: 0.017454080283641815, p_loss: 2.7642245292663574, mean_rew: -0.34314350128849014, var_rew: 0.19498581751524435
Running avgs for agent 6: q_loss: 0.014680763706564903, p_loss: 2.826599597930908, mean_rew: -0.3564121900086745, var_rew: 0.19320263792065137
Running avgs for agent 7: q_loss: 0.020902542397379875, p_loss: 2.877140998840332, mean_rew: -0.37504128475850657, var_rew: 0.1903262840932259
Running avgs for agent 8: q_loss: 0.014882251620292664, p_loss: 2.901754379272461, mean_rew: -0.35515094970093375, var_rew: 0.184020542181378
Running avgs for agent 9: q_loss: 0.023684214800596237, p_loss: 3.1709065437316895, mean_rew: -0.37351696195590867, var_rew: 0.18489252020029084
steps: 749975, episodes: 30000, mean episode reward: -73.16033393941197, agent episode reward: [-6.7375721810548255, -6.288203154825376, -6.2069138469103065, -7.625818873107792, -6.577517394438834, -7.979849939383164, -7.785055573409003, -8.389233464985548, -7.91068416947388, -7.6594853418232285], time: 1528.681
Running avgs for agent 0: q_loss: 0.04994220659136772, p_loss: 1.8836129903793335, mean_rew: -0.274148470967534, var_rew: 0.13875064696924594
Running avgs for agent 1: q_loss: 0.046033382415771484, p_loss: 1.8954318761825562, mean_rew: -0.2747084017181953, var_rew: 0.13841028245511386
Running avgs for agent 2: q_loss: 0.05452026426792145, p_loss: 1.968334674835205, mean_rew: -0.2742471662959598, var_rew: 0.1464907701411598
Running avgs for agent 3: q_loss: 0.044175855815410614, p_loss: 2.495734453201294, mean_rew: -0.294879046334122, var_rew: 0.13736910756825813
Running avgs for agent 4: q_loss: 0.04952161759138107, p_loss: 2.0555737018585205, mean_rew: -0.27452006976529575, var_rew: 0.1491008205410528
Running avgs for agent 5: q_loss: 0.01999964937567711, p_loss: 2.745434045791626, mean_rew: -0.339453509688222, var_rew: 0.19214146521579858
Running avgs for agent 6: q_loss: 0.01821136847138405, p_loss: 2.8265469074249268, mean_rew: -0.3545290518087632, var_rew: 0.19167259586723892
Running avgs for agent 7: q_loss: 0.017987782135605812, p_loss: 2.8842275142669678, mean_rew: -0.37392763939002277, var_rew: 0.18998901749025904
Running avgs for agent 8: q_loss: 0.016097567975521088, p_loss: 2.886162519454956, mean_rew: -0.35480462355999903, var_rew: 0.18389055885001387
Running avgs for agent 9: q_loss: 0.015171240083873272, p_loss: 3.1379706859588623, mean_rew: -0.3718770535184156, var_rew: 0.18403496534508254
steps: 774975, episodes: 31000, mean episode reward: -72.01236903602255, agent episode reward: [-6.472651611090246, -6.235100496191081, -6.121212988284661, -7.343854206168067, -6.467209807343395, -7.6109308369137, -7.701586029498643, -8.38139800196354, -7.853532974330951, -7.824892084238271], time: 902.392
Running avgs for agent 0: q_loss: 0.05035199224948883, p_loss: 1.8652480840682983, mean_rew: -0.2754623463411867, var_rew: 0.13865072957469432
Running avgs for agent 1: q_loss: 0.0493185892701149, p_loss: 1.8467156887054443, mean_rew: -0.27247677693557376, var_rew: 0.1359377444629248
Running avgs for agent 2: q_loss: 0.048492878675460815, p_loss: 1.9228672981262207, mean_rew: -0.27340456844072614, var_rew: 0.14799751366527703
Running avgs for agent 3: q_loss: 0.0447605736553669, p_loss: 2.539125919342041, mean_rew: -0.2947599096131297, var_rew: 0.13663565450045842
Running avgs for agent 4: q_loss: 0.03908228129148483, p_loss: 2.0482094287872314, mean_rew: -0.2750039872525435, var_rew: 0.14881246112180008
Running avgs for agent 5: q_loss: 0.022277453914284706, p_loss: 2.7337260246276855, mean_rew: -0.3392335189389227, var_rew: 0.19084662360830912
Running avgs for agent 6: q_loss: 0.018256444483995438, p_loss: 2.83036470413208, mean_rew: -0.3543686956237342, var_rew: 0.1908042255345568
Running avgs for agent 7: q_loss: 0.023436950519680977, p_loss: 2.8895199298858643, mean_rew: -0.3732705649141461, var_rew: 0.18891908817251968
Running avgs for agent 8: q_loss: 0.023372787982225418, p_loss: 2.8274283409118652, mean_rew: -0.35370271414544696, var_rew: 0.18375937608932866
Running avgs for agent 9: q_loss: 0.01723402738571167, p_loss: 3.097791910171509, mean_rew: -0.37052781154706155, var_rew: 0.18301944417986682
steps: 799975, episodes: 32000, mean episode reward: -71.99841776945404, agent episode reward: [-6.720092809200524, -6.075608707249109, -6.051944717319538, -7.19011519188294, -6.402436450945938, -7.59179309220244, -7.71479677899538, -8.300045459389434, -7.940275043020913, -8.011309519247824], time: 978.714
Running avgs for agent 0: q_loss: 0.04648295044898987, p_loss: 1.8354313373565674, mean_rew: -0.2738166236341351, var_rew: 0.13772527473054902
Running avgs for agent 1: q_loss: 0.05446832627058029, p_loss: 1.8080488443374634, mean_rew: -0.27125326765172747, var_rew: 0.1353908259614035
Running avgs for agent 2: q_loss: 0.050111353397369385, p_loss: 1.8681025505065918, mean_rew: -0.2717705370320276, var_rew: 0.14684640278614644
Running avgs for agent 3: q_loss: 0.04454013705253601, p_loss: 2.5897305011749268, mean_rew: -0.29636980670127483, var_rew: 0.1363988574762799
Running avgs for agent 4: q_loss: 0.045770931988954544, p_loss: 2.0354502201080322, mean_rew: -0.27458253717795267, var_rew: 0.14825481469091886
Running avgs for agent 5: q_loss: 0.020911995321512222, p_loss: 2.706873893737793, mean_rew: -0.33848031408945417, var_rew: 0.19059630737522548
Running avgs for agent 6: q_loss: 0.015957340598106384, p_loss: 2.8330512046813965, mean_rew: -0.3529394998061431, var_rew: 0.1898302289785689
Running avgs for agent 7: q_loss: 0.02840486913919449, p_loss: 2.89357328414917, mean_rew: -0.37205815988525354, var_rew: 0.18726678115797887
Running avgs for agent 8: q_loss: 0.023300906643271446, p_loss: 2.7602615356445312, mean_rew: -0.35096596767807214, var_rew: 0.1814198813738833
Running avgs for agent 9: q_loss: 0.015765881165862083, p_loss: 3.061264991760254, mean_rew: -0.36647035991155724, var_rew: 0.18154040014998535
steps: 824975, episodes: 33000, mean episode reward: -72.21191732575369, agent episode reward: [-6.610611598251591, -6.131490974745363, -6.096717831793787, -7.5787714820449255, -6.184211569641864, -7.660024140439499, -7.49527138385413, -8.20986191834062, -8.467373881051541, -7.777582545590378], time: 1335.878
Running avgs for agent 0: q_loss: 0.04821634665131569, p_loss: 1.8135448694229126, mean_rew: -0.27331606971544625, var_rew: 0.13588299812784893
Running avgs for agent 1: q_loss: 0.04541433975100517, p_loss: 1.7811723947525024, mean_rew: -0.27184610090698924, var_rew: 0.1357361234054209
Running avgs for agent 2: q_loss: 0.051950037479400635, p_loss: 1.8084748983383179, mean_rew: -0.2709033729411482, var_rew: 0.14484338235558394
Running avgs for agent 3: q_loss: 0.04327921196818352, p_loss: 2.6251108646392822, mean_rew: -0.29544770776505597, var_rew: 0.13511555013209958
Running avgs for agent 4: q_loss: 0.04806111752986908, p_loss: 2.021653890609741, mean_rew: -0.27397157201616035, var_rew: 0.1468442490794304
Running avgs for agent 5: q_loss: 0.022514645010232925, p_loss: 2.6843974590301514, mean_rew: -0.33656798363374785, var_rew: 0.1892979532973981
Running avgs for agent 6: q_loss: 0.016589626669883728, p_loss: 2.8357651233673096, mean_rew: -0.35288921503340814, var_rew: 0.19099717049065726
Running avgs for agent 7: q_loss: 0.020339710637927055, p_loss: 2.8838164806365967, mean_rew: -0.370314920952712, var_rew: 0.1841937121330761
Running avgs for agent 8: q_loss: 0.025134341791272163, p_loss: 2.731956958770752, mean_rew: -0.35136646362428864, var_rew: 0.181602940257145
Running avgs for agent 9: q_loss: 0.01607212796807289, p_loss: 3.032478094100952, mean_rew: -0.3646821854245944, var_rew: 0.1799827161999267
