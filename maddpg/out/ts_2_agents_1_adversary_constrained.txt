python train.py --scenario simple_push --num-adversaries 1
2018-10-22 13:39:21.289063: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
q_func_vars [<tf.Variable 'agent_0/q_func/fully_connected/weights:0' shape=(37, 64) dtype=float32_ref>, <tf.Variable 'agent_0/q_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'agent_0/q_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref>, <tf.Variable 'agent_0/q_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'agent_0/q_func/fully_connected_2/weights:0' shape=(64, 1) dtype=float32_ref>, <tf.Variable 'agent_0/q_func/fully_connected_2/biases:0' shape=(1,) dtype=float32_ref>]
<tf.Variable 'agent_0/q_func/fully_connected/weights:0' shape=(37, 64) dtype=float32_ref> Tensor("agent_0/gradients/agent_0/q_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(37, 64), dtype=float32)
<tf.Variable 'agent_0/q_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_0/gradients/agent_0/q_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_0/q_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_0/gradients/agent_0/q_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_0/q_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_0/gradients/agent_0/q_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_0/q_func/fully_connected_2/weights:0' shape=(64, 1) dtype=float32_ref> Tensor("agent_0/gradients/agent_0/q_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 1), dtype=float32)
<tf.Variable 'agent_0/q_func/fully_connected_2/biases:0' shape=(1,) dtype=float32_ref> Tensor("agent_0/gradients/agent_0/q_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(1,), dtype=float32)
<tf.Variable 'agent_0/lamda_constraint0:0' shape=(64,) dtype=float32_ref> Tensor("agent_0/gradients_1/agent_0/mul_grad/tuple/control_dependency:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_0/p_func/fully_connected/weights:0' shape=(8, 64) dtype=float32_ref> Tensor("agent_0_1/gradients/agent_0_1/p_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(8, 64), dtype=float32)
<tf.Variable 'agent_0/p_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_0_1/gradients/agent_0_1/p_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_0/p_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_0_1/gradients/agent_0_1/p_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_0/p_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_0_1/gradients/agent_0_1/p_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_0/p_func/fully_connected_2/weights:0' shape=(64, 5) dtype=float32_ref> Tensor("agent_0_1/gradients/agent_0_1/p_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 5), dtype=float32)
<tf.Variable 'agent_0/p_func/fully_connected_2/biases:0' shape=(5,) dtype=float32_ref> Tensor("agent_0_1/gradients/agent_0_1/p_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(5,), dtype=float32)
q_func_vars [<tf.Variable 'agent_1/q_func/fully_connected/weights:0' shape=(37, 64) dtype=float32_ref>, <tf.Variable 'agent_1/q_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'agent_1/q_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref>, <tf.Variable 'agent_1/q_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'agent_1/q_func/fully_connected_2/weights:0' shape=(64, 1) dtype=float32_ref>, <tf.Variable 'agent_1/q_func/fully_connected_2/biases:0' shape=(1,) dtype=float32_ref>]
<tf.Variable 'agent_1/q_func/fully_connected/weights:0' shape=(37, 64) dtype=float32_ref> Tensor("agent_1/gradients/agent_1/q_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(37, 64), dtype=float32)
<tf.Variable 'agent_1/q_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_1/gradients/agent_1/q_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_1/q_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_1/gradients/agent_1/q_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_1/q_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_1/gradients/agent_1/q_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_1/q_func/fully_connected_2/weights:0' shape=(64, 1) dtype=float32_ref> Tensor("agent_1/gradients/agent_1/q_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 1), dtype=float32)
<tf.Variable 'agent_1/q_func/fully_connected_2/biases:0' shape=(1,) dtype=float32_ref> Tensor("agent_1/gradients/agent_1/q_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(1,), dtype=float32)
<tf.Variable 'agent_1/lamda_constraint1:0' shape=(64,) dtype=float32_ref> Tensor("agent_1/gradients_1/agent_1/mul_grad/tuple/control_dependency:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_1/p_func/fully_connected/weights:0' shape=(19, 64) dtype=float32_ref> Tensor("agent_1_1/gradients/agent_1_1/p_func/fully_connected/MatMul_grad/tuple/control_dependency_1:0", shape=(19, 64), dtype=float32)
<tf.Variable 'agent_1/p_func/fully_connected/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_1_1/gradients/agent_1_1/p_func/fully_connected/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_1/p_func/fully_connected_1/weights:0' shape=(64, 64) dtype=float32_ref> Tensor("agent_1_1/gradients/agent_1_1/p_func/fully_connected_1/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 64), dtype=float32)
<tf.Variable 'agent_1/p_func/fully_connected_1/biases:0' shape=(64,) dtype=float32_ref> Tensor("agent_1_1/gradients/agent_1_1/p_func/fully_connected_1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(64,), dtype=float32)
<tf.Variable 'agent_1/p_func/fully_connected_2/weights:0' shape=(64, 5) dtype=float32_ref> Tensor("agent_1_1/gradients/agent_1_1/p_func/fully_connected_2/MatMul_grad/tuple/control_dependency_1:0", shape=(64, 5), dtype=float32)
<tf.Variable 'agent_1/p_func/fully_connected_2/biases:0' shape=(5,) dtype=float32_ref> Tensor("agent_1_1/gradients/agent_1_1/p_func/fully_connected_2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(5,), dtype=float32)
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -27.548275256052886, agent episode reward: [-0.610039064324408, -26.93823619172848], time: 18.179
/anaconda2/envs/py36/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/anaconda2/envs/py36/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
Running avgs for agent 0: q_loss: nan, q_loss2: nan, p_loss: nan, mean_rew: nan, var_rew: nan
Running avgs for agent 1: q_loss: nan, q_loss2: nan, p_loss: nan, mean_rew: nan, var_rew: nan
steps: 49975, episodes: 2000, mean episode reward: -22.13405845230123, agent episode reward: [-6.356688518522673, -15.777369933778559], time: 28.422
Running avgs for agent 0: q_loss: 1.4862693548202515, q_loss2: -1.4834058284759521, p_loss: 0.09784300625324249, mean_rew: -0.08848011629332055, var_rew: 0.5672309056106971
Running avgs for agent 1: q_loss: 1.0108567476272583, q_loss2: -1.0002813339233398, p_loss: 1.4050112962722778, mean_rew: -1.014224607081518, var_rew: 0.3679810990112023
steps: 74975, episodes: 3000, mean episode reward: -10.73407551632669, agent episode reward: [-4.107424118280988, -6.626651398045704], time: 28.03
Running avgs for agent 0: q_loss: 2.4723639488220215, q_loss2: -2.47233247756958, p_loss: 0.35616737604141235, mean_rew: -0.1498995788757597, var_rew: 0.4229275520176078
Running avgs for agent 1: q_loss: 2.409531831741333, q_loss2: -2.4095659255981445, p_loss: 2.0305726528167725, mean_rew: -0.7450117094392743, var_rew: 0.4085769349619049
steps: 99975, episodes: 4000, mean episode reward: -9.564129035347431, agent episode reward: [-2.670678583489496, -6.893450451857936], time: 28.47
Running avgs for agent 0: q_loss: 3.052994728088379, q_loss2: -3.052633285522461, p_loss: 0.5432646870613098, mean_rew: -0.14359814556514325, var_rew: 0.3366410540940171
Running avgs for agent 1: q_loss: 3.5085325241088867, q_loss2: -3.5085361003875732, p_loss: 2.3233680725097656, mean_rew: -0.605599373707486, var_rew: 0.38745494950759535
^[[Bsteps: 124975, episodes: 5000, mean episode reward: -9.176806278468254, agent episode reward: [-2.565299615447234, -6.611506663021021], time: 28.63
Running avgs for agent 0: q_loss: 3.5250444412231445, q_loss2: -3.5240840911865234, p_loss: 0.6568700075149536, mean_rew: -0.13613488455278167, var_rew: 0.28987883085006616
Running avgs for agent 1: q_loss: 4.3088459968566895, q_loss2: -4.30879545211792, p_loss: 2.3746628761291504, mean_rew: -0.5302311263380655, var_rew: 0.35677243957381916
steps: 149975, episodes: 6000, mean episode reward: -9.068147327486418, agent episode reward: [-2.3202930430683937, -6.7478542844180245], time: 28.8
Running avgs for agent 0: q_loss: 3.9051754474639893, q_loss2: -3.902830123901367, p_loss: 0.7488234639167786, mean_rew: -0.12882945934108367, var_rew: 0.2585202646509331
Running avgs for agent 1: q_loss: 5.027271270751953, q_loss2: -5.027207851409912, p_loss: 2.340087890625, mean_rew: -0.4841701418667664, var_rew: 0.33513349649380064
steps: 174975, episodes: 7000, mean episode reward: -9.063685026507985, agent episode reward: [-2.40878632872913, -6.654898697778856], time: 28.464
Running avgs for agent 0: q_loss: 4.231391906738281, q_loss2: -4.22855281829834, p_loss: 0.8015798330307007, mean_rew: -0.12203596432604842, var_rew: 0.23568040032253748
Running avgs for agent 1: q_loss: 5.6237473487854, q_loss2: -5.623535633087158, p_loss: 2.2594780921936035, mean_rew: -0.44997354348981305, var_rew: 0.31536024920507266
steps: 199975, episodes: 8000, mean episode reward: -8.810802069834004, agent episode reward: [-2.225232444342131, -6.585569625491873], time: 28.383
Running avgs for agent 0: q_loss: 4.627523422241211, q_loss2: -4.624110698699951, p_loss: 0.8361551761627197, mean_rew: -0.11900219676848728, var_rew: 0.2234394940920515
Running avgs for agent 1: q_loss: 6.15415620803833, q_loss2: -6.153785228729248, p_loss: 2.1799519062042236, mean_rew: -0.42402923352877825, var_rew: 0.2988505799420291
steps: 224975, episodes: 9000, mean episode reward: -8.947967033996022, agent episode reward: [-2.218324577127789, -6.729642456868234], time: 28.928
Running avgs for agent 0: q_loss: 4.890434741973877, q_loss2: -4.8883514404296875, p_loss: 0.8519917130470276, mean_rew: -0.11699058128356722, var_rew: 0.20886222136165816
Running avgs for agent 1: q_loss: 6.722182750701904, q_loss2: -6.721928119659424, p_loss: 2.1258838176727295, mean_rew: -0.40818320377947703, var_rew: 0.2886174157805798
steps: 249975, episodes: 10000, mean episode reward: -9.132937290926057, agent episode reward: [-2.0929163075734274, -7.04002098335263], time: 28.556
Running avgs for agent 0: q_loss: 5.259955406188965, q_loss2: -5.256251811981201, p_loss: 0.8348777890205383, mean_rew: -0.11177562687613854, var_rew: 0.2016518597073271
Running avgs for agent 1: q_loss: 7.188949108123779, q_loss2: -7.188650608062744, p_loss: 2.062150478363037, mean_rew: -0.3931402131236232, var_rew: 0.2770213584691934
steps: 274975, episodes: 11000, mean episode reward: -8.887138248289917, agent episode reward: [-2.072089260206529, -6.815048988083389], time: 28.64
Running avgs for agent 0: q_loss: 5.554893493652344, q_loss2: -5.551124095916748, p_loss: 0.825073778629303, mean_rew: -0.10948068900214038, var_rew: 0.19352597656987552
Running avgs for agent 1: q_loss: 7.590656280517578, q_loss2: -7.590218544006348, p_loss: 2.00567364692688, mean_rew: -0.38037129942998454, var_rew: 0.26574735696481344
steps: 299975, episodes: 12000, mean episode reward: -8.675042418962065, agent episode reward: [-1.7596141503164124, -6.915428268645654], time: 29.096
Running avgs for agent 0: q_loss: 5.794012069702148, q_loss2: -5.790566444396973, p_loss: 0.8013830780982971, mean_rew: -0.1054089322582697, var_rew: 0.1851982324201377
Running avgs for agent 1: q_loss: 8.070472717285156, q_loss2: -8.070014953613281, p_loss: 1.965207815170288, mean_rew: -0.3705286142711623, var_rew: 0.25907602235119515
^[[Bsteps: 324975, episodes: 13000, mean episode reward: -8.855675831930629, agent episode reward: [-2.2147299778422704, -6.640945854088357], time: 29.51
Running avgs for agent 0: q_loss: 6.177813529968262, q_loss2: -6.174232006072998, p_loss: 0.8098560571670532, mean_rew: -0.10508784190777118, var_rew: 0.18251305635054615
Running avgs for agent 1: q_loss: 8.656947135925293, q_loss2: -8.656522750854492, p_loss: 1.9631361961364746, mean_rew: -0.36518232827164954, var_rew: 0.2567509238998005
steps: 349975, episodes: 14000, mean episode reward: -8.887692158946333, agent episode reward: [-1.9106761957928702, -6.9770159631534625], time: 29.058
Running avgs for agent 0: q_loss: 6.5020341873168945, q_loss2: -6.498629570007324, p_loss: 0.8108009099960327, mean_rew: -0.103283367478718, var_rew: 0.1787099005995111
Running avgs for agent 1: q_loss: 9.046374320983887, q_loss2: -9.045953750610352, p_loss: 1.940492868423462, mean_rew: -0.3583388551914642, var_rew: 0.24945151615607306
steps: 374975, episodes: 15000, mean episode reward: -8.596018387682015, agent episode reward: [-1.7928052972817614, -6.803213090400252], time: 29.391
Running avgs for agent 0: q_loss: 6.745840549468994, q_loss2: -6.7426252365112305, p_loss: 0.8080394268035889, mean_rew: -0.10143458661713871, var_rew: 0.17331908199434082
Running avgs for agent 1: q_loss: 9.471582412719727, q_loss2: -9.470959663391113, p_loss: 1.9208447933197021, mean_rew: -0.35184698600263403, var_rew: 0.24402522396688198
steps: 399975, episodes: 16000, mean episode reward: -8.949295867773351, agent episode reward: [-2.0373641465675543, -6.911931721205796], time: 28.648
Running avgs for agent 0: q_loss: 7.050052642822266, q_loss2: -7.04689359664917, p_loss: 0.7975097298622131, mean_rew: -0.09911710431213176, var_rew: 0.17011748327852808
Running avgs for agent 1: q_loss: 9.923405647277832, q_loss2: -9.922685623168945, p_loss: 1.913083791732788, mean_rew: -0.3484186189695083, var_rew: 0.24002178902280952
steps: 424975, episodes: 17000, mean episode reward: -8.956401202199567, agent episode reward: [-1.7961629576634714, -7.160238244536096], time: 28.731
Running avgs for agent 0: q_loss: 7.411481857299805, q_loss2: -7.4083027839660645, p_loss: 0.79183030128479, mean_rew: -0.09780703178835784, var_rew: 0.16863856303695351
Running avgs for agent 1: q_loss: 10.344467163085938, q_loss2: -10.343829154968262, p_loss: 1.8959890604019165, mean_rew: -0.3436711751784953, var_rew: 0.2358934940992032
steps: 449975, episodes: 18000, mean episode reward: -8.598841480490835, agent episode reward: [-1.9213812925522433, -6.677460187938592], time: 29.596
Running avgs for agent 0: q_loss: 7.703541278839111, q_loss2: -7.700275421142578, p_loss: 0.77639240026474, mean_rew: -0.09544915008251191, var_rew: 0.16587576786837774
Running avgs for agent 1: q_loss: 10.760154724121094, q_loss2: -10.759482383728027, p_loss: 1.8793737888336182, mean_rew: -0.33918647604727853, var_rew: 0.23207546151168762
steps: 474975, episodes: 19000, mean episode reward: -8.745232062904824, agent episode reward: [-1.9761228283928878, -6.769109234511936], time: 29.242
Running avgs for agent 0: q_loss: 7.9927978515625, q_loss2: -7.98957633972168, p_loss: 0.7620381116867065, mean_rew: -0.09584603201015025, var_rew: 0.16330021945506445
Running avgs for agent 1: q_loss: 11.154698371887207, q_loss2: -11.153792381286621, p_loss: 1.8599369525909424, mean_rew: -0.3339743928397253, var_rew: 0.22826355281935493
steps: 499975, episodes: 20000, mean episode reward: -8.461863748845103, agent episode reward: [-1.7820339596617674, -6.679829789183336], time: 29.031
Running avgs for agent 0: q_loss: 8.306906700134277, q_loss2: -8.303955078125, p_loss: 0.7336063385009766, mean_rew: -0.09454285958534561, var_rew: 0.16148805652546003
Running avgs for agent 1: q_loss: 11.552750587463379, q_loss2: -11.552154541015625, p_loss: 1.8568682670593262, mean_rew: -0.332715822273035, var_rew: 0.22489318895141935
steps: 524975, episodes: 21000, mean episode reward: -8.61215693621722, agent episode reward: [-1.7521909779359899, -6.85996595828123], time: 28.668
Running avgs for agent 0: q_loss: 8.526468276977539, q_loss2: -8.522833824157715, p_loss: 0.7033554911613464, mean_rew: -0.09245722490680865, var_rew: 0.1580207126994859
Running avgs for agent 1: q_loss: 11.887775421142578, q_loss2: -11.886528968811035, p_loss: 1.8357021808624268, mean_rew: -0.327894227687687, var_rew: 0.22063211755640474
steps: 549975, episodes: 22000, mean episode reward: -8.715920397859751, agent episode reward: [-2.0417582277282365, -6.674162170131514], time: 28.39
Running avgs for agent 0: q_loss: 8.796878814697266, q_loss2: -8.793471336364746, p_loss: 0.6802552938461304, mean_rew: -0.09072042246880764, var_rew: 0.15586610071723345
Running avgs for agent 1: q_loss: 12.42155647277832, q_loss2: -12.420486450195312, p_loss: 1.8260177373886108, mean_rew: -0.32568687868051155, var_rew: 0.22037166847547635
steps: 574975, episodes: 23000, mean episode reward: -9.034690739415048, agent episode reward: [-2.1916531856061336, -6.843037553808914], time: 28.502
Running avgs for agent 0: q_loss: 9.113273620605469, q_loss2: -9.109723091125488, p_loss: 0.6769678592681885, mean_rew: -0.09321497288731258, var_rew: 0.154646979569966
Running avgs for agent 1: q_loss: 12.665982246398926, q_loss2: -12.66556167602539, p_loss: 1.8033381700515747, mean_rew: -0.3224664607605239, var_rew: 0.21525538730202723
steps: 599975, episodes: 24000, mean episode reward: -9.062765660730129, agent episode reward: [-2.17913270342398, -6.883632957306148], time: 28.57
Running avgs for agent 0: q_loss: 9.42687702178955, q_loss2: -9.423463821411133, p_loss: 0.6547073721885681, mean_rew: -0.09184461490018307, var_rew: 0.15348187399903093
Running avgs for agent 1: q_loss: 13.277003288269043, q_loss2: -13.276433944702148, p_loss: 1.8010852336883545, mean_rew: -0.3221830657648944, var_rew: 0.21641103294810973
steps: 624975, episodes: 25000, mean episode reward: -8.86253867171134, agent episode reward: [-1.9716356367755348, -6.890903034935805], time: 28.445
Running avgs for agent 0: q_loss: 9.736760139465332, q_loss2: -9.733345031738281, p_loss: 0.64988112449646, mean_rew: -0.09278460376326907, var_rew: 0.15237721132594778
Running avgs for agent 1: q_loss: 13.687928199768066, q_loss2: -13.687002182006836, p_loss: 1.7834951877593994, mean_rew: -0.3193605511196214, var_rew: 0.21443587333972053
steps: 649975, episodes: 26000, mean episode reward: -8.692202919668139, agent episode reward: [-2.0080918598042725, -6.6841110598638664], time: 28.644
Running avgs for agent 0: q_loss: 10.081615447998047, q_loss2: -10.078336715698242, p_loss: 0.6361693739891052, mean_rew: -0.09008631670597209, var_rew: 0.1518687189786596
Running avgs for agent 1: q_loss: 14.1045560836792, q_loss2: -14.103877067565918, p_loss: 1.7720383405685425, mean_rew: -0.31822780017688623, var_rew: 0.2126749622658641
steps: 674975, episodes: 27000, mean episode reward: -8.977336973820629, agent episode reward: [-2.181118335325565, -6.796218638495065], time: 28.549
Running avgs for agent 0: q_loss: 10.260808944702148, q_loss2: -10.257474899291992, p_loss: 0.6562484502792358, mean_rew: -0.09140103092811767, var_rew: 0.14900675959646106
Running avgs for agent 1: q_loss: 14.539509773254395, q_loss2: -14.53894329071045, p_loss: 1.7590197324752808, mean_rew: -0.3161643347643195, var_rew: 0.2112698591803001
steps: 699975, episodes: 28000, mean episode reward: -8.910373964233194, agent episode reward: [-2.329024582572726, -6.581349381660469], time: 29.22
Running avgs for agent 0: q_loss: 10.597404479980469, q_loss2: -10.594172477722168, p_loss: 0.6629490256309509, mean_rew: -0.09047979995746736, var_rew: 0.14852751854567833
Running avgs for agent 1: q_loss: 15.086631774902344, q_loss2: -15.086054801940918, p_loss: 1.7525579929351807, mean_rew: -0.3152019328646818, var_rew: 0.21161077331257583
steps: 724975, episodes: 29000, mean episode reward: -8.880654456727608, agent episode reward: [-2.2553797868332217, -6.625274669894387], time: 28.498
Running avgs for agent 0: q_loss: 11.016528129577637, q_loss2: -11.013559341430664, p_loss: 0.6898406744003296, mean_rew: -0.09077243935654311, var_rew: 0.1492035022984078
Running avgs for agent 1: q_loss: 15.271484375, q_loss2: -15.271015167236328, p_loss: 1.7230311632156372, mean_rew: -0.311589340945881, var_rew: 0.20697862372099846
steps: 749975, episodes: 30000, mean episode reward: -8.873401576944728, agent episode reward: [-2.257652060841931, -6.615749516102798], time: 28.391
Running avgs for agent 0: q_loss: 11.020767211914062, q_loss2: -11.018092155456543, p_loss: 0.70745849609375, mean_rew: -0.09123479208573218, var_rew: 0.14443023754036452
Running avgs for agent 1: q_loss: 15.710556983947754, q_loss2: -15.710320472717285, p_loss: 1.714812159538269, mean_rew: -0.31007475119671385, var_rew: 0.20598810934227998
steps: 774975, episodes: 31000, mean episode reward: -8.677808215120745, agent episode reward: [-2.031246333621805, -6.646561881498941], time: 28.522
Running avgs for agent 0: q_loss: 11.520769119262695, q_loss2: -11.517958641052246, p_loss: 0.7216794490814209, mean_rew: -0.09070295746562008, var_rew: 0.14622226095054497
Running avgs for agent 1: q_loss: 16.229473114013672, q_loss2: -16.229082107543945, p_loss: 1.71146559715271, mean_rew: -0.309784260829049, var_rew: 0.20607211548779467
steps: 799975, episodes: 32000, mean episode reward: -8.745251998028099, agent episode reward: [-1.568134180833723, -7.177117817194377], time: 28.445
Running avgs for agent 0: q_loss: 11.785059928894043, q_loss2: -11.782280921936035, p_loss: 0.7064635753631592, mean_rew: -0.08868938852909415, var_rew: 0.14501263298279055
Running avgs for agent 1: q_loss: 16.481191635131836, q_loss2: -16.480831146240234, p_loss: 1.6969846487045288, mean_rew: -0.30832409321839777, var_rew: 0.20286800456821338
steps: 824975, episodes: 33000, mean episode reward: -8.754829645315903, agent episode reward: [-1.4261873908674503, -7.328642254448454], time: 28.344
Running avgs for agent 0: q_loss: 12.0983304977417, q_loss2: -12.095236778259277, p_loss: 0.6942243576049805, mean_rew: -0.0888254076296683, var_rew: 0.1444330199134346
Running avgs for agent 1: q_loss: 16.968616485595703, q_loss2: -16.967906951904297, p_loss: 1.6985251903533936, mean_rew: -0.3086857854879481, var_rew: 0.20265860944126127
steps: 849975, episodes: 34000, mean episode reward: -8.774383314728878, agent episode reward: [-1.4396798604151673, -7.33470345431371], time: 28.441
Running avgs for agent 0: q_loss: 12.307287216186523, q_loss2: -12.304527282714844, p_loss: 0.6685487627983093, mean_rew: -0.0878034203856308, var_rew: 0.14272178478599426
Running avgs for agent 1: q_loss: 17.3091983795166, q_loss2: -17.30876922607422, p_loss: 1.6975549459457397, mean_rew: -0.3070345816643131, var_rew: 0.20079070731635928
steps: 874975, episodes: 35000, mean episode reward: -8.90461070623512, agent episode reward: [-0.988779735281328, -7.915830970953792], time: 28.608
Running avgs for agent 0: q_loss: 12.590578079223633, q_loss2: -12.587565422058105, p_loss: 0.6413236856460571, mean_rew: -0.08764772124695054, var_rew: 0.14193772793770906
Running avgs for agent 1: q_loss: 17.84632110595703, q_loss2: -17.846046447753906, p_loss: 1.7132092714309692, mean_rew: -0.30733243059185655, var_rew: 0.20117976210932817
steps: 899975, episodes: 36000, mean episode reward: -8.841763839768104, agent episode reward: [-1.017034207936549, -7.824729631831555], time: 28.369
Running avgs for agent 0: q_loss: 12.874329566955566, q_loss2: -12.871679306030273, p_loss: 0.5831387042999268, mean_rew: -0.08484448200177422, var_rew: 0.14118853824374758
Running avgs for agent 1: q_loss: 18.240293502807617, q_loss2: -18.23985481262207, p_loss: 1.7095489501953125, mean_rew: -0.306509750722316, var_rew: 0.2000346928420821
steps: 924975, episodes: 37000, mean episode reward: -9.210599128465276, agent episode reward: [-1.9727211601899044, -7.237877968275372], time: 28.512
Running avgs for agent 0: q_loss: 13.26661491394043, q_loss2: -13.263548851013184, p_loss: 0.5382678508758545, mean_rew: -0.08484652957610471, var_rew: 0.1416117523190463
Running avgs for agent 1: q_loss: 18.55299949645996, q_loss2: -18.552804946899414, p_loss: 1.693838119506836, mean_rew: -0.307248610151371, var_rew: 0.1980772755143957
steps: 949975, episodes: 38000, mean episode reward: -8.873053025361804, agent episode reward: [-2.3191157987634177, -6.553937226598385], time: 28.868
Running avgs for agent 0: q_loss: 13.502474784851074, q_loss2: -13.499835968017578, p_loss: 0.4908628463745117, mean_rew: -0.08434568221269162, var_rew: 0.14041764916802743
Running avgs for agent 1: q_loss: 19.020957946777344, q_loss2: -19.020702362060547, p_loss: 1.7074066400527954, mean_rew: -0.3070892802715856, var_rew: 0.19779317542607958
steps: 974975, episodes: 39000, mean episode reward: -8.664219091921364, agent episode reward: [-1.6276683985278788, -7.036550693393487], time: 28.893
Running avgs for agent 0: q_loss: 13.888668060302734, q_loss2: -13.885436058044434, p_loss: 0.4651941657066345, mean_rew: -0.08564122876358092, var_rew: 0.1407788634364992
Running avgs for agent 1: q_loss: 19.609159469604492, q_loss2: -19.608531951904297, p_loss: 1.7224092483520508, mean_rew: -0.30675994929067973, var_rew: 0.19877951596077575
steps: 999975, episodes: 40000, mean episode reward: -8.838934429136517, agent episode reward: [-1.2666532128246257, -7.57228121631189], time: 28.91
Running avgs for agent 0: q_loss: 14.046189308166504, q_loss2: -14.043452262878418, p_loss: 0.4324905276298523, mean_rew: -0.0844358409289653, var_rew: 0.1388979624478248
Running avgs for agent 1: q_loss: 19.905378341674805, q_loss2: -19.905160903930664, p_loss: 1.7143878936767578, mean_rew: -0.3063185588761085, var_rew: 0.19681684081618434
steps: 1024975, episodes: 41000, mean episode reward: -8.534147403218705, agent episode reward: [-0.7441265381124433, -7.790020865106262], time: 28.913
Running avgs for agent 0: q_loss: 13.88748836517334, q_loss2: -13.884566307067871, p_loss: 0.38676702976226807, mean_rew: -0.08331677519857206, var_rew: 0.1340807583285255
Running avgs for agent 1: q_loss: 19.366254806518555, q_loss2: -19.36594009399414, p_loss: 1.649288296699524, mean_rew: -0.2967846733945686, var_rew: 0.18696094978393854
steps: 1049975, episodes: 42000, mean episode reward: -8.645965001289394, agent episode reward: [-1.2106924597034532, -7.4352725415859435], time: 29.009
Running avgs for agent 0: q_loss: 13.002514839172363, q_loss2: -13.000659942626953, p_loss: 0.3342724144458771, mean_rew: -0.08244109902161897, var_rew: 0.1226953454352408
Running avgs for agent 1: q_loss: 17.714704513549805, q_loss2: -17.71458625793457, p_loss: 1.5191229581832886, mean_rew: -0.2794430158550858, var_rew: 0.16707947023969058
steps: 1074975, episodes: 43000, mean episode reward: -8.604010324041152, agent episode reward: [-1.4350083612081272, -7.169001962833025], time: 28.868
Running avgs for agent 0: q_loss: 13.024606704711914, q_loss2: -13.022866249084473, p_loss: 0.29797375202178955, mean_rew: -0.07754218473517155, var_rew: 0.12009839177579011
Running avgs for agent 1: q_loss: 18.101070404052734, q_loss2: -18.10091781616211, p_loss: 1.5101889371871948, mean_rew: -0.27754915900721705, var_rew: 0.1668234765606747
steps: 1099975, episodes: 44000, mean episode reward: -8.564772543077666, agent episode reward: [-1.7027988646884247, -6.861973678389241], time: 29.064
Running avgs for agent 0: q_loss: 13.272031784057617, q_loss2: -13.270308494567871, p_loss: 0.29990753531455994, mean_rew: -0.07674045573986278, var_rew: 0.1196413149577729
Running avgs for agent 1: q_loss: 18.403074264526367, q_loss2: -18.402957916259766, p_loss: 1.4978936910629272, mean_rew: -0.27819636415826043, var_rew: 0.1658054956514675
steps: 1124975, episodes: 45000, mean episode reward: -8.591294031962398, agent episode reward: [-2.1401709599275094, -6.451123072034887], time: 29.031
Running avgs for agent 0: q_loss: 13.47577953338623, q_loss2: -13.474218368530273, p_loss: 0.2950427830219269, mean_rew: -0.0749198724550231, var_rew: 0.11882111973790652
Running avgs for agent 1: q_loss: 18.95161247253418, q_loss2: -18.951351165771484, p_loss: 1.4922077655792236, mean_rew: -0.2780949579672346, var_rew: 0.16699963436678802
steps: 1149975, episodes: 46000, mean episode reward: -8.741114199064853, agent episode reward: [-2.055385161259773, -6.68572903780508], time: 29.132
Running avgs for agent 0: q_loss: 13.741181373596191, q_loss2: -13.739608764648438, p_loss: 0.29201292991638184, mean_rew: -0.07440790630640416, var_rew: 0.11855648142245238
Running avgs for agent 1: q_loss: 19.263835906982422, q_loss2: -19.263538360595703, p_loss: 1.4888641834259033, mean_rew: -0.27719797511188293, var_rew: 0.16613593136249802
steps: 1174975, episodes: 47000, mean episode reward: -8.74656572346065, agent episode reward: [-1.9824740607380458, -6.764091662722605], time: 29.047
Running avgs for agent 0: q_loss: 14.155845642089844, q_loss2: -14.153965950012207, p_loss: 0.2832191288471222, mean_rew: -0.07523141398325534, var_rew: 0.1195779269191207
Running avgs for agent 1: q_loss: 19.671741485595703, q_loss2: -19.671480178833008, p_loss: 1.490976333618164, mean_rew: -0.2773820758101154, var_rew: 0.16608453499674652
steps: 1199975, episodes: 48000, mean episode reward: -8.54198844399919, agent episode reward: [-1.7203889623062145, -6.821599481692974], time: 28.551
Running avgs for agent 0: q_loss: 14.412583351135254, q_loss2: -14.41087532043457, p_loss: 0.2672228515148163, mean_rew: -0.07355522147413218, var_rew: 0.1192632132910698
Running avgs for agent 1: q_loss: 20.14200782775879, q_loss2: -20.14189910888672, p_loss: 1.4926711320877075, mean_rew: -0.2774994843590599, var_rew: 0.1665538327584744
steps: 1224975, episodes: 49000, mean episode reward: -8.526472178700637, agent episode reward: [-1.838747005940901, -6.687725172759735], time: 29.11
Running avgs for agent 0: q_loss: 14.427109718322754, q_loss2: -14.425683975219727, p_loss: 0.26049092411994934, mean_rew: -0.07354283535296244, var_rew: 0.11698487132572982
Running avgs for agent 1: q_loss: 20.60341453552246, q_loss2: -20.60325813293457, p_loss: 1.4961936473846436, mean_rew: -0.2792212753804741, var_rew: 0.1669396376512086
steps: 1249975, episodes: 50000, mean episode reward: -8.479135792409783, agent episode reward: [-1.4374930766908711, -7.041642715718912], time: 29.182
Running avgs for agent 0: q_loss: 15.001585960388184, q_loss2: -15.000081062316895, p_loss: 0.2395097017288208, mean_rew: -0.07170035888243945, var_rew: 0.11922859845829609
Running avgs for agent 1: q_loss: 21.160852432250977, q_loss2: -21.16067123413086, p_loss: 1.5009171962738037, mean_rew: -0.2797475024640046, var_rew: 0.16807244318778203
steps: 1274975, episodes: 51000, mean episode reward: -8.31233586387814, agent episode reward: [-1.462034778905677, -6.850301084972463], time: 29.019
Running avgs for agent 0: q_loss: 15.2179594039917, q_loss2: -15.216490745544434, p_loss: 0.22931310534477234, mean_rew: -0.07258718655138038, var_rew: 0.11862178476920496
Running avgs for agent 1: q_loss: 21.381080627441406, q_loss2: -21.380863189697266, p_loss: 1.4982388019561768, mean_rew: -0.27829792089781336, var_rew: 0.16654185579460637
steps: 1299975, episodes: 52000, mean episode reward: -8.426272160108162, agent episode reward: [-1.371464681535275, -7.054807478572887], time: 29.196
Running avgs for agent 0: q_loss: 15.534341812133789, q_loss2: -15.532578468322754, p_loss: 0.2142000049352646, mean_rew: -0.07189309802733436, var_rew: 0.11879960235045074
Running avgs for agent 1: q_loss: 21.531957626342773, q_loss2: -21.531770706176758, p_loss: 1.4901957511901855, mean_rew: -0.27658203455001895, var_rew: 0.16453790649282377
steps: 1324975, episodes: 53000, mean episode reward: -8.360185872235371, agent episode reward: [-1.122242993777018, -7.237942878458354], time: 29.052
Running avgs for agent 0: q_loss: 15.774801254272461, q_loss2: -15.77332878112793, p_loss: 0.19750811159610748, mean_rew: -0.07044228074766486, var_rew: 0.11838717017654891
Running avgs for agent 1: q_loss: 22.144609451293945, q_loss2: -22.14438247680664, p_loss: 1.5010457038879395, mean_rew: -0.2790346540955159, var_rew: 0.16606252600397495
steps: 1349975, episodes: 54000, mean episode reward: -8.604392929352619, agent episode reward: [-1.6012747715041353, -7.003118157848483], time: 29.071
Running avgs for agent 0: q_loss: 15.923455238342285, q_loss2: -15.921515464782715, p_loss: 0.18655645847320557, mean_rew: -0.06964970076112545, var_rew: 0.11732601980947178
Running avgs for agent 1: q_loss: 22.376630783081055, q_loss2: -22.376453399658203, p_loss: 1.4908171892166138, mean_rew: -0.27760224649293036, var_rew: 0.16474956568034468
steps: 1374975, episodes: 55000, mean episode reward: -8.646610140851946, agent episode reward: [-1.3816217857181397, -7.264988355133807], time: 28.939
Running avgs for agent 0: q_loss: 16.369495391845703, q_loss2: -16.367828369140625, p_loss: 0.18041524291038513, mean_rew: -0.06999680870269706, var_rew: 0.11845082513113535
Running avgs for agent 1: q_loss: 22.99345588684082, q_loss2: -22.993188858032227, p_loss: 1.4894534349441528, mean_rew: -0.2795291101043295, var_rew: 0.16624040105702653
steps: 1399975, episodes: 56000, mean episode reward: -8.319576155962245, agent episode reward: [-1.0071597344227734, -7.3124164215394725], time: 28.977
Running avgs for agent 0: q_loss: 16.583375930786133, q_loss2: -16.581832885742188, p_loss: 0.16337332129478455, mean_rew: -0.06897936705662283, var_rew: 0.11788549556454939
Running avgs for agent 1: q_loss: 23.299800872802734, q_loss2: -23.299562454223633, p_loss: 1.488842487335205, mean_rew: -0.27853138927271875, var_rew: 0.1654885925793083
steps: 1424975, episodes: 57000, mean episode reward: -8.536199969008784, agent episode reward: [-1.174241410297157, -7.3619585587116285], time: 28.91
Running avgs for agent 0: q_loss: 16.863800048828125, q_loss2: -16.862354278564453, p_loss: 0.13944365084171295, mean_rew: -0.06962789329043165, var_rew: 0.11780453269058236
Running avgs for agent 1: q_loss: 23.822933197021484, q_loss2: -23.822742462158203, p_loss: 1.5144929885864258, mean_rew: -0.2803350161920007, var_rew: 0.16626658087420304
steps: 1449975, episodes: 58000, mean episode reward: -8.629613044441506, agent episode reward: [-1.1524199495889593, -7.477193094852546], time: 28.97
Running avgs for agent 0: q_loss: 17.098173141479492, q_loss2: -17.096359252929688, p_loss: 0.09308143705129623, mean_rew: -0.06806803347399851, var_rew: 0.11740273615981309
Running avgs for agent 1: q_loss: 24.326553344726562, q_loss2: -24.32635498046875, p_loss: 1.5427809953689575, mean_rew: -0.2803974954968656, var_rew: 0.16688359193440738
steps: 1474975, episodes: 59000, mean episode reward: -9.005953720681807, agent episode reward: [-1.845381419192438, -7.1605723014893705], time: 29.182
Running avgs for agent 0: q_loss: 17.299001693725586, q_loss2: -17.297483444213867, p_loss: 0.042683836072683334, mean_rew: -0.06610115985950059, var_rew: 0.1167891780064406
Running avgs for agent 1: q_loss: 24.50932502746582, q_loss2: -24.50905418395996, p_loss: 1.5767796039581299, mean_rew: -0.27996494403958505, var_rew: 0.16533310741488583
steps: 1499975, episodes: 60000, mean episode reward: -8.763894716599815, agent episode reward: [-1.8894144285361847, -6.874480288063631], time: 29.132
Running avgs for agent 0: q_loss: 17.64776039123535, q_loss2: -17.64623260498047, p_loss: 0.05611298978328705, mean_rew: -0.06769275913526135, var_rew: 0.11717745055752159
Running avgs for agent 1: q_loss: 25.062929153442383, q_loss2: -25.062705993652344, p_loss: 1.6232469081878662, mean_rew: -0.28134900928523404, var_rew: 0.16627385382700907
Traceback (most recent call last):

