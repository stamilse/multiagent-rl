python train_vanila.py --scenario simple_push --num-adversaries 1 --independent-learner True
2018-10-27 22:05:28.160478: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -27.19188165830991, agent episode reward: [-1.2944392600221741, -25.897442398287737], time: 18.222
/anaconda2/envs/py36/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/anaconda2/envs/py36/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, var_rew: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, var_rew: nan
steps: 49975, episodes: 2000, mean episode reward: -23.323312979479038, agent episode reward: [-8.065071463375627, -15.258241516103407], time: 25.199
Running avgs for agent 0: q_loss: 0.19498136639595032, p_loss: 0.011571122333407402, mean_rew: -0.10399155544323821, var_rew: 0.5449094923639795
Running avgs for agent 1: q_loss: 0.018741291016340256, p_loss: 1.4520262479782104, mean_rew: -0.968497221034014, var_rew: 0.3064655999492917
steps: 74975, episodes: 3000, mean episode reward: -16.593359972706626, agent episode reward: [-9.93753506685444, -6.655824905852185], time: 24.398
Running avgs for agent 0: q_loss: 0.15931560099124908, p_loss: 0.32081106305122375, mean_rew: -0.22634848295577853, var_rew: 0.4746119961082481
Running avgs for agent 1: q_loss: 0.00249564740806818, p_loss: 2.0750718116760254, mean_rew: -0.7198460737695428, var_rew: 0.35897665805296597
steps: 99975, episodes: 4000, mean episode reward: -15.905214826544768, agent episode reward: [-9.613416518670755, -6.2917983078740125], time: 24.534
Running avgs for agent 0: q_loss: 0.16865597665309906, p_loss: 0.7233869433403015, mean_rew: -0.27791870550376296, var_rew: 0.41046085843015
Running avgs for agent 1: q_loss: 0.0033107788767665625, p_loss: 2.3028388023376465, mean_rew: -0.5850625829256348, var_rew: 0.34840713413293695
steps: 124975, episodes: 5000, mean episode reward: -14.426916272920476, agent episode reward: [-7.7106243787564415, -6.716291894164034], time: 24.465
Running avgs for agent 0: q_loss: 0.20224997401237488, p_loss: 1.124784231185913, mean_rew: -0.2917428480949194, var_rew: 0.36365447302215737
Running avgs for agent 1: q_loss: 0.003344945376738906, p_loss: 2.319591760635376, mean_rew: -0.5126638489357963, var_rew: 0.327415007074396
steps: 149975, episodes: 6000, mean episode reward: -13.223314582944532, agent episode reward: [-6.713248992720516, -6.510065590224017], time: 24.578
Running avgs for agent 0: q_loss: 0.24654661118984222, p_loss: 1.4335390329360962, mean_rew: -0.29064335283140647, var_rew: 0.328553521817232
Running avgs for agent 1: q_loss: 0.0038241578731685877, p_loss: 2.2435545921325684, mean_rew: -0.4657329079609131, var_rew: 0.30906419791297346
steps: 174975, episodes: 7000, mean episode reward: -12.734622078437964, agent episode reward: [-6.03574007852435, -6.698881999913615], time: 24.488
Running avgs for agent 0: q_loss: 0.28335288166999817, p_loss: 1.6357282400131226, mean_rew: -0.28783073094317785, var_rew: 0.3031238494601869
Running avgs for agent 1: q_loss: 0.0036961149889975786, p_loss: 2.1524784564971924, mean_rew: -0.4350827061294908, var_rew: 0.2927541005205728
steps: 199975, episodes: 8000, mean episode reward: -11.811717164961141, agent episode reward: [-5.225820779919928, -6.585896385041213], time: 24.484
Running avgs for agent 0: q_loss: 0.30095556378364563, p_loss: 1.6872390508651733, mean_rew: -0.2768290347845266, var_rew: 0.2856799336662426
Running avgs for agent 1: q_loss: 0.004083475098013878, p_loss: 2.0751631259918213, mean_rew: -0.41414119491313717, var_rew: 0.28168319737652986
steps: 224975, episodes: 9000, mean episode reward: -11.153701975705209, agent episode reward: [-4.683132602693089, -6.470569373012121], time: 24.437
Running avgs for agent 0: q_loss: 0.31548669934272766, p_loss: 1.6685618162155151, mean_rew: -0.26749096558898716, var_rew: 0.26874415239778404
Running avgs for agent 1: q_loss: 0.004311731550842524, p_loss: 1.9881428480148315, mean_rew: -0.3946048040193242, var_rew: 0.26990407476306577
steps: 249975, episodes: 10000, mean episode reward: -11.271409059121808, agent episode reward: [-4.712487505913763, -6.558921553208045], time: 24.705
Running avgs for agent 0: q_loss: 0.31598249077796936, p_loss: 1.6172828674316406, mean_rew: -0.25952676236898786, var_rew: 0.2538762850566666
Running avgs for agent 1: q_loss: 0.0036272560246288776, p_loss: 1.9148123264312744, mean_rew: -0.3786339957435661, var_rew: 0.2596357824294486
steps: 274975, episodes: 11000, mean episode reward: -10.871217045546667, agent episode reward: [-4.331219937803927, -6.53999710774274], time: 24.306
Running avgs for agent 0: q_loss: 0.31906938552856445, p_loss: 1.553580403327942, mean_rew: -0.25173578929722895, var_rew: 0.2435286589907363
Running avgs for agent 1: q_loss: 0.0036299903877079487, p_loss: 1.87558114528656, mean_rew: -0.36949009747348943, var_rew: 0.254180554932937
steps: 299975, episodes: 12000, mean episode reward: -10.949256687546633, agent episode reward: [-4.446499959634473, -6.50275672791216], time: 24.263
Running avgs for agent 0: q_loss: 0.30508238077163696, p_loss: 1.5367075204849243, mean_rew: -0.24707046513662534, var_rew: 0.23191564340330434
Running avgs for agent 1: q_loss: 0.004540828987956047, p_loss: 1.8313405513763428, mean_rew: -0.3594781770968473, var_rew: 0.24786630082788397
steps: 324975, episodes: 13000, mean episode reward: -10.987500525641028, agent episode reward: [-4.382416214513763, -6.605084311127262], time: 24.185
Running avgs for agent 0: q_loss: 0.3101033866405487, p_loss: 1.5241484642028809, mean_rew: -0.23982860327633884, var_rew: 0.22553337725821687
Running avgs for agent 1: q_loss: 0.003947746474295855, p_loss: 1.7961963415145874, mean_rew: -0.35170369899971876, var_rew: 0.24213087498474867
steps: 349975, episodes: 14000, mean episode reward: -11.057043323111003, agent episode reward: [-4.321426518793982, -6.735616804317024], time: 24.196
Running avgs for agent 0: q_loss: 0.32826894521713257, p_loss: 1.5202897787094116, mean_rew: -0.23520660235145954, var_rew: 0.2173448917821686
Running avgs for agent 1: q_loss: 0.0033400144893676043, p_loss: 1.7659536600112915, mean_rew: -0.3456170524809203, var_rew: 0.23621954449167168
steps: 374975, episodes: 15000, mean episode reward: -11.050291083507688, agent episode reward: [-4.438507930294118, -6.61178315321357], time: 24.128
Running avgs for agent 0: q_loss: 0.3169628977775574, p_loss: 1.5101842880249023, mean_rew: -0.2297924479032047, var_rew: 0.2102353934015965
Running avgs for agent 1: q_loss: 0.005128613207489252, p_loss: 1.7565906047821045, mean_rew: -0.341489951071758, var_rew: 0.23365046394623334
steps: 399975, episodes: 16000, mean episode reward: -10.777140762019602, agent episode reward: [-4.210253921691572, -6.566886840328032], time: 24.183
Running avgs for agent 0: q_loss: 0.3265816271305084, p_loss: 1.5392868518829346, mean_rew: -0.22918896617216242, var_rew: 0.20398171524355224
Running avgs for agent 1: q_loss: 0.0039374325424432755, p_loss: 1.734331488609314, mean_rew: -0.3361459516360524, var_rew: 0.22885845953845424
steps: 424975, episodes: 17000, mean episode reward: -10.765637778976295, agent episode reward: [-4.080864887132852, -6.684772891843443], time: 23.91
Running avgs for agent 0: q_loss: 0.33217304944992065, p_loss: 1.5036062002182007, mean_rew: -0.2237258966476021, var_rew: 0.19863680619259946
Running avgs for agent 1: q_loss: 0.0031664231792092323, p_loss: 1.7122092247009277, mean_rew: -0.33161039464495534, var_rew: 0.2242883985480001
steps: 449975, episodes: 18000, mean episode reward: -10.906144145928884, agent episode reward: [-4.262219745386439, -6.643924400542445], time: 23.94
Running avgs for agent 0: q_loss: 0.3501090109348297, p_loss: 1.4907981157302856, mean_rew: -0.22020023349436316, var_rew: 0.19532712371346525
Running avgs for agent 1: q_loss: 0.005393343046307564, p_loss: 1.7076886892318726, mean_rew: -0.32963783743583724, var_rew: 0.22401446611756304
steps: 474975, episodes: 19000, mean episode reward: -11.08444976026375, agent episode reward: [-4.413810110865745, -6.6706396493980025], time: 23.993
Running avgs for agent 0: q_loss: 0.3702104687690735, p_loss: 1.4723222255706787, mean_rew: -0.21839114189622846, var_rew: 0.18978604631364512
Running avgs for agent 1: q_loss: 0.003670393954962492, p_loss: 1.668619155883789, mean_rew: -0.32380442767299517, var_rew: 0.21844494916707247
steps: 499975, episodes: 20000, mean episode reward: -11.147721057247658, agent episode reward: [-4.366497770833403, -6.781223286414253], time: 23.957
Running avgs for agent 0: q_loss: 0.38410624861717224, p_loss: 1.4616150856018066, mean_rew: -0.21637322981515653, var_rew: 0.1885039078189097
Running avgs for agent 1: q_loss: 0.004014818929135799, p_loss: 1.6546028852462769, mean_rew: -0.32116873744856655, var_rew: 0.2176608126308185
steps: 524975, episodes: 21000, mean episode reward: -10.923438594033298, agent episode reward: [-4.739863064504812, -6.183575529528487], time: 23.952
Running avgs for agent 0: q_loss: 0.36963874101638794, p_loss: 1.4612904787063599, mean_rew: -0.21542066571768193, var_rew: 0.18363127745946423
Running avgs for agent 1: q_loss: 0.0039043128490448, p_loss: 1.6448216438293457, mean_rew: -0.32029638968964175, var_rew: 0.21672401330283977
steps: 549975, episodes: 22000, mean episode reward: -10.93634663394799, agent episode reward: [-4.404068356660989, -6.532278277286999], time: 23.96
Running avgs for agent 0: q_loss: 0.42771270871162415, p_loss: 1.434170126914978, mean_rew: -0.2130991577511191, var_rew: 0.18082757419577447
Running avgs for agent 1: q_loss: 0.005670784041285515, p_loss: 1.6115561723709106, mean_rew: -0.3140726418052218, var_rew: 0.21265648744027527
steps: 574975, episodes: 23000, mean episode reward: -10.916686537014666, agent episode reward: [-4.183507739513254, -6.733178797501411], time: 24.286
Running avgs for agent 0: q_loss: 0.4149181842803955, p_loss: 1.438858151435852, mean_rew: -0.2109791252105142, var_rew: 0.17925084229341512
Running avgs for agent 1: q_loss: 0.004002412781119347, p_loss: 1.6070201396942139, mean_rew: -0.3142314430344162, var_rew: 0.2119790428877812
steps: 599975, episodes: 24000, mean episode reward: -11.230230837421129, agent episode reward: [-4.588419724894551, -6.641811112526579], time: 24.614
Running avgs for agent 0: q_loss: 0.4727548360824585, p_loss: 1.4350091218948364, mean_rew: -0.20917135025319045, var_rew: 0.1756302690305229
Running avgs for agent 1: q_loss: 0.0037326766178011894, p_loss: 1.590366005897522, mean_rew: -0.31147540777028443, var_rew: 0.2094027419286551
steps: 624975, episodes: 25000, mean episode reward: -11.35130967229166, agent episode reward: [-4.884796254817649, -6.466513417474011], time: 24.676
Running avgs for agent 0: q_loss: 0.5178709626197815, p_loss: 1.4079631567001343, mean_rew: -0.2084305831088061, var_rew: 0.17419953937089427
Running avgs for agent 1: q_loss: 0.005381889641284943, p_loss: 1.5956484079360962, mean_rew: -0.3110610318249044, var_rew: 0.21066761352836738
steps: 649975, episodes: 26000, mean episode reward: -11.989614200407972, agent episode reward: [-5.3143754536426275, -6.675238746765346], time: 24.397
Running avgs for agent 0: q_loss: 0.5604835152626038, p_loss: 1.4123280048370361, mean_rew: -0.20875534664822942, var_rew: 0.17287015140554488
Running avgs for agent 1: q_loss: 0.005192288197577, p_loss: 1.57716965675354, mean_rew: -0.3089733584890609, var_rew: 0.20806592715790806
steps: 674975, episodes: 27000, mean episode reward: -11.074335294285197, agent episode reward: [-5.010620987030444, -6.063714307254753], time: 24.347
Running avgs for agent 0: q_loss: 0.5738523006439209, p_loss: 1.402711272239685, mean_rew: -0.2068974113458486, var_rew: 0.17137392773019416
Running avgs for agent 1: q_loss: 0.004122759681195021, p_loss: 1.551949381828308, mean_rew: -0.3049463854806603, var_rew: 0.2058730562540155
steps: 699975, episodes: 28000, mean episode reward: -11.627103562731008, agent episode reward: [-5.080409422483688, -6.54669414024732], time: 24.215
Running avgs for agent 0: q_loss: 0.5878597497940063, p_loss: 1.4220308065414429, mean_rew: -0.20955283664480773, var_rew: 0.1686234080280338
Running avgs for agent 1: q_loss: 0.0037454552948474884, p_loss: 1.539218544960022, mean_rew: -0.30406943147774074, var_rew: 0.20438115927308723
steps: 724975, episodes: 29000, mean episode reward: -11.885546554982149, agent episode reward: [-5.417867777629357, -6.467678777352793], time: 24.63
Running avgs for agent 0: q_loss: 0.6715059876441956, p_loss: 1.3920857906341553, mean_rew: -0.2090366365709858, var_rew: 0.1695181344827285
Running avgs for agent 1: q_loss: 0.0050251055508852005, p_loss: 1.532020092010498, mean_rew: -0.3032785228822051, var_rew: 0.2048045002482307
steps: 749975, episodes: 30000, mean episode reward: -11.471016633075829, agent episode reward: [-5.147533304505185, -6.323483328570646], time: 24.402
Running avgs for agent 0: q_loss: 0.6526769399642944, p_loss: 1.4135409593582153, mean_rew: -0.20817710774553266, var_rew: 0.16630601363414102
Running avgs for agent 1: q_loss: 0.0036424093414098024, p_loss: 1.5195308923721313, mean_rew: -0.30165461107941066, var_rew: 0.20410151850442135
steps: 774975, episodes: 31000, mean episode reward: -12.324204622972331, agent episode reward: [-5.709391162784717, -6.614813460187616], time: 24.478
Running avgs for agent 0: q_loss: 0.7929226160049438, p_loss: 1.4186420440673828, mean_rew: -0.20818401157390576, var_rew: 0.1683685858390342
Running avgs for agent 1: q_loss: 0.0035204451996833086, p_loss: 1.5004570484161377, mean_rew: -0.29876348281934123, var_rew: 0.20193182383966046
steps: 799975, episodes: 32000, mean episode reward: -11.337640060118654, agent episode reward: [-4.958000737850712, -6.3796393222679395], time: 24.5
Running avgs for agent 0: q_loss: 0.7654011249542236, p_loss: 1.3948379755020142, mean_rew: -0.20785346914484595, var_rew: 0.16698266503803016
Running avgs for agent 1: q_loss: 0.005266760941594839, p_loss: 1.4931530952453613, mean_rew: -0.29812799209472507, var_rew: 0.20153249425743858
steps: 824975, episodes: 33000, mean episode reward: -12.311524849805759, agent episode reward: [-5.764616551643386, -6.546908298162374], time: 24.343
Running avgs for agent 0: q_loss: 0.8715632557868958, p_loss: 1.3664352893829346, mean_rew: -0.20730538792073122, var_rew: 0.16911513968283642
Running avgs for agent 1: q_loss: 0.004614957608282566, p_loss: 1.4857394695281982, mean_rew: -0.2978598532119064, var_rew: 0.2014879776246526
steps: 849975, episodes: 34000, mean episode reward: -11.848282936069237, agent episode reward: [-5.061765679318831, -6.786517256750407], time: 24.473
Running avgs for agent 0: q_loss: 0.8764263987541199, p_loss: 1.402154803276062, mean_rew: -0.21068008322704837, var_rew: 0.1690500814097495
Running avgs for agent 1: q_loss: 0.003881964599713683, p_loss: 1.4720221757888794, mean_rew: -0.2960213506433608, var_rew: 0.20031510233061947
steps: 874975, episodes: 35000, mean episode reward: -12.124928821881202, agent episode reward: [-5.676294206805289, -6.448634615075913], time: 24.331
Running avgs for agent 0: q_loss: 0.9241716265678406, p_loss: 1.3916596174240112, mean_rew: -0.20831533081890044, var_rew: 0.1705449932286681
Running avgs for agent 1: q_loss: 0.004236853681504726, p_loss: 1.4717234373092651, mean_rew: -0.2968874127449177, var_rew: 0.20182712989197865
steps: 899975, episodes: 36000, mean episode reward: -12.023411169504683, agent episode reward: [-5.717915239810579, -6.305495929694105], time: 24.514
Running avgs for agent 0: q_loss: 1.030943512916565, p_loss: 1.3461538553237915, mean_rew: -0.21070728139383307, var_rew: 0.17310058564225617
Running avgs for agent 1: q_loss: 0.004775766283273697, p_loss: 1.4519957304000854, mean_rew: -0.2941239073615823, var_rew: 0.20066286728578167
steps: 924975, episodes: 37000, mean episode reward: -11.889171749384836, agent episode reward: [-5.409300260691052, -6.4798714886937825], time: 24.584
Running avgs for agent 0: q_loss: 1.0534217357635498, p_loss: 1.3294707536697388, mean_rew: -0.21160094213448857, var_rew: 0.17485992727477065
Running avgs for agent 1: q_loss: 0.007300224155187607, p_loss: 1.4369845390319824, mean_rew: -0.29437786162555624, var_rew: 0.19958449130158612
steps: 949975, episodes: 38000, mean episode reward: -12.403692286528885, agent episode reward: [-5.8866783908080675, -6.517013895720818], time: 24.927
Running avgs for agent 0: q_loss: 1.2526293992996216, p_loss: 1.290302038192749, mean_rew: -0.21016464764054593, var_rew: 0.17644109851212939
Running avgs for agent 1: q_loss: 0.004417266696691513, p_loss: 1.4145721197128296, mean_rew: -0.29229973531169423, var_rew: 0.19834549994742864
steps: 974975, episodes: 39000, mean episode reward: -12.075996082139852, agent episode reward: [-5.529279060435315, -6.546717021704537], time: 24.564
Running avgs for agent 0: q_loss: 1.0083417892456055, p_loss: 1.2765259742736816, mean_rew: -0.20991953325684126, var_rew: 0.17719986871729504
Running avgs for agent 1: q_loss: 0.0047368695959448814, p_loss: 1.405555248260498, mean_rew: -0.29228452794955173, var_rew: 0.19918076627329634
steps: 999975, episodes: 40000, mean episode reward: -12.650254655450624, agent episode reward: [-6.127476486234655, -6.52277816921597], time: 24.439
Running avgs for agent 0: q_loss: 1.1791433095932007, p_loss: 1.2711660861968994, mean_rew: -0.21131770488838378, var_rew: 0.17848413761611806
Running avgs for agent 1: q_loss: 0.004916979931294918, p_loss: 1.3856223821640015, mean_rew: -0.29068443523525406, var_rew: 0.19741847674806587
steps: 1024975, episodes: 41000, mean episode reward: -13.250393357273023, agent episode reward: [-6.783243985971276, -6.467149371301747], time: 24.797
Running avgs for agent 0: q_loss: 1.2165378332138062, p_loss: 1.2764195203781128, mean_rew: -0.21410640614886225, var_rew: 0.1801651671440728
Running avgs for agent 1: q_loss: 0.004431398585438728, p_loss: 1.3257856369018555, mean_rew: -0.28088206995862686, var_rew: 0.18962091053939273
steps: 1049975, episodes: 42000, mean episode reward: -12.371984574865234, agent episode reward: [-5.953318456186279, -6.4186661186789555], time: 24.714
Running avgs for agent 0: q_loss: 0.5777404308319092, p_loss: 1.4358346462249756, mean_rew: -0.21668619960347288, var_rew: 0.16728299178505857
Running avgs for agent 1: q_loss: 0.004209380131214857, p_loss: 1.2289968729019165, mean_rew: -0.26368117967842114, var_rew: 0.1748555726825585
^[[Bsteps: 1074975, episodes: 43000, mean episode reward: -11.946436176494437, agent episode reward: [-5.454652131942124, -6.491784044552315], time: 25.007
Running avgs for agent 0: q_loss: 0.3063473403453827, p_loss: 1.5115066766738892, mean_rew: -0.21467811315698168, var_rew: 0.1672095045974229
Running avgs for agent 1: q_loss: 0.003600929630920291, p_loss: 1.2084267139434814, mean_rew: -0.2600037854075967, var_rew: 0.17190679049085852
steps: 1099975, episodes: 44000, mean episode reward: -12.160893072137569, agent episode reward: [-5.521581974670964, -6.639311097466605], time: 24.574
Running avgs for agent 0: q_loss: 0.24430082738399506, p_loss: 1.4773186445236206, mean_rew: -0.20777610946936068, var_rew: 0.1644708472308472
Running avgs for agent 1: q_loss: 0.003707808442413807, p_loss: 1.22394859790802, mean_rew: -0.26274313015587075, var_rew: 0.17455133807343742
steps: 1124975, episodes: 45000, mean episode reward: -11.748211970661666, agent episode reward: [-5.2230218743437025, -6.525190096317965], time: 25.244
Running avgs for agent 0: q_loss: 0.21031615138053894, p_loss: 1.4464861154556274, mean_rew: -0.2051561176150716, var_rew: 0.16761126131360576
Running avgs for agent 1: q_loss: 0.003911847248673439, p_loss: 1.2216308116912842, mean_rew: -0.26160851132550533, var_rew: 0.17414349750782668
steps: 1149975, episodes: 46000, mean episode reward: -11.958009609329023, agent episode reward: [-5.24696325433796, -6.711046354991064], time: 25.325
Running avgs for agent 0: q_loss: 0.18096312880516052, p_loss: 1.4336308240890503, mean_rew: -0.20365427729631497, var_rew: 0.1655262229159452
Running avgs for agent 1: q_loss: 0.0037055397406220436, p_loss: 1.2197446823120117, mean_rew: -0.26191108250435924, var_rew: 0.17340639772086375
steps: 1174975, episodes: 47000, mean episode reward: -11.425710803284305, agent episode reward: [-4.987023514472346, -6.43868728881196], time: 25.494
Running avgs for agent 0: q_loss: 0.1569090336561203, p_loss: 1.4579250812530518, mean_rew: -0.2016266567555806, var_rew: 0.16744836013460462
Running avgs for agent 1: q_loss: 0.003943897783756256, p_loss: 1.221380591392517, mean_rew: -0.26188853759617364, var_rew: 0.1739153790781374
steps: 1199975, episodes: 48000, mean episode reward: -10.885666330779582, agent episode reward: [-4.630460065867808, -6.255206264911775], time: 25.879
Running avgs for agent 0: q_loss: 0.15893898904323578, p_loss: 1.4908634424209595, mean_rew: -0.20131405442952707, var_rew: 0.16704556349852107
Running avgs for agent 1: q_loss: 0.004036364611238241, p_loss: 1.232386589050293, mean_rew: -0.26394140323696136, var_rew: 0.1755127226289735
^R  
steps: 1224975, episodes: 49000, mean episode reward: -11.046613159373416, agent episode reward: [-4.395104416666609, -6.6515087427068105], time: 25.526
Running avgs for agent 0: q_loss: 0.15139520168304443, p_loss: 1.523099660873413, mean_rew: -0.20060225394762876, var_rew: 0.17063784414050276
Running avgs for agent 1: q_loss: 0.0044905999675393105, p_loss: 1.219659686088562, mean_rew: -0.26094773017322825, var_rew: 0.17339998987404862
steps: 1249975, episodes: 50000, mean episode reward: -10.622417205214079, agent episode reward: [-4.252572008377912, -6.369845196836168], time: 25.545
Running avgs for agent 0: q_loss: 0.14199508726596832, p_loss: 1.557971715927124, mean_rew: -0.20199534234546354, var_rew: 0.16935262412205093
Running avgs for agent 1: q_loss: 0.00444996589794755, p_loss: 1.220914602279663, mean_rew: -0.2620249587697427, var_rew: 0.1733647719425736
steps: 1274975, episodes: 51000, mean episode reward: -11.134718479069946, agent episode reward: [-4.480279173561809, -6.654439305508138], time: 25.271
Running avgs for agent 0: q_loss: 0.13593457639217377, p_loss: 1.597989559173584, mean_rew: -0.19996867595804535, var_rew: 0.16864008089059307
Running avgs for agent 1: q_loss: 0.004324898589402437, p_loss: 1.216089129447937, mean_rew: -0.2607710657639547, var_rew: 0.1731141629732971
steps: 1299975, episodes: 52000, mean episode reward: -10.878781262799494, agent episode reward: [-4.317807991130059, -6.5609732716694324], time: 25.231
Running avgs for agent 0: q_loss: 0.1267876923084259, p_loss: 1.6216458082199097, mean_rew: -0.20041193339399105, var_rew: 0.1663929762670764
Running avgs for agent 1: q_loss: 0.0042032948695123196, p_loss: 1.2203655242919922, mean_rew: -0.2611850164721861, var_rew: 0.174360309365811
steps: 1324975, episodes: 53000, mean episode reward: -10.512875332564613, agent episode reward: [-3.7278497958985053, -6.785025536666111], time: 25.122
Running avgs for agent 0: q_loss: 0.12199867516756058, p_loss: 1.647855520248413, mean_rew: -0.2003527013629095, var_rew: 0.16768430901172615
Running avgs for agent 1: q_loss: 0.00409635528922081, p_loss: 1.2320070266723633, mean_rew: -0.26305707453924926, var_rew: 0.17614898244511168
steps: 1349975, episodes: 54000, mean episode reward: -10.441454912582348, agent episode reward: [-3.8791117671672692, -6.562343145415077], time: 25.078
Running avgs for agent 0: q_loss: 0.1171608716249466, p_loss: 1.6718958616256714, mean_rew: -0.20194216696735787, var_rew: 0.17190181453082362
Running avgs for agent 1: q_loss: 0.0038504409603774548, p_loss: 1.2274638414382935, mean_rew: -0.26155788875698266, var_rew: 0.17568806275653975
steps: 1374975, episodes: 55000, mean episode reward: -10.367587512977705, agent episode reward: [-4.016985202119326, -6.350602310858377], time: 24.972
Running avgs for agent 0: q_loss: 0.11426631361246109, p_loss: 1.6777671575546265, mean_rew: -0.19978399484544201, var_rew: 0.16712915304625567
Running avgs for agent 1: q_loss: 0.004064168781042099, p_loss: 1.2169216871261597, mean_rew: -0.25966678740465987, var_rew: 0.17359869167576863
steps: 1399975, episodes: 56000, mean episode reward: -10.202509967346355, agent episode reward: [-3.515510475341041, -6.686999492005314], time: 25.179
Running avgs for agent 0: q_loss: 0.11510631442070007, p_loss: 1.6743924617767334, mean_rew: -0.19707826321474214, var_rew: 0.1668100802850861
Running avgs for agent 1: q_loss: 0.0040726070292294025, p_loss: 1.23013174533844, mean_rew: -0.2615543658936963, var_rew: 0.17637673856560543
steps: 1424975, episodes: 57000, mean episode reward: -9.79022506477174, agent episode reward: [-3.2242416574773807, -6.565983407294358], time: 25.101
Running avgs for agent 0: q_loss: 0.11386663466691971, p_loss: 1.6733540296554565, mean_rew: -0.1977337654220708, var_rew: 0.16915163970519653
Running avgs for agent 1: q_loss: 0.004087330773472786, p_loss: 1.2309365272521973, mean_rew: -0.26100779356785553, var_rew: 0.17617734398409976
steps: 1449975, episodes: 58000, mean episode reward: -9.82317588251183, agent episode reward: [-3.2758873756450133, -6.547288506866817], time: 25.194
Running avgs for agent 0: q_loss: 0.10983076691627502, p_loss: 1.658162236213684, mean_rew: -0.19624354908595718, var_rew: 0.16782425629763514
Running avgs for agent 1: q_loss: 0.004099603742361069, p_loss: 1.2259317636489868, mean_rew: -0.2599860617711802, var_rew: 0.17523113398876178
steps: 1474975, episodes: 59000, mean episode reward: -9.866679348286034, agent episode reward: [-3.376944640422492, -6.489734707863543], time: 24.993
Running avgs for agent 0: q_loss: 0.11398095637559891, p_loss: 1.6459153890609741, mean_rew: -0.1947695835841869, var_rew: 0.16883921229751064
Running avgs for agent 1: q_loss: 0.004237613640725613, p_loss: 1.2343829870224, mean_rew: -0.2621293669755476, var_rew: 0.1766688877965048
steps: 1499975, episodes: 60000, mean episode reward: -9.695062554370677, agent episode reward: [-3.218081633108902, -6.476980921261776], time: 25.043
Running avgs for agent 0: q_loss: 0.11350774019956589, p_loss: 1.63960862159729, mean_rew: -0.19482153533070984, var_rew: 0.16868917428579372
Running avgs for agent 1: q_loss: 0.004178173840045929, p_loss: 1.2213826179504395, mean_rew: -0.2603820193165768, var_rew: 0.17536836617542984
Traceback (most recent call last):
  File "train_vanila.py", line 234, in <module>

