python train.py --scenario simple_push --num-adversaries 1
2018-10-11 16:33:15.600489: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -27.284266003290345, agent episode reward: [-2.2892982879190527, -24.994967715371292], time: 20.082
steps: 49975, episodes: 2000, mean episode reward: -20.760770139701897, agent episode reward: [-2.611049002207065, -18.149721137494836], time: 31.764
steps: 74975, episodes: 3000, mean episode reward: -10.754501499274937, agent episode reward: [-3.9112493785877933, -6.843252120687143], time: 30.346
steps: 99975, episodes: 4000, mean episode reward: -9.369091059895883, agent episode reward: [-2.8830664337287852, -6.486024626167097], time: 31.398
steps: 124975, episodes: 5000, mean episode reward: -9.275644289443479, agent episode reward: [-2.38232896450792, -6.893315324935558], time: 31.466
steps: 149975, episodes: 6000, mean episode reward: -9.011419663368327, agent episode reward: [-2.237225044494521, -6.774194618873808], time: 31.223
steps: 174975, episodes: 7000, mean episode reward: -8.981682397725875, agent episode reward: [-2.2267373582694825, -6.754945039456391], time: 31.669
steps: 199975, episodes: 8000, mean episode reward: -9.020193744357355, agent episode reward: [-2.2480400509530973, -6.77215369340426], time: 33.239
steps: 224975, episodes: 9000, mean episode reward: -8.830704116151544, agent episode reward: [-1.8396390674961083, -6.991065048655437], time: 34.087
steps: 249975, episodes: 10000, mean episode reward: -8.829267166491533, agent episode reward: [-2.036472165061287, -6.792795001430244], time: 35.32
steps: 274975, episodes: 11000, mean episode reward: -8.813113407029803, agent episode reward: [-2.0155511352213935, -6.7975622718084106], time: 37.364
steps: 299975, episodes: 12000, mean episode reward: -8.468284405454899, agent episode reward: [-1.7783654625951961, -6.689918942859702], time: 37.335
steps: 324975, episodes: 13000, mean episode reward: -9.11423738749393, agent episode reward: [-2.1837032864753687, -6.93053410101856], time: 37.405
steps: 349975, episodes: 14000, mean episode reward: -8.771386693191802, agent episode reward: [-1.7196134717692948, -7.051773221422509], time: 35.828
steps: 374975, episodes: 15000, mean episode reward: -8.789132366987328, agent episode reward: [-1.919167196154926, -6.869965170832402], time: 35.025
steps: 399975, episodes: 16000, mean episode reward: -8.707835404517045, agent episode reward: [-1.809569800351679, -6.898265604165365], time: 35.427
steps: 424975, episodes: 17000, mean episode reward: -8.882335915930186, agent episode reward: [-2.0709355881704283, -6.811400327759757], time: 34.247
steps: 449975, episodes: 18000, mean episode reward: -8.728580447144681, agent episode reward: [-1.8973412451937455, -6.831239201950935], time: 34.92
steps: 474975, episodes: 19000, mean episode reward: -8.44379161935399, agent episode reward: [-1.6691222896744289, -6.774669329679559], time: 36.56
steps: 499975, episodes: 20000, mean episode reward: -8.719003490949774, agent episode reward: [-1.7855360865568108, -6.933467404392963], time: 39.14
steps: 524975, episodes: 21000, mean episode reward: -8.825332009607203, agent episode reward: [-2.069903873249167, -6.755428136358038], time: 40.318
steps: 549975, episodes: 22000, mean episode reward: -8.517155383548003, agent episode reward: [-1.6967005854902584, -6.820454798057744], time: 40.013
steps: 574975, episodes: 23000, mean episode reward: -8.732112316152747, agent episode reward: [-1.6512052901038858, -7.080907026048861], time: 37.636
steps: 599975, episodes: 24000, mean episode reward: -8.793141683762682, agent episode reward: [-1.9444543796759195, -6.848687304086763], time: 37.95
steps: 624975, episodes: 25000, mean episode reward: -8.593478699831925, agent episode reward: [-1.9149586239794438, -6.67852007585248], time: 36.302
steps: 649975, episodes: 26000, mean episode reward: -8.466542130233302, agent episode reward: [-1.682556521915833, -6.78398560831747], time: 36.031
steps: 674975, episodes: 27000, mean episode reward: -8.656230662874247, agent episode reward: [-1.7790928004389133, -6.877137862435333], time: 36.448
steps: 699975, episodes: 28000, mean episode reward: -8.45000626076446, agent episode reward: [-2.009310391269909, -6.440695869494552], time: 37.148
steps: 724975, episodes: 29000, mean episode reward: -8.602953290227449, agent episode reward: [-1.8280605131692673, -6.774892777058181], time: 37.89
steps: 749975, episodes: 30000, mean episode reward: -8.99470401409182, agent episode reward: [-2.12154762759142, -6.8731563865004], time: 42.84
steps: 774975, episodes: 31000, mean episode reward: -8.83038561730102, agent episode reward: [-2.082811058003286, -6.747574559297736], time: 37.331
steps: 799975, episodes: 32000, mean episode reward: -8.613394599464861, agent episode reward: [-1.739506957121071, -6.873887642343792], time: 36.637
steps: 824975, episodes: 33000, mean episode reward: -8.756721413257164, agent episode reward: [-2.254138413607032, -6.502582999650133], time: 36.331
steps: 849975, episodes: 34000, mean episode reward: -8.454905121760444, agent episode reward: [-1.7330228861532333, -6.72188223560721], time: 37.806
steps: 874975, episodes: 35000, mean episode reward: -8.728626574199449, agent episode reward: [-1.7412849084783082, -6.987341665721141], time: 36.894
steps: 899975, episodes: 36000, mean episode reward: -8.673163719018069, agent episode reward: [-2.09602307047889, -6.577140648539177], time: 38.392
steps: 924975, episodes: 37000, mean episode reward: -8.391008371622355, agent episode reward: [-1.9855282905717768, -6.40548008105058], time: 39.572
steps: 949975, episodes: 38000, mean episode reward: -8.88465941995231, agent episode reward: [-2.119048174399542, -6.765611245552767], time: 38.177
steps: 974975, episodes: 39000, mean episode reward: -9.045039565660744, agent episode reward: [-2.291861775734466, -6.753177789926278], time: 45.702
steps: 999975, episodes: 40000, mean episode reward: -9.089061683098802, agent episode reward: [-1.949488690209747, -7.139572992889054], time: 43.025
steps: 1024975, episodes: 41000, mean episode reward: -9.050717034343437, agent episode reward: [-1.9115455371742542, -7.1391714971691815], time: 50.481
steps: 1049975, episodes: 42000, mean episode reward: -9.093161006720461, agent episode reward: [-2.0486887789938053, -7.044472227726655], time: 47.635
steps: 1074975, episodes: 43000, mean episode reward: -9.11990847681061, agent episode reward: [-1.5330925798595367, -7.586815896951073], time: 49.056


