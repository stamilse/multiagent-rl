python train.py --scenario simple_push --num-adversaries 1
2018-10-23 12:37:12.884437: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -27.3912907734959, agent episode reward: [-0.6556832257860935, -26.735607547709805], time: 23.936
/anaconda2/envs/py36/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/anaconda2/envs/py36/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
Running avgs for agent 0: q_loss: nan, q_loss2: nan, p_loss: nan, mean_rew: nan, var_rew: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, q_loss2: nan, p_loss: nan, mean_rew: nan, var_rew: nan, lamda: nan
steps: 49975, episodes: 2000, mean episode reward: -21.681081355240476, agent episode reward: [-5.526957576740272, -16.1541237785002], time: 36.122
Running avgs for agent 0: q_loss: 0.5552406907081604, q_loss2: -0.5516651272773743, p_loss: 0.056947216391563416, mean_rew: -0.07482831967257692, var_rew: 0.5677993238127712, lamda: 1.0123381614685059
Running avgs for agent 1: q_loss: 0.4179880619049072, q_loss2: -0.4157100319862366, p_loss: 1.4050079584121704, mean_rew: -1.0182686832452514, var_rew: 0.4449919734935913, lamda: 1.01338529586792
steps: 74975, episodes: 3000, mean episode reward: -11.357257265800568, agent episode reward: [-4.625586229445954, -6.731671036354613], time: 36.699
Running avgs for agent 0: q_loss: 0.3909054398536682, q_loss2: -0.391085684299469, p_loss: 0.30341556668281555, mean_rew: -0.1401998879582212, var_rew: 0.4169967228585289, lamda: 1.0348182916641235
Running avgs for agent 1: q_loss: 0.4408537745475769, q_loss2: -0.44103506207466125, p_loss: 2.066375970840454, mean_rew: -0.7481685255167961, var_rew: 0.47037149322004135, lamda: 1.039458990097046
steps: 99975, episodes: 4000, mean episode reward: -9.517137690944262, agent episode reward: [-2.854740122861647, -6.662397568082613], time: 35.792
Running avgs for agent 0: q_loss: 0.3107529878616333, q_loss2: -0.31044483184814453, p_loss: 0.49646812677383423, mean_rew: -0.1415210766856428, var_rew: 0.32734984387634564, lamda: 1.0537734031677246
Running avgs for agent 1: q_loss: 0.41116419434547424, q_loss2: -0.4114016592502594, p_loss: 2.3372035026550293, mean_rew: -0.6069797702620355, var_rew: 0.4315390912041423, lamda: 1.0639538764953613
steps: 124975, episodes: 5000, mean episode reward: -8.876199246452485, agent episode reward: [-2.338771538710448, -6.537427707742036], time: 36.355
Running avgs for agent 0: q_loss: 0.274122029542923, q_loss2: -0.27306121587753296, p_loss: 0.6318305730819702, mean_rew: -0.13319534175212802, var_rew: 0.2805302617157284, lamda: 1.0709490776062012
Running avgs for agent 1: q_loss: 0.3792388141155243, q_loss2: -0.3793695569038391, p_loss: 2.3767459392547607, mean_rew: -0.5293363581379512, var_rew: 0.39439519637360815, lamda: 1.086677074432373
steps: 149975, episodes: 6000, mean episode reward: -8.903817542077464, agent episode reward: [-2.4543451186778054, -6.4494724233996585], time: 36.605
Running avgs for agent 0: q_loss: 0.2584971487522125, q_loss2: -0.25750139355659485, p_loss: 0.7415671944618225, mean_rew: -0.12750563165142165, var_rew: 0.2516778063652138, lamda: 1.0874265432357788
Running avgs for agent 1: q_loss: 0.35457879304885864, q_loss2: -0.3544628918170929, p_loss: 2.3299524784088135, mean_rew: -0.4821266955742531, var_rew: 0.3632426375043367, lamda: 1.1082719564437866
steps: 174975, episodes: 7000, mean episode reward: -9.052394039494818, agent episode reward: [-2.3201222665231533, -6.732271772971664], time: 40.834
Running avgs for agent 0: q_loss: 0.23824742436408997, q_loss2: -0.23557813465595245, p_loss: 0.7886574268341064, mean_rew: -0.12016317715904329, var_rew: 0.23201552149690954, lamda: 1.103729486465454
Running avgs for agent 1: q_loss: 0.33417290449142456, q_loss2: -0.3342761695384979, p_loss: 2.254417896270752, mean_rew: -0.44853296856387004, var_rew: 0.3401270727930337, lamda: 1.129090428352356
steps: 199975, episodes: 8000, mean episode reward: -9.136541908766437, agent episode reward: [-2.292719959168645, -6.843821949597793], time: 43.422
Running avgs for agent 0: q_loss: 0.22404608130455017, q_loss2: -0.22132855653762817, p_loss: 0.8167402148246765, mean_rew: -0.11676991614196909, var_rew: 0.21540676417257437, lamda: 1.120032787322998
Running avgs for agent 1: q_loss: 0.31905028223991394, q_loss2: -0.31871387362480164, p_loss: 2.1782891750335693, mean_rew: -0.42523343155342136, var_rew: 0.32216126319539257, lamda: 1.1496633291244507
steps: 224975, episodes: 9000, mean episode reward: -8.683376893828274, agent episode reward: [-2.134221947603355, -6.5491549462249194], time: 49.015
Running avgs for agent 0: q_loss: 0.21079522371292114, q_loss2: -0.20767804980278015, p_loss: 0.8390392065048218, mean_rew: -0.11375107785469993, var_rew: 0.20486971690967756, lamda: 1.1364585161209106
Running avgs for agent 1: q_loss: 0.3107328712940216, q_loss2: -0.31039246916770935, p_loss: 2.1077499389648438, mean_rew: -0.4061249354665317, var_rew: 0.30820968316226743, lamda: 1.1699001789093018
steps: 249975, episodes: 10000, mean episode reward: -8.86822647869853, agent episode reward: [-2.305041455200819, -6.563185023497711], time: 42.739
Running avgs for agent 0: q_loss: 0.20053640007972717, q_loss2: -0.19748817384243011, p_loss: 0.8491327166557312, mean_rew: -0.11035687951713351, var_rew: 0.19517469580671362, lamda: 1.1531789302825928
Running avgs for agent 1: q_loss: 0.2962660491466522, q_loss2: -0.29547613859176636, p_loss: 2.037228584289551, mean_rew: -0.3902052410247111, var_rew: 0.29185261998895906, lamda: 1.1901308298110962
steps: 274975, episodes: 11000, mean episode reward: -9.047321739480285, agent episode reward: [-2.2974627508653747, -6.749858988614911], time: 51.262
Running avgs for agent 0: q_loss: 0.19782091677188873, q_loss2: -0.19434675574302673, p_loss: 0.8566717505455017, mean_rew: -0.11012721957982313, var_rew: 0.1884239083372265, lamda: 1.1702215671539307
Running avgs for agent 1: q_loss: 0.2938665747642517, q_loss2: -0.2933390438556671, p_loss: 2.0076804161071777, mean_rew: -0.37997998156959023, var_rew: 0.28540120976882416, lamda: 1.2102497816085815
steps: 299975, episodes: 12000, mean episode reward: -8.856758954167073, agent episode reward: [-2.0452224530994667, -6.811536501067607], time: 44.143
Running avgs for agent 0: q_loss: 0.1925915777683258, q_loss2: -0.18850579857826233, p_loss: 0.8672847747802734, mean_rew: -0.10763357938835838, var_rew: 0.18116969692388124, lamda: 1.1877228021621704
Running avgs for agent 1: q_loss: 0.2880314290523529, q_loss2: -0.2866707742214203, p_loss: 1.9659675359725952, mean_rew: -0.37049061013654727, var_rew: 0.2752375104972514, lamda: 1.2306445837020874
steps: 324975, episodes: 13000, mean episode reward: -8.630783067300554, agent episode reward: [-1.6635248357189498, -6.967258231581606], time: 47.025
Running avgs for agent 0: q_loss: 0.18725314736366272, q_loss2: -0.18377234041690826, p_loss: 0.868281364440918, mean_rew: -0.10600080553222661, var_rew: 0.17765179091326275, lamda: 1.2055617570877075
Running avgs for agent 1: q_loss: 0.2797817587852478, q_loss2: -0.2788064479827881, p_loss: 1.9338191747665405, mean_rew: -0.36255866603880477, var_rew: 0.2671015648490065, lamda: 1.2512582540512085
steps: 349975, episodes: 14000, mean episode reward: -8.822625464457575, agent episode reward: [-1.7994283424897017, -7.023197121967875], time: 43.139
Running avgs for agent 0: q_loss: 0.18128548562526703, q_loss2: -0.17802710831165314, p_loss: 0.8528095483779907, mean_rew: -0.10181512299535651, var_rew: 0.17181954228588622, lamda: 1.2238925695419312
Running avgs for agent 1: q_loss: 0.2758142948150635, q_loss2: -0.27529487013816833, p_loss: 1.9189352989196777, mean_rew: -0.35649734372083325, var_rew: 0.2619439970780914, lamda: 1.2719992399215698
^[[Bsteps: 374975, episodes: 15000, mean episode reward: -8.786597469858199, agent episode reward: [-1.6387382096536047, -7.147859260204596], time: 40.121
Running avgs for agent 0: q_loss: 0.1847686469554901, q_loss2: -0.1801653802394867, p_loss: 0.8425026535987854, mean_rew: -0.10071452400415723, var_rew: 0.16972501958472705, lamda: 1.2427140474319458
Running avgs for agent 1: q_loss: 0.2738460302352905, q_loss2: -0.27299487590789795, p_loss: 1.9012634754180908, mean_rew: -0.3509424964555707, var_rew: 0.25554794081687005, lamda: 1.2929975986480713
steps: 399975, episodes: 16000, mean episode reward: -8.857834834765086, agent episode reward: [-1.6131259904500583, -7.244708844315028], time: 41.238
Running avgs for agent 0: q_loss: 0.18374700844287872, q_loss2: -0.17875531315803528, p_loss: 0.8171808123588562, mean_rew: -0.09689087342679707, var_rew: 0.16584019846700135, lamda: 1.2621245384216309
Running avgs for agent 1: q_loss: 0.27533888816833496, q_loss2: -0.2740229368209839, p_loss: 1.8991731405258179, mean_rew: -0.34862990125020665, var_rew: 0.2516595043778425, lamda: 1.3140815496444702
steps: 424975, episodes: 17000, mean episode reward: -8.6769290095016, agent episode reward: [-1.8617208389533144, -6.815208170548287], time: 42.228
Running avgs for agent 0: q_loss: 0.17417362332344055, q_loss2: -0.17092131078243256, p_loss: 0.8251875042915344, mean_rew: -0.0988528826757609, var_rew: 0.1626495578457097, lamda: 1.2817606925964355
Running avgs for agent 1: q_loss: 0.2744315564632416, q_loss2: -0.2727162837982178, p_loss: 1.8745014667510986, mean_rew: -0.3440900353230064, var_rew: 0.246636415909554, lamda: 1.3355796337127686
steps: 449975, episodes: 18000, mean episode reward: -8.499594682451486, agent episode reward: [-1.6635121266123156, -6.836082555839169], time: 50.685
Running avgs for agent 0: q_loss: 0.17852145433425903, q_loss2: -0.1741919070482254, p_loss: 0.7872245907783508, mean_rew: -0.09381899951794818, var_rew: 0.1607355956561935, lamda: 1.301803469657898
Running avgs for agent 1: q_loss: 0.26293236017227173, q_loss2: -0.2623695731163025, p_loss: 1.8456203937530518, mean_rew: -0.33720694486320396, var_rew: 0.23976220684940938, lamda: 1.3569735288619995
steps: 474975, episodes: 19000, mean episode reward: -8.71780870137061, agent episode reward: [-1.6074812686354312, -7.110327432735179], time: 42.923
Running avgs for agent 0: q_loss: 0.17446230351924896, q_loss2: -0.17096848785877228, p_loss: 0.7711201906204224, mean_rew: -0.09318439055889044, var_rew: 0.15814219896747422, lamda: 1.3224421739578247
Running avgs for agent 1: q_loss: 0.27195361256599426, q_loss2: -0.26932811737060547, p_loss: 1.841293215751648, mean_rew: -0.33530684183774095, var_rew: 0.2364301018262609, lamda: 1.3787404298782349
steps: 499975, episodes: 20000, mean episode reward: -8.84653576512427, agent episode reward: [-1.3680825304306794, -7.478453234693589], time: 46.016
Running avgs for agent 0: q_loss: 0.17735686898231506, q_loss2: -0.1730699986219406, p_loss: 0.7447934746742249, mean_rew: -0.08985054375763468, var_rew: 0.15686945908739136, lamda: 1.343496322631836
Running avgs for agent 1: q_loss: 0.2651928663253784, q_loss2: -0.2630388140678406, p_loss: 1.8305531740188599, mean_rew: -0.3333387938181063, var_rew: 0.23207621959759453, lamda: 1.4004870653152466
steps: 524975, episodes: 21000, mean episode reward: -8.398119436197106, agent episode reward: [-1.1027715548494739, -7.295347881347632], time: 54.222
Running avgs for agent 0: q_loss: 0.17729488015174866, q_loss2: -0.17301736772060394, p_loss: 0.7060344815254211, mean_rew: -0.08913931802352076, var_rew: 0.15429551881393147, lamda: 1.3647624254226685
Running avgs for agent 1: q_loss: 0.26055091619491577, q_loss2: -0.26000598073005676, p_loss: 1.8373539447784424, mean_rew: -0.33152988572218456, var_rew: 0.22948507013173608, lamda: 1.4224179983139038
steps: 549975, episodes: 22000, mean episode reward: -8.661382424259948, agent episode reward: [-1.5378729167046161, -7.123509507555332], time: 49.26
Running avgs for agent 0: q_loss: 0.16869954764842987, q_loss2: -0.166226327419281, p_loss: 0.6750895977020264, mean_rew: -0.08787998097163509, var_rew: 0.1523198968767163, lamda: 1.3863134384155273
Running avgs for agent 1: q_loss: 0.2604663074016571, q_loss2: -0.25992733240127563, p_loss: 1.8463441133499146, mean_rew: -0.3305104428109351, var_rew: 0.2262627633151326, lamda: 1.444682002067566
steps: 574975, episodes: 23000, mean episode reward: -8.700894016419253, agent episode reward: [-1.7783804560844685, -6.922513560334786], time: 52.759
Running avgs for agent 0: q_loss: 0.1806906759738922, q_loss2: -0.1751127392053604, p_loss: 0.6352174282073975, mean_rew: -0.08664380860124055, var_rew: 0.15108762011064536, lamda: 1.408163070678711
Running avgs for agent 1: q_loss: 0.26359644532203674, q_loss2: -0.2615515887737274, p_loss: 1.8297785520553589, mean_rew: -0.3266769642845815, var_rew: 0.22138030197880323, lamda: 1.4670552015304565
steps: 599975, episodes: 24000, mean episode reward: -8.933480221496017, agent episode reward: [-1.76598426488276, -7.167495956613257], time: 53.151
Running avgs for agent 0: q_loss: 0.177958145737648, q_loss2: -0.17328894138336182, p_loss: 0.6224722862243652, mean_rew: -0.08744602337718743, var_rew: 0.15077494560207697, lamda: 1.4304500818252563
Running avgs for agent 1: q_loss: 0.25997838377952576, q_loss2: -0.259375661611557, p_loss: 1.822130560874939, mean_rew: -0.3255474719357095, var_rew: 0.22054603160685218, lamda: 1.4893654584884644
steps: 624975, episodes: 25000, mean episode reward: -8.650200178937087, agent episode reward: [-1.9071525294386216, -6.7430476494984655], time: 49.243
Running avgs for agent 0: q_loss: 0.1726311594247818, q_loss2: -0.16962930560112, p_loss: 0.5830186009407043, mean_rew: -0.08530712244982212, var_rew: 0.1482537925194672, lamda: 1.4529887437820435
Running avgs for agent 1: q_loss: 0.262979656457901, q_loss2: -0.26171180605888367, p_loss: 1.8028926849365234, mean_rew: -0.32253333345807955, var_rew: 0.217795122378628, lamda: 1.5121643543243408
steps: 649975, episodes: 26000, mean episode reward: -8.477661106501117, agent episode reward: [-1.5315246446955706, -6.946136461805545], time: 46.419
Running avgs for agent 0: q_loss: 0.1706700176000595, q_loss2: -0.16762591898441315, p_loss: 0.5581320524215698, mean_rew: -0.08506535962581016, var_rew: 0.14606675106989878, lamda: 1.4753613471984863
Running avgs for agent 1: q_loss: 0.2641400992870331, q_loss2: -0.2629587650299072, p_loss: 1.785132646560669, mean_rew: -0.3208903823960823, var_rew: 0.21591043501713347, lamda: 1.534860610961914
steps: 674975, episodes: 27000, mean episode reward: -8.569761242907123, agent episode reward: [-1.754243529561524, -6.815517713345598], time: 58.409
Running avgs for agent 0: q_loss: 0.17265008389949799, q_loss2: -0.16972464323043823, p_loss: 0.5266718864440918, mean_rew: -0.08407080723122999, var_rew: 0.14576613439253763, lamda: 1.4978837966918945
Running avgs for agent 1: q_loss: 0.2655949592590332, q_loss2: -0.2650776207447052, p_loss: 1.7738786935806274, mean_rew: -0.320034016665214, var_rew: 0.21621228007315643, lamda: 1.5581578016281128
steps: 699975, episodes: 28000, mean episode reward: -8.824131541063103, agent episode reward: [-1.883154828138825, -6.940976712924277], time: 52.853
Running avgs for agent 0: q_loss: 0.17119158804416656, q_loss2: -0.16883963346481323, p_loss: 0.4932612180709839, mean_rew: -0.08230399098887298, var_rew: 0.14472677725988542, lamda: 1.5208505392074585
Running avgs for agent 1: q_loss: 0.27167898416519165, q_loss2: -0.26963648200035095, p_loss: 1.7570502758026123, mean_rew: -0.31797793610454395, var_rew: 0.21419391705669394, lamda: 1.5815134048461914
steps: 724975, episodes: 29000, mean episode reward: -8.622455853420798, agent episode reward: [-1.3742854186909503, -7.248170434729848], time: 47.092
Running avgs for agent 0: q_loss: 0.1781710833311081, q_loss2: -0.17419403791427612, p_loss: 0.48830166459083557, mean_rew: -0.08281527966136615, var_rew: 0.14366475733839734, lamda: 1.5439010858535767
Running avgs for agent 1: q_loss: 0.2632879316806793, q_loss2: -0.2626830041408539, p_loss: 1.7406514883041382, mean_rew: -0.3167448623511092, var_rew: 0.20981037759498933, lamda: 1.6047253608703613
steps: 749975, episodes: 30000, mean episode reward: -8.579058550015363, agent episode reward: [-0.9673418096837844, -7.6117167403315795], time: 60.936
Running avgs for agent 0: q_loss: 0.1767244189977646, q_loss2: -0.17350894212722778, p_loss: 0.47289565205574036, mean_rew: -0.08195468138994286, var_rew: 0.14320769500707453, lamda: 1.5671323537826538
Running avgs for agent 1: q_loss: 0.27117037773132324, q_loss2: -0.2689633369445801, p_loss: 1.7305032014846802, mean_rew: -0.3151650896316778, var_rew: 0.20842709708096682, lamda: 1.6279951333999634
steps: 774975, episodes: 31000, mean episode reward: -8.804356632905943, agent episode reward: [-0.6516168339404392, -8.152739798965504], time: 64.175
Running avgs for agent 0: q_loss: 0.18134985864162445, q_loss2: -0.17727261781692505, p_loss: 0.4402727782726288, mean_rew: -0.08129126790537902, var_rew: 0.14320197099803078, lamda: 1.5907195806503296
Running avgs for agent 1: q_loss: 0.2710937559604645, q_loss2: -0.26982587575912476, p_loss: 1.7211331129074097, mean_rew: -0.3158659500513744, var_rew: 0.2075891133529738, lamda: 1.6513935327529907
steps: 799975, episodes: 32000, mean episode reward: -9.332200183652969, agent episode reward: [-0.5360776246486182, -8.79612255900435], time: 58.805
Running avgs for agent 0: q_loss: 0.17738217115402222, q_loss2: -0.1752355694770813, p_loss: 0.3614952564239502, mean_rew: -0.0778101129993669, var_rew: 0.1429352017140077, lamda: 1.6144546270370483
Running avgs for agent 1: q_loss: 0.2740662693977356, q_loss2: -0.2733410894870758, p_loss: 1.6892341375350952, mean_rew: -0.3175414328853229, var_rew: 0.20730201864272454, lamda: 1.6749184131622314
steps: 824975, episodes: 33000, mean episode reward: -9.013446409259695, agent episode reward: [-1.1334756173488185, -7.879970791910878], time: 65.97
Running avgs for agent 0: q_loss: 0.18306247889995575, q_loss2: -0.17923212051391602, p_loss: 0.29464101791381836, mean_rew: -0.07688920893627292, var_rew: 0.1403483040315835, lamda: 1.6381313800811768
Running avgs for agent 1: q_loss: 0.27444496750831604, q_loss2: -0.27388525009155273, p_loss: 1.6453176736831665, mean_rew: -0.31829747036349826, var_rew: 0.20514084031248042, lamda: 1.698681116104126
steps: 849975, episodes: 34000, mean episode reward: -8.767351438439134, agent episode reward: [-1.458316300308278, -7.309035138130855], time: 67.914
Running avgs for agent 0: q_loss: 0.18666525185108185, q_loss2: -0.18295571208000183, p_loss: 0.2576671838760376, mean_rew: -0.07622203491846463, var_rew: 0.14136687275480028, lamda: 1.6617954969406128
Running avgs for agent 1: q_loss: 0.2777928113937378, q_loss2: -0.2763047516345978, p_loss: 1.6184649467468262, mean_rew: -0.31684066132955463, var_rew: 0.2027495483183246, lamda: 1.7223261594772339
steps: 874975, episodes: 35000, mean episode reward: -8.749356973773544, agent episode reward: [-1.7159433943361138, -7.033413579437431], time: 72.03
Running avgs for agent 0: q_loss: 0.18237553536891937, q_loss2: -0.17938584089279175, p_loss: 0.2378278523683548, mean_rew: -0.07568496781774411, var_rew: 0.13991404520252956, lamda: 1.6854774951934814
Running avgs for agent 1: q_loss: 0.28099575638771057, q_loss2: -0.279897004365921, p_loss: 1.6057578325271606, mean_rew: -0.31832260245694133, var_rew: 0.2040322667084879, lamda: 1.7461620569229126
steps: 899975, episodes: 36000, mean episode reward: -8.788729774874506, agent episode reward: [-1.2127241527390789, -7.576005622135426], time: 68.563
Running avgs for agent 0: q_loss: 0.1795034259557724, q_loss2: -0.17725104093551636, p_loss: 0.22980082035064697, mean_rew: -0.07654910740846345, var_rew: 0.13882563369434078, lamda: 1.7094519138336182
Running avgs for agent 1: q_loss: 0.2804242670536041, q_loss2: -0.2797512710094452, p_loss: 1.5659539699554443, mean_rew: -0.31577156134839995, var_rew: 0.20206855582971536, lamda: 1.7700616121292114
steps: 924975, episodes: 37000, mean episode reward: -8.668408286549147, agent episode reward: [-1.3812207224345259, -7.287187564114621], time: 71.489
Running avgs for agent 0: q_loss: 0.18531985580921173, q_loss2: -0.18185684084892273, p_loss: 0.221263587474823, mean_rew: -0.07478056270898224, var_rew: 0.13811545889530039, lamda: 1.7331777811050415
Running avgs for agent 1: q_loss: 0.2831041216850281, q_loss2: -0.2819294333457947, p_loss: 1.539597988128662, mean_rew: -0.3147683922727057, var_rew: 0.19966122709139575, lamda: 1.7939857244491577
steps: 949975, episodes: 38000, mean episode reward: -8.907291650040902, agent episode reward: [-1.8554642606373541, -7.051827389403546], time: 78.792
Running avgs for agent 0: q_loss: 0.18474631011486053, q_loss2: -0.18111416697502136, p_loss: 0.23290641605854034, mean_rew: -0.07499456306987648, var_rew: 0.1359922953743206, lamda: 1.7567583322525024
Running avgs for agent 1: q_loss: 0.27832522988319397, q_loss2: -0.27771759033203125, p_loss: 1.4774219989776611, mean_rew: -0.3137755221487224, var_rew: 0.1973949591471098, lamda: 1.8177460432052612
steps: 974975, episodes: 39000, mean episode reward: -8.86564204180937, agent episode reward: [-1.3516688069207454, -7.513973234888622], time: 79.968
Running avgs for agent 0: q_loss: 0.1855054646730423, q_loss2: -0.18290948867797852, p_loss: 0.22968515753746033, mean_rew: -0.07429441503153075, var_rew: 0.13735385830970215, lamda: 1.7805360555648804
Running avgs for agent 1: q_loss: 0.28032422065734863, q_loss2: -0.2795569896697998, p_loss: 1.4429081678390503, mean_rew: -0.3124807059100925, var_rew: 0.19767396679190238, lamda: 1.841543436050415
steps: 999975, episodes: 40000, mean episode reward: -8.729034644553373, agent episode reward: [-1.4237071991297854, -7.305327445423587], time: 69.655
Running avgs for agent 0: q_loss: 0.19581815600395203, q_loss2: -0.19124296307563782, p_loss: 0.23378174006938934, mean_rew: -0.07409683770355555, var_rew: 0.1384467436249696, lamda: 1.8048579692840576
Running avgs for agent 1: q_loss: 0.2841048538684845, q_loss2: -0.2829432487487793, p_loss: 1.4610965251922607, mean_rew: -0.312041565775032, var_rew: 0.1966832979796841, lamda: 1.8655614852905273
steps: 1024975, episodes: 41000, mean episode reward: -8.625617680787247, agent episode reward: [-1.3012773264484105, -7.324340354338836], time: 71.477
Running avgs for agent 0: q_loss: 0.17980991303920746, q_loss2: -0.17627617716789246, p_loss: 0.23502331972122192, mean_rew: -0.0737120699103138, var_rew: 0.13053803671287556, lamda: 1.8285456895828247
Running avgs for agent 1: q_loss: 0.2651229202747345, q_loss2: -0.26489952206611633, p_loss: 1.398332118988037, mean_rew: -0.30123322302363387, var_rew: 0.18693754344307836, lamda: 1.889072299003601
steps: 1049975, episodes: 42000, mean episode reward: -8.548322426596485, agent episode reward: [-0.7232833708967721, -7.825039055699713], time: 77.596
Running avgs for agent 0: q_loss: 0.15635736286640167, q_loss2: -0.15505480766296387, p_loss: 0.22357800602912903, mean_rew: -0.07274413076033727, var_rew: 0.1216110629333147, lamda: 1.8500959873199463
Running avgs for agent 1: q_loss: 0.23154158890247345, q_loss2: -0.23134745657444, p_loss: 1.3081235885620117, mean_rew: -0.28653233655694954, var_rew: 0.16774164692138374, lamda: 1.9104701280593872
steps: 1074975, episodes: 43000, mean episode reward: -8.410431616519196, agent episode reward: [-1.146411705396103, -7.264019911123093], time: 87.092
Running avgs for agent 0: q_loss: 0.14973700046539307, q_loss2: -0.14856407046318054, p_loss: 0.19404111802577972, mean_rew: -0.06730915562726332, var_rew: 0.11757578610804408, lamda: 1.8705508708953857
Running avgs for agent 1: q_loss: 0.23391474783420563, q_loss2: -0.23372699320316315, p_loss: 1.3485463857650757, mean_rew: -0.28658224109813035, var_rew: 0.1674352172160417, lamda: 1.9309406280517578
steps: 1099975, episodes: 44000, mean episode reward: -8.663859799573437, agent episode reward: [-1.5708320189800833, -7.093027780593353], time: 73.829
Running avgs for agent 0: q_loss: 0.1543528288602829, q_loss2: -0.15301048755645752, p_loss: 0.1826888918876648, mean_rew: -0.06544887305732769, var_rew: 0.11898781775068223, lamda: 1.8913578987121582
Running avgs for agent 1: q_loss: 0.23336362838745117, q_loss2: -0.23309263586997986, p_loss: 1.3561294078826904, mean_rew: -0.2853346012779014, var_rew: 0.1655983345625624, lamda: 1.9521020650863647
steps: 1124975, episodes: 45000, mean episode reward: -8.43251025758136, agent episode reward: [-1.761748074657443, -6.670762182923918], time: 67.746
Running avgs for agent 0: q_loss: 0.15438488125801086, q_loss2: -0.15295937657356262, p_loss: 0.16211211681365967, mean_rew: -0.06298580850290764, var_rew: 0.11788913914706968, lamda: 1.9131488800048828
Running avgs for agent 1: q_loss: 0.23654046654701233, q_loss2: -0.2363174706697464, p_loss: 1.362920880317688, mean_rew: -0.2862097065360798, var_rew: 0.1659375320190355, lamda: 1.9738622903823853
steps: 1149975, episodes: 46000, mean episode reward: -8.354046607986792, agent episode reward: [-1.6146661402434577, -6.739380467743333], time: 61.66
Running avgs for agent 0: q_loss: 0.1577676236629486, q_loss2: -0.1561068743467331, p_loss: 0.19201305508613586, mean_rew: -0.06254641588923535, var_rew: 0.118518659045733, lamda: 1.9352308511734009
Running avgs for agent 1: q_loss: 0.2427118867635727, q_loss2: -0.24245211482048035, p_loss: 1.392990231513977, mean_rew: -0.28844175199711486, var_rew: 0.16778811207384833, lamda: 1.996356725692749
steps: 1174975, episodes: 47000, mean episode reward: -8.292056808211711, agent episode reward: [-1.1117515082043925, -7.18030530000732], time: 59.789
Running avgs for agent 0: q_loss: 0.15854090452194214, q_loss2: -0.15686386823654175, p_loss: 0.23787298798561096, mean_rew: -0.06309399977983511, var_rew: 0.11789569712396195, lamda: 1.9579575061798096
Running avgs for agent 1: q_loss: 0.24051626026630402, q_loss2: -0.24026499688625336, p_loss: 1.3927675485610962, mean_rew: -0.28545562956291726, var_rew: 0.16548531917798043, lamda: 2.019287109375
steps: 1199975, episodes: 48000, mean episode reward: -8.401444340626792, agent episode reward: [-0.9033261660726662, -7.4981181745541265], time: 62.664
Running avgs for agent 0: q_loss: 0.16155393421649933, q_loss2: -0.1598116010427475, p_loss: 0.2731470465660095, mean_rew: -0.06136228739375899, var_rew: 0.11870163636727682, lamda: 1.981114387512207
Running avgs for agent 1: q_loss: 0.24385489523410797, q_loss2: -0.24359354376792908, p_loss: 1.4107412099838257, mean_rew: -0.2873387039527976, var_rew: 0.16569705669785503, lamda: 2.0424094200134277
steps: 1224975, episodes: 49000, mean episode reward: -8.541854426146408, agent episode reward: [-0.4923040680568145, -8.049550358089594], time: 61.574
Running avgs for agent 0: q_loss: 0.16147787868976593, q_loss2: -0.159775510430336, p_loss: 0.27822765707969666, mean_rew: -0.05909635827204755, var_rew: 0.11768847568845718, lamda: 2.0043864250183105
Running avgs for agent 1: q_loss: 0.24398916959762573, q_loss2: -0.24375084042549133, p_loss: 1.4109843969345093, mean_rew: -0.2873772765260338, var_rew: 0.16426449893794376, lamda: 2.0657925605773926
steps: 1249975, episodes: 50000, mean episode reward: -8.439645027009147, agent episode reward: [-0.5184576306843001, -7.921187396324849], time: 58.992
Running avgs for agent 0: q_loss: 0.1661897897720337, q_loss2: -0.16446185111999512, p_loss: 0.24458512663841248, mean_rew: -0.057615603010950166, var_rew: 0.1187438258180498, lamda: 2.028038740158081
Running avgs for agent 1: q_loss: 0.2468615621328354, q_loss2: -0.24662776291370392, p_loss: 1.4075437784194946, mean_rew: -0.2890821008016589, var_rew: 0.16433748045648908, lamda: 2.089284658432007
steps: 1274975, episodes: 51000, mean episode reward: -8.45642111004379, agent episode reward: [-0.4148772717019091, -8.04154383834188], time: 61.208
Running avgs for agent 0: q_loss: 0.1695568561553955, q_loss2: -0.1677444726228714, p_loss: 0.15553046762943268, mean_rew: -0.05695678699031015, var_rew: 0.11912577828155367, lamda: 2.0522217750549316
Running avgs for agent 1: q_loss: 0.2511366009712219, q_loss2: -0.2508734464645386, p_loss: 1.392026662826538, mean_rew: -0.2916738435703601, var_rew: 0.16530765326313626, lamda: 2.113266706466675
steps: 1299975, episodes: 52000, mean episode reward: -8.729268307942547, agent episode reward: [-0.18761158958033666, -8.541656718362212], time: 63.341
Running avgs for agent 0: q_loss: 0.1706262230873108, q_loss2: -0.1685153841972351, p_loss: 0.024574659764766693, mean_rew: -0.054410711934681964, var_rew: 0.11761818495477651, lamda: 2.0763051509857178
Running avgs for agent 1: q_loss: 0.2516583800315857, q_loss2: -0.25139671564102173, p_loss: 1.3495322465896606, mean_rew: -0.2918328753539392, var_rew: 0.16385602616905037, lamda: 2.1373777389526367
steps: 1324975, episodes: 53000, mean episode reward: -8.421445089013785, agent episode reward: [0.18530133906937918, -8.606746428083165], time: 61.677
Running avgs for agent 0: q_loss: 0.17461377382278442, q_loss2: -0.17263545095920563, p_loss: -0.10492359101772308, mean_rew: -0.051460996801211696, var_rew: 0.11866335983248123, lamda: 2.100480079650879
Running avgs for agent 1: q_loss: 0.2531892657279968, q_loss2: -0.25286808609962463, p_loss: 1.3147565126419067, mean_rew: -0.2938889501957936, var_rew: 0.16298531706362102, lamda: 2.1614298820495605
steps: 1349975, episodes: 54000, mean episode reward: -8.93854027951686, agent episode reward: [0.21922604327197187, -9.15776632278883], time: 64.46
Running avgs for agent 0: q_loss: 0.1791873723268509, q_loss2: -0.17715881764888763, p_loss: -0.23185227811336517, mean_rew: -0.04992936571876727, var_rew: 0.11924711213419154, lamda: 2.125080108642578
Running avgs for agent 1: q_loss: 0.25624027848243713, q_loss2: -0.25594720244407654, p_loss: 1.2872358560562134, mean_rew: -0.2974508893176668, var_rew: 0.16309738429423976, lamda: 2.1854984760284424
steps: 1374975, episodes: 55000, mean episode reward: -8.874999678075453, agent episode reward: [0.8109892922906856, -9.685988970366138], time: 58.738
Running avgs for agent 0: q_loss: 0.1825730800628662, q_loss2: -0.1806158423423767, p_loss: -0.37386542558670044, mean_rew: -0.048556687548167145, var_rew: 0.11948383885181556, lamda: 2.149705648422241
Running avgs for agent 1: q_loss: 0.25433534383773804, q_loss2: -0.254099041223526, p_loss: 1.2469514608383179, mean_rew: -0.2984979136561758, var_rew: 0.1617348400352639, lamda: 2.209693670272827
steps: 1399975, episodes: 56000, mean episode reward: -8.350278944087139, agent episode reward: [-1.105606946443529, -7.244671997643611], time: 61.992
Running avgs for agent 0: q_loss: 0.1836899071931839, q_loss2: -0.18165956437587738, p_loss: -0.48674100637435913, mean_rew: -0.04641259963350523, var_rew: 0.11858796357942064, lamda: 2.1744048595428467
Running avgs for agent 1: q_loss: 0.25172170996665955, q_loss2: -0.2514392137527466, p_loss: 1.2132099866867065, mean_rew: -0.29860379069297494, var_rew: 0.1591914464541652, lamda: 2.2336370944976807
steps: 1424975, episodes: 57000, mean episode reward: -8.668675258684543, agent episode reward: [-1.3181609580619946, -7.350514300622547], time: 58.441
Running avgs for agent 0: q_loss: 0.18711520731449127, q_loss2: -0.18505226075649261, p_loss: -0.4512072205543518, mean_rew: -0.04635484495033353, var_rew: 0.11957187600061912, lamda: 2.199021100997925
Running avgs for agent 1: q_loss: 0.2575730085372925, q_loss2: -0.25724610686302185, p_loss: 1.2049909830093384, mean_rew: -0.30072180723564407, var_rew: 0.16038482150797612, lamda: 2.257662534713745
steps: 1449975, episodes: 58000, mean episode reward: -8.640939973857, agent episode reward: [-1.083114795023556, -7.557825178833444], time: 58.649
Running avgs for agent 0: q_loss: 0.18712495267391205, q_loss2: -0.18515875935554504, p_loss: -0.38583484292030334, mean_rew: -0.0459911702719205, var_rew: 0.11931407055592168, lamda: 2.223933696746826
Running avgs for agent 1: q_loss: 0.2635663151741028, q_loss2: -0.26329830288887024, p_loss: 1.1987780332565308, mean_rew: -0.3027713687639883, var_rew: 0.1619100418027398, lamda: 2.2821664810180664
steps: 1474975, episodes: 59000, mean episode reward: -8.690507914846332, agent episode reward: [-0.1972208359118551, -8.493287078934475], time: 59.591
Running avgs for agent 0: q_loss: 0.18747113645076752, q_loss2: -0.18551208078861237, p_loss: -0.327273964881897, mean_rew: -0.04567563302796478, var_rew: 0.11858062328370775, lamda: 2.2485151290893555
Running avgs for agent 1: q_loss: 0.2612701654434204, q_loss2: -0.26103830337524414, p_loss: 1.2274845838546753, mean_rew: -0.3023013070365647, var_rew: 0.16027405435415415, lamda: 2.3066747188568115
steps: 1499975, episodes: 60000, mean episode reward: -8.276900474316834, agent episode reward: [-0.8766865947195103, -7.400213879597324], time: 54.537
Running avgs for agent 0: q_loss: 0.19008933007717133, q_loss2: -0.18820959329605103, p_loss: -0.3054397702217102, mean_rew: -0.04367250674478359, var_rew: 0.11912437552319918, lamda: 2.273205518722534
Running avgs for agent 1: q_loss: 0.2631777822971344, q_loss2: -0.26295116543769836, p_loss: 1.313260793685913, mean_rew: -0.3025773617869548, var_rew: 0.16044022320571558, lamda: 2.33115553855896
