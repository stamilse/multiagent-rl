python train_vanila.py --scenario simple_push --num-adversaries 1 --independent-learner True
2018-10-27 22:23:15.430642: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
in here
in here
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -27.223792801726464, agent episode reward: [-0.6160050039826126, -26.607787797743853], time: 19.087
/anaconda2/envs/py36/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/anaconda2/envs/py36/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, var_rew: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, var_rew: nan
steps: 49975, episodes: 2000, mean episode reward: -23.70204423139594, agent episode reward: [-5.498255029149401, -18.203789202246547], time: 25.808
Running avgs for agent 0: q_loss: 0.19440904259681702, p_loss: 0.11558164656162262, mean_rew: -0.03757518412260324, var_rew: 0.6761687200330473
Running avgs for agent 1: q_loss: 0.01764106936752796, p_loss: 1.4699941873550415, mean_rew: -1.0597704084294406, var_rew: 0.49505826757700666
steps: 74975, episodes: 3000, mean episode reward: -17.49242786348233, agent episode reward: [-10.24534818427324, -7.247079679209091], time: 25.459
Running avgs for agent 0: q_loss: 0.15769699215888977, p_loss: 0.26333752274513245, mean_rew: -0.16939160054136915, var_rew: 0.5533728995268777
Running avgs for agent 1: q_loss: 0.0028251081239432096, p_loss: 2.173224687576294, mean_rew: -0.7846201395094472, var_rew: 0.5184246203579295
steps: 99975, episodes: 4000, mean episode reward: -17.37958277070532, agent episode reward: [-10.555852044039861, -6.823730726665456], time: 25.71
Running avgs for agent 0: q_loss: 0.17781759798526764, p_loss: 0.5944702625274658, mean_rew: -0.24753984879533436, var_rew: 0.48115047314708365
Running avgs for agent 1: q_loss: 0.004429528024047613, p_loss: 2.480686902999878, mean_rew: -0.6371432094790707, var_rew: 0.47237610190037865
steps: 124975, episodes: 5000, mean episode reward: -15.121986395374028, agent episode reward: [-8.450914436934013, -6.671071958440015], time: 25.826
Running avgs for agent 0: q_loss: 0.21744179725646973, p_loss: 0.9615644812583923, mean_rew: -0.27782140553470036, var_rew: 0.42537812843075096
Running avgs for agent 1: q_loss: 0.004394202027469873, p_loss: 2.5286619663238525, mean_rew: -0.554275934513007, var_rew: 0.43030061622132715
steps: 149975, episodes: 6000, mean episode reward: -13.891539543246562, agent episode reward: [-7.152992664602185, -6.738546878644378], time: 25.57
Running avgs for agent 0: q_loss: 0.26476165652275085, p_loss: 1.2465413808822632, mean_rew: -0.2837158531771869, var_rew: 0.3821795555383415
Running avgs for agent 1: q_loss: 0.0044618211686611176, p_loss: 2.477613925933838, mean_rew: -0.5024632881348817, var_rew: 0.39835552374891997
steps: 174975, episodes: 7000, mean episode reward: -12.657998196948132, agent episode reward: [-6.140499227036024, -6.517498969912108], time: 25.528
Running avgs for agent 0: q_loss: 0.3047997057437897, p_loss: 1.4296678304672241, mean_rew: -0.28090235285419574, var_rew: 0.3470257926986924
Running avgs for agent 1: q_loss: 0.005044031888246536, p_loss: 2.3729326725006104, mean_rew: -0.4651839867213257, var_rew: 0.3690968785826956
steps: 199975, episodes: 8000, mean episode reward: -12.054858761460746, agent episode reward: [-5.740203897332838, -6.314654864127909], time: 25.286
Running avgs for agent 0: q_loss: 0.34399646520614624, p_loss: 1.5064473152160645, mean_rew: -0.27310057426819184, var_rew: 0.31657429697205586
Running avgs for agent 1: q_loss: 0.004713630769401789, p_loss: 2.2668261528015137, mean_rew: -0.4375887989367463, var_rew: 0.34642250356955584
steps: 224975, episodes: 9000, mean episode reward: -12.395120440357868, agent episode reward: [-5.8052184969517535, -6.5899019434061135], time: 25.322
Running avgs for agent 0: q_loss: 0.3697459101676941, p_loss: 1.5671086311340332, mean_rew: -0.27115853011667346, var_rew: 0.29676209334008374
Running avgs for agent 1: q_loss: 0.005076673347502947, p_loss: 2.1822240352630615, mean_rew: -0.41711993215104465, var_rew: 0.33317350742518
steps: 249975, episodes: 10000, mean episode reward: -11.88453637530596, agent episode reward: [-5.400183564265653, -6.484352811040307], time: 25.26
Running avgs for agent 0: q_loss: 0.4045785069465637, p_loss: 1.57064950466156, mean_rew: -0.263738796283278, var_rew: 0.28095880207780305
Running avgs for agent 1: q_loss: 0.004693442024290562, p_loss: 2.1048808097839355, mean_rew: -0.4018226835690829, var_rew: 0.31619800015653426
steps: 274975, episodes: 11000, mean episode reward: -12.198917715703924, agent episode reward: [-5.550139460246895, -6.648778255457031], time: 25.107
Running avgs for agent 0: q_loss: 0.43947532773017883, p_loss: 1.5617822408676147, mean_rew: -0.2613092379744868, var_rew: 0.2687327171366547
Running avgs for agent 1: q_loss: 0.005773573648184538, p_loss: 2.0406854152679443, mean_rew: -0.38741878404394886, var_rew: 0.30395605742567916
steps: 299975, episodes: 12000, mean episode reward: -12.07235643210265, agent episode reward: [-5.191325709899155, -6.881030722203495], time: 25.249
Running avgs for agent 0: q_loss: 0.48493698239326477, p_loss: 1.5662513971328735, mean_rew: -0.2569211092135573, var_rew: 0.2525335967349502
Running avgs for agent 1: q_loss: 0.005706850439310074, p_loss: 1.99385666847229, mean_rew: -0.37738769024325913, var_rew: 0.2921197870110726
steps: 324975, episodes: 13000, mean episode reward: -12.183966792260646, agent episode reward: [-5.466756108433972, -6.717210683826672], time: 25.402
Running avgs for agent 0: q_loss: 0.4909462332725525, p_loss: 1.5518430471420288, mean_rew: -0.2527947292664504, var_rew: 0.2448555218768163
Running avgs for agent 1: q_loss: 0.005018982104957104, p_loss: 1.9506717920303345, mean_rew: -0.3671273395069645, var_rew: 0.2853485705850931
steps: 349975, episodes: 14000, mean episode reward: -11.70873452485914, agent episode reward: [-4.950982793589235, -6.757751731269908], time: 25.361
Running avgs for agent 0: q_loss: 0.5463119149208069, p_loss: 1.5293020009994507, mean_rew: -0.2509950486296237, var_rew: 0.23596591477516915
Running avgs for agent 1: q_loss: 0.00518086738884449, p_loss: 1.9315634965896606, mean_rew: -0.3619642100537152, var_rew: 0.28098212804087314
steps: 374975, episodes: 15000, mean episode reward: -11.845808930823377, agent episode reward: [-5.389521248724747, -6.4562876820986315], time: 25.207
Running avgs for agent 0: q_loss: 0.5715488791465759, p_loss: 1.4990681409835815, mean_rew: -0.2457600789781014, var_rew: 0.22978502482377294
Running avgs for agent 1: q_loss: 0.0049813962541520596, p_loss: 1.8999109268188477, mean_rew: -0.3563079597865216, var_rew: 0.27347689676282244
steps: 399975, episodes: 16000, mean episode reward: -11.711946148263523, agent episode reward: [-5.097954163250482, -6.613991985013043], time: 25.163
Running avgs for agent 0: q_loss: 0.6402857303619385, p_loss: 1.481445550918579, mean_rew: -0.2435513731436947, var_rew: 0.22147116350534646
Running avgs for agent 1: q_loss: 0.011302310973405838, p_loss: 1.8537720441818237, mean_rew: -0.34821825368659903, var_rew: 0.2656280609119029
steps: 424975, episodes: 17000, mean episode reward: -11.629122902088376, agent episode reward: [-5.102499106719375, -6.526623795369003], time: 24.721
Running avgs for agent 0: q_loss: 0.6967170238494873, p_loss: 1.4779160022735596, mean_rew: -0.24309861472706296, var_rew: 0.21623002623013457
Running avgs for agent 1: q_loss: 0.002872937824577093, p_loss: 1.8334128856658936, mean_rew: -0.34490664923508074, var_rew: 0.26182487405972976
steps: 449975, episodes: 18000, mean episode reward: -11.438635731551674, agent episode reward: [-4.987502188187101, -6.4511335433645725], time: 24.305
Running avgs for agent 0: q_loss: 0.7434747815132141, p_loss: 1.4338786602020264, mean_rew: -0.24038132906127008, var_rew: 0.21175201054876522
Running avgs for agent 1: q_loss: 0.0047839004546403885, p_loss: 1.8056038618087769, mean_rew: -0.3388727504025539, var_rew: 0.25620333915728416
steps: 474975, episodes: 19000, mean episode reward: -11.634456203611547, agent episode reward: [-5.160664528699613, -6.473791674911934], time: 24.258
Running avgs for agent 0: q_loss: 0.8548835515975952, p_loss: 1.417612075805664, mean_rew: -0.23973286809720668, var_rew: 0.20502108734218988
Running avgs for agent 1: q_loss: 0.004817652050405741, p_loss: 1.7683981657028198, mean_rew: -0.33228256114965876, var_rew: 0.24858366955965322
steps: 499975, episodes: 20000, mean episode reward: -11.853451884455135, agent episode reward: [-5.248150713513158, -6.605301170941978], time: 24.254
Running avgs for agent 0: q_loss: 0.9050854444503784, p_loss: 1.3938572406768799, mean_rew: -0.2380422076698277, var_rew: 0.2018995855666422
Running avgs for agent 1: q_loss: 0.0033678365871310234, p_loss: 1.7583799362182617, mean_rew: -0.33087022908543473, var_rew: 0.2459936131032311
steps: 524975, episodes: 21000, mean episode reward: -11.336728063551872, agent episode reward: [-4.7550859170367445, -6.5816421465151285], time: 24.348
Running avgs for agent 0: q_loss: 1.012333869934082, p_loss: 1.3648284673690796, mean_rew: -0.23521492572951036, var_rew: 0.1994499765350072
Running avgs for agent 1: q_loss: 0.0058127534575760365, p_loss: 1.73762845993042, mean_rew: -0.3266055415152566, var_rew: 0.24305356711934892
steps: 549975, episodes: 22000, mean episode reward: -11.063976736488595, agent episode reward: [-4.588487883948341, -6.475488852540253], time: 24.829
Running avgs for agent 0: q_loss: 1.124459981918335, p_loss: 1.3164798021316528, mean_rew: -0.23200303209176618, var_rew: 0.1954355767404736
Running avgs for agent 1: q_loss: 0.005247934255748987, p_loss: 1.729007601737976, mean_rew: -0.3252882088113241, var_rew: 0.24160519760932408
steps: 574975, episodes: 23000, mean episode reward: -11.842392700544867, agent episode reward: [-5.247830600803019, -6.59456209974185], time: 25.013
Running avgs for agent 0: q_loss: 1.2332401275634766, p_loss: 1.3085147142410278, mean_rew: -0.2304643005308482, var_rew: 0.190360897417494
Running avgs for agent 1: q_loss: 0.0034032189287245274, p_loss: 1.7084825038909912, mean_rew: -0.32156989792023716, var_rew: 0.23791829372037188
steps: 599975, episodes: 24000, mean episode reward: -12.55130961491952, agent episode reward: [-5.789220186638953, -6.7620894282805635], time: 25.289
Running avgs for agent 0: q_loss: 1.2878667116165161, p_loss: 1.302380084991455, mean_rew: -0.23009354792647035, var_rew: 0.1908700821247088
Running avgs for agent 1: q_loss: 0.004597967956215143, p_loss: 1.690001130104065, mean_rew: -0.3186131403833234, var_rew: 0.23444375492580302
steps: 624975, episodes: 25000, mean episode reward: -11.912075333406014, agent episode reward: [-5.270229008228833, -6.641846325177181], time: 25.147
Running avgs for agent 0: q_loss: 1.459648609161377, p_loss: 1.2466144561767578, mean_rew: -0.22840768817090765, var_rew: 0.19035917070278124
Running avgs for agent 1: q_loss: 0.0037772732321172953, p_loss: 1.6668232679367065, mean_rew: -0.3154255667157656, var_rew: 0.23136713575963727
steps: 649975, episodes: 26000, mean episode reward: -12.334266958252199, agent episode reward: [-5.542653925898123, -6.791613032354078], time: 24.465
Running avgs for agent 0: q_loss: 1.425390601158142, p_loss: 1.2081184387207031, mean_rew: -0.22839886311152838, var_rew: 0.1906035750169667
Running avgs for agent 1: q_loss: 0.005148703698068857, p_loss: 1.654287576675415, mean_rew: -0.31536178396496123, var_rew: 0.229027417230904
steps: 674975, episodes: 27000, mean episode reward: -12.404259065303625, agent episode reward: [-5.6823411113021765, -6.72191795400145], time: 24.275
Running avgs for agent 0: q_loss: 1.4643993377685547, p_loss: 1.2452645301818848, mean_rew: -0.22840678265863107, var_rew: 0.18994619044162125
Running avgs for agent 1: q_loss: 0.005179386120289564, p_loss: 1.6319736242294312, mean_rew: -0.31338192126796177, var_rew: 0.22876829024866163
steps: 699975, episodes: 28000, mean episode reward: -13.390578297176384, agent episode reward: [-6.436691915734928, -6.953886381441456], time: 24.734
Running avgs for agent 0: q_loss: 1.4652851819992065, p_loss: 1.132076382637024, mean_rew: -0.22883643124000805, var_rew: 0.19423781786625963
Running avgs for agent 1: q_loss: 0.0040749176405370235, p_loss: 1.6083532571792603, mean_rew: -0.3118557183630845, var_rew: 0.22507533468503602
steps: 724975, episodes: 29000, mean episode reward: -12.851353874341894, agent episode reward: [-5.951454932789289, -6.899898941552603], time: 24.899
Running avgs for agent 0: q_loss: 1.4999792575836182, p_loss: 1.0718896389007568, mean_rew: -0.23078462825779583, var_rew: 0.1961635767315484
Running avgs for agent 1: q_loss: 0.005329214036464691, p_loss: 1.595849871635437, mean_rew: -0.31048012591051793, var_rew: 0.22350032309285114
steps: 749975, episodes: 30000, mean episode reward: -13.249494718358266, agent episode reward: [-6.412045116098836, -6.83744960225943], time: 24.442
Running avgs for agent 0: q_loss: 1.5627548694610596, p_loss: 0.785280168056488, mean_rew: -0.23064859923458528, var_rew: 0.1986679088121814
Running avgs for agent 1: q_loss: 0.0040612299926579, p_loss: 1.578274130821228, mean_rew: -0.3086444137931502, var_rew: 0.22120600887483272
steps: 774975, episodes: 31000, mean episode reward: -13.151822469479312, agent episode reward: [-6.402525618976784, -6.749296850502527], time: 24.464
Running avgs for agent 0: q_loss: 1.5757707357406616, p_loss: 0.5287236571311951, mean_rew: -0.23082728371811825, var_rew: 0.2010866130753111
Running avgs for agent 1: q_loss: 0.005592999514192343, p_loss: 1.5623469352722168, mean_rew: -0.30954350015302007, var_rew: 0.22100333752299212
steps: 799975, episodes: 32000, mean episode reward: -12.778142908674068, agent episode reward: [-6.33745251050119, -6.440690398172878], time: 24.964
Running avgs for agent 0: q_loss: 1.700740933418274, p_loss: 0.3951239585876465, mean_rew: -0.23187311600524946, var_rew: 0.20396401286079993
Running avgs for agent 1: q_loss: 0.007066681981086731, p_loss: 1.5275126695632935, mean_rew: -0.3066427365867815, var_rew: 0.21826147986492492
steps: 824975, episodes: 33000, mean episode reward: -12.773486682336898, agent episode reward: [-5.735660690355349, -7.037825991981549], time: 24.959
Running avgs for agent 0: q_loss: 1.880806565284729, p_loss: 0.3547082543373108, mean_rew: -0.232627897617042, var_rew: 0.20105620908098656
Running avgs for agent 1: q_loss: 0.004379670601338148, p_loss: 1.4968167543411255, mean_rew: -0.30416471597006356, var_rew: 0.2144534128445859
steps: 849975, episodes: 34000, mean episode reward: -12.274752294941617, agent episode reward: [-5.315710513329467, -6.959041781612152], time: 24.567
Running avgs for agent 0: q_loss: 2.0177152156829834, p_loss: 0.3506408631801605, mean_rew: -0.23354995344243704, var_rew: 0.2008935552659458
Running avgs for agent 1: q_loss: 0.004498724360018969, p_loss: 1.493048071861267, mean_rew: -0.304290901346394, var_rew: 0.21383467631098368
steps: 874975, episodes: 35000, mean episode reward: -13.067358226288626, agent episode reward: [-6.311192346009943, -6.756165880278682], time: 24.377
Running avgs for agent 0: q_loss: 2.492037296295166, p_loss: 0.3071739077568054, mean_rew: -0.23139256840108213, var_rew: 0.19820962386139615
Running avgs for agent 1: q_loss: 0.00443440955132246, p_loss: 1.502864122390747, mean_rew: -0.304944580719117, var_rew: 0.21357910768293026
steps: 899975, episodes: 36000, mean episode reward: -12.627514650273637, agent episode reward: [-5.827758971117759, -6.799755679155878], time: 24.284
Running avgs for agent 0: q_loss: 2.1563820838928223, p_loss: 0.369810551404953, mean_rew: -0.2339384047448231, var_rew: 0.1980533602588377
Running avgs for agent 1: q_loss: 0.004730057902634144, p_loss: 1.4988107681274414, mean_rew: -0.3024220708915244, var_rew: 0.21106405575987203
steps: 924975, episodes: 37000, mean episode reward: -12.370717969517075, agent episode reward: [-5.44472222246734, -6.925995747049737], time: 24.511
Running avgs for agent 0: q_loss: 2.542677640914917, p_loss: 0.347258061170578, mean_rew: -0.2333845502113248, var_rew: 0.19497529853567053
Running avgs for agent 1: q_loss: 0.0050628455355763435, p_loss: 1.5168099403381348, mean_rew: -0.3027820741655669, var_rew: 0.21156154740463665
steps: 949975, episodes: 38000, mean episode reward: -12.80802221328163, agent episode reward: [-5.345151726716345, -7.462870486565287], time: 24.689
Running avgs for agent 0: q_loss: 2.7870707511901855, p_loss: 0.4019298255443573, mean_rew: -0.2319599973167759, var_rew: 0.1927242951232044
Running avgs for agent 1: q_loss: 0.004201125353574753, p_loss: 1.5235719680786133, mean_rew: -0.30158295596188456, var_rew: 0.20958882398447146
steps: 974975, episodes: 39000, mean episode reward: -13.644689122426644, agent episode reward: [-5.8336209294404195, -7.811068192986225], time: 24.399
Running avgs for agent 0: q_loss: 2.882183074951172, p_loss: 0.46492260694503784, mean_rew: -0.23236749567460255, var_rew: 0.1937634612013873
Running avgs for agent 1: q_loss: 0.004524345509707928, p_loss: 1.5392595529556274, mean_rew: -0.30237672755373135, var_rew: 0.20954245440976924
steps: 999975, episodes: 40000, mean episode reward: -12.955063590339547, agent episode reward: [-5.9650656368821, -6.989997953457447], time: 24.36
Running avgs for agent 0: q_loss: 2.7865450382232666, p_loss: 0.5019485354423523, mean_rew: -0.23391937474266575, var_rew: 0.19248197862611108
Running avgs for agent 1: q_loss: 0.004360111430287361, p_loss: 1.5477690696716309, mean_rew: -0.30310449057188865, var_rew: 0.20825098761233324
steps: 1024975, episodes: 41000, mean episode reward: -12.132266330819624, agent episode reward: [-5.496483804152047, -6.635782526667578], time: 24.317
Running avgs for agent 0: q_loss: 3.0959370136260986, p_loss: 0.5550318360328674, mean_rew: -0.2335065466650861, var_rew: 0.18848645703938274
Running avgs for agent 1: q_loss: 0.00557303661480546, p_loss: 1.4895951747894287, mean_rew: -0.2916525688460224, var_rew: 0.1991015738766705
steps: 1049975, episodes: 42000, mean episode reward: -11.355337664600038, agent episode reward: [-4.398506702685551, -6.956830961914487], time: 24.289
Running avgs for agent 0: q_loss: 0.7952207326889038, p_loss: 1.1534063816070557, mean_rew: -0.2395015565052679, var_rew: 0.17139932859543847
Running avgs for agent 1: q_loss: 0.004495087079703808, p_loss: 1.3558303117752075, mean_rew: -0.2729406488096738, var_rew: 0.17387506493699784
steps: 1074975, episodes: 43000, mean episode reward: -10.516956194452229, agent episode reward: [-3.882944506564773, -6.634011687887458], time: 24.324
Running avgs for agent 0: q_loss: 0.22143353521823883, p_loss: 1.244575023651123, mean_rew: -0.23349809211842995, var_rew: 0.1659274237335935
Running avgs for agent 1: q_loss: 0.00476593803614378, p_loss: 1.3312517404556274, mean_rew: -0.27020493925419325, var_rew: 0.17144820053012896
steps: 1099975, episodes: 44000, mean episode reward: -10.163446824094713, agent episode reward: [-3.5856149573354665, -6.577831866759246], time: 24.333
Running avgs for agent 0: q_loss: 0.166493758559227, p_loss: 1.251462697982788, mean_rew: -0.2269253894653678, var_rew: 0.1618230919801428
Running avgs for agent 1: q_loss: 0.004587257746607065, p_loss: 1.3143739700317383, mean_rew: -0.26929075281748144, var_rew: 0.17044149407916212
steps: 1124975, episodes: 45000, mean episode reward: -10.058543995125001, agent episode reward: [-3.326619258501643, -6.731924736623356], time: 24.304
Running avgs for agent 0: q_loss: 0.14736104011535645, p_loss: 1.2573342323303223, mean_rew: -0.21899355852680122, var_rew: 0.15588291451409167
Running avgs for agent 1: q_loss: 0.004777731839567423, p_loss: 1.3012460470199585, mean_rew: -0.2694607142932658, var_rew: 0.17038475859468355
steps: 1149975, episodes: 46000, mean episode reward: -9.797933170264226, agent episode reward: [-3.039100541334061, -6.758832628930166], time: 24.376
Running avgs for agent 0: q_loss: 0.13001516461372375, p_loss: 1.2781156301498413, mean_rew: -0.21534697870470715, var_rew: 0.1553218802370648
Running avgs for agent 1: q_loss: 0.00461209611967206, p_loss: 1.2980520725250244, mean_rew: -0.2698148441532204, var_rew: 0.1705105434956318
steps: 1174975, episodes: 47000, mean episode reward: -9.957176309746469, agent episode reward: [-2.8079530132753487, -7.1492232964711215], time: 24.298
Running avgs for agent 0: q_loss: 0.12606902420520782, p_loss: 1.2718678712844849, mean_rew: -0.21058945255085745, var_rew: 0.15337404179613356
Running avgs for agent 1: q_loss: 0.00506027415394783, p_loss: 1.2756233215332031, mean_rew: -0.26964942497199185, var_rew: 0.17012136780769593
steps: 1199975, episodes: 48000, mean episode reward: -9.669101641859504, agent episode reward: [-1.9023680800008895, -7.766733561858615], time: 24.298
Running avgs for agent 0: q_loss: 0.12151148915290833, p_loss: 1.2724312543869019, mean_rew: -0.2078764562484289, var_rew: 0.15609663927495296
Running avgs for agent 1: q_loss: 0.006658274680376053, p_loss: 1.2406089305877686, mean_rew: -0.26998231665225014, var_rew: 0.16929003034792633
steps: 1224975, episodes: 49000, mean episode reward: -9.782422164569818, agent episode reward: [-1.6369264890123791, -8.14549567555744], time: 24.423
Running avgs for agent 0: q_loss: 0.11781904846429825, p_loss: 1.2688549757003784, mean_rew: -0.2044799901717109, var_rew: 0.1560006260345865
Running avgs for agent 1: q_loss: 0.007129087578505278, p_loss: 1.2598990201950073, mean_rew: -0.272063910689711, var_rew: 0.1691702031466774
steps: 1249975, episodes: 50000, mean episode reward: -9.423926117826628, agent episode reward: [-1.4716277881013082, -7.95229832972532], time: 24.382
Running avgs for agent 0: q_loss: 0.10963203012943268, p_loss: 1.2486203908920288, mean_rew: -0.20068791875590192, var_rew: 0.15494454897571494
Running avgs for agent 1: q_loss: 0.009090742096304893, p_loss: 1.2986760139465332, mean_rew: -0.2754103073811394, var_rew: 0.16989857207222736
steps: 1274975, episodes: 51000, mean episode reward: -9.730396253110634, agent episode reward: [-1.53462333451175, -8.195772918598882], time: 24.327
Running avgs for agent 0: q_loss: 0.11361787468194962, p_loss: 1.2202632427215576, mean_rew: -0.1969203944501555, var_rew: 0.1558538555038405
Running avgs for agent 1: q_loss: 0.013458645902574062, p_loss: 1.3138703107833862, mean_rew: -0.27538060342369447, var_rew: 0.16900912318787986
steps: 1299975, episodes: 52000, mean episode reward: -9.396885842019643, agent episode reward: [-1.8756787712118603, -7.521207070807782], time: 24.297
Running avgs for agent 0: q_loss: 0.1115715429186821, p_loss: 1.189683198928833, mean_rew: -0.19283033548906256, var_rew: 0.15608591178551678
Running avgs for agent 1: q_loss: 0.01497685257345438, p_loss: 1.3235725164413452, mean_rew: -0.2758902168587596, var_rew: 0.16783701505751739
steps: 1324975, episodes: 53000, mean episode reward: -9.620981959442513, agent episode reward: [-1.816911529627976, -7.804070429814537], time: 24.27
Running avgs for agent 0: q_loss: 0.10987162590026855, p_loss: 1.1705063581466675, mean_rew: -0.18978425863444717, var_rew: 0.15511698211888814
Running avgs for agent 1: q_loss: 0.015491883270442486, p_loss: 1.3155218362808228, mean_rew: -0.27730688475696186, var_rew: 0.16830744064043687
steps: 1349975, episodes: 54000, mean episode reward: -9.662164777259825, agent episode reward: [-1.8053083769631182, -7.8568564002967065], time: 24.438
Running avgs for agent 0: q_loss: 0.10936598479747772, p_loss: 1.1424885988235474, mean_rew: -0.18677044744575608, var_rew: 0.15519146964083608
Running avgs for agent 1: q_loss: 0.015017608180642128, p_loss: 1.2811427116394043, mean_rew: -0.27862816069647073, var_rew: 0.16691840290790083
steps: 1374975, episodes: 55000, mean episode reward: -9.84274212424903, agent episode reward: [-1.7385963928468917, -8.10414573140214], time: 24.223
Running avgs for agent 0: q_loss: 0.10153037309646606, p_loss: 1.0988222360610962, mean_rew: -0.184184178212782, var_rew: 0.15296298220146207
Running avgs for agent 1: q_loss: 0.015460067428648472, p_loss: 1.2448683977127075, mean_rew: -0.2811180184862741, var_rew: 0.1680104333277555
steps: 1399975, episodes: 56000, mean episode reward: -9.525074517453868, agent episode reward: [-1.0026047088896304, -8.522469808564237], time: 24.25
Running avgs for agent 0: q_loss: 0.10675607621669769, p_loss: 1.0606496334075928, mean_rew: -0.17829334144774042, var_rew: 0.15492178105789134
Running avgs for agent 1: q_loss: 0.014609859324991703, p_loss: 1.1672333478927612, mean_rew: -0.2801580816336822, var_rew: 0.16500299413533515
steps: 1424975, episodes: 57000, mean episode reward: -9.955204678522088, agent episode reward: [-0.29345616851251066, -9.661748510009579], time: 24.24
Running avgs for agent 0: q_loss: 0.10218746960163116, p_loss: 0.9996441602706909, mean_rew: -0.17147135887721152, var_rew: 0.15368891509891522
Running avgs for agent 1: q_loss: 0.01636856235563755, p_loss: 1.0595134496688843, mean_rew: -0.28499446555410846, var_rew: 0.16608181402082045
steps: 1449975, episodes: 58000, mean episode reward: -10.057397762385063, agent episode reward: [-0.7671463325995238, -9.290251429785542], time: 24.201
Running avgs for agent 0: q_loss: 0.1033572107553482, p_loss: 0.9629898071289062, mean_rew: -0.1697903080752819, var_rew: 0.15868559662626325
Running avgs for agent 1: q_loss: 0.01421287003904581, p_loss: 0.9904870986938477, mean_rew: -0.28659564587464353, var_rew: 0.1644556816303293
steps: 1474975, episodes: 59000, mean episode reward: -9.719699803594795, agent episode reward: [-2.1511625450502714, -7.5685372585445245], time: 24.254
Running avgs for agent 0: q_loss: 0.10475471615791321, p_loss: 0.9085193276405334, mean_rew: -0.16737038200908722, var_rew: 0.15668744410344942
Running avgs for agent 1: q_loss: 0.019781773909926414, p_loss: 0.9873026013374329, mean_rew: -0.2886558844416029, var_rew: 0.16463715157978406
steps: 1499975, episodes: 60000, mean episode reward: -9.681559636242415, agent episode reward: [-2.0112561433512113, -7.670303492891202], time: 24.343
Running avgs for agent 0: q_loss: 0.10351520031690598, p_loss: 0.8441841006278992, mean_rew: -0.16322171882732198, var_rew: 0.15765052782050834
Running avgs for agent 1: q_loss: 0.020605426281690598, p_loss: 0.922151505947113, mean_rew: -0.29006802898185, var_rew: 0.1637211631905322
