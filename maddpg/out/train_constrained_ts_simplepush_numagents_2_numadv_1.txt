python train.py --scenario simple_push --num-adversaries 1 --lr_actor 0.005 --lr_critic 0.01 --lr_lamda 0.0001 --alpha 0.05
2018-10-27 22:56:28.054526: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -27.098564123152023, agent episode reward: [-1.327440944150909, -25.771123179001115], time: 18.374
/anaconda2/envs/py36/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/anaconda2/envs/py36/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
Running avgs for agent 0: q_loss: nan, q_loss2: nan, p_loss: nan, mean_rew: nan, var_rew: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, q_loss2: nan, p_loss: nan, mean_rew: nan, var_rew: nan, lamda: nan
steps: 49975, episodes: 2000, mean episode reward: -26.445533845052406, agent episode reward: [-5.5838838067297365, -20.861650038322665], time: 29.122
Running avgs for agent 0: q_loss: 0.5901719927787781, q_loss2: -0.5538650155067444, p_loss: 0.1993299275636673, mean_rew: -0.101510262008567, var_rew: 0.05750427022576332, lamda: 1.0117254257202148
Running avgs for agent 1: q_loss: 1.0250262022018433, q_loss2: -1.0193121433258057, p_loss: 1.467410683631897, mean_rew: -1.0726705472280564, var_rew: 0.0035174957010895014, lamda: 1.012107491493225
steps: 74975, episodes: 3000, mean episode reward: -13.032346244836036, agent episode reward: [-5.197394424343743, -7.834951820492294], time: 28.686
Running avgs for agent 0: q_loss: 3.2056477069854736, q_loss2: -3.119412422180176, p_loss: 0.38517045974731445, mean_rew: -0.15693119873588393, var_rew: 0.03081716224551201, lamda: 1.0373928546905518
Running avgs for agent 1: q_loss: 3.3896543979644775, q_loss2: -3.4362106323242188, p_loss: 2.3025736808776855, mean_rew: -0.8185152005895614, var_rew: -7.201862172223628e-05, lamda: 1.037034273147583
steps: 99975, episodes: 4000, mean episode reward: -10.52805883809578, agent episode reward: [-3.003669108783553, -7.524389729312226], time: 28.951
Running avgs for agent 0: q_loss: 10.761152267456055, q_loss2: -10.033695220947266, p_loss: 0.5642795562744141, mean_rew: -0.16118300627241147, var_rew: 0.021801240742206573, lamda: 1.0633854866027832
Running avgs for agent 1: q_loss: 14.255526542663574, q_loss2: -13.854928016662598, p_loss: 2.6745433807373047, mean_rew: -0.6671563572566592, var_rew: 0.05971766635775566, lamda: 1.0622825622558594
steps: 124975, episodes: 5000, mean episode reward: -10.13911878053933, agent episode reward: [-2.4103471562832075, -7.728771624256123], time: 28.961
Running avgs for agent 0: q_loss: 18.813446044921875, q_loss2: -17.47011375427246, p_loss: 0.580045759677887, mean_rew: -0.14594312006024038, var_rew: 0.0009348997846245766, lamda: 1.0889261960983276
Running avgs for agent 1: q_loss: 21.057144165039062, q_loss2: -21.233442306518555, p_loss: 2.7234585285186768, mean_rew: -0.588330751341686, var_rew: 0.1262015551328659, lamda: 1.0874207019805908
steps: 149975, episodes: 6000, mean episode reward: -10.286017565367736, agent episode reward: [-2.387618046222371, -7.898399519145365], time: 29.019
Running avgs for agent 0: q_loss: 27.214921951293945, q_loss2: -23.957542419433594, p_loss: 0.5402055382728577, mean_rew: -0.13887814903569062, var_rew: -0.008377992548048496, lamda: 1.1142690181732178
Running avgs for agent 1: q_loss: 51.68760299682617, q_loss2: -51.14104080200195, p_loss: 2.6284492015838623, mean_rew: -0.5376534999295127, var_rew: 0.07445649802684784, lamda: 1.1125092506408691
steps: 174975, episodes: 7000, mean episode reward: -10.67722004896776, agent episode reward: [-2.0369960757156105, -8.640223973252152], time: 29.008
Running avgs for agent 0: q_loss: 48.44602584838867, q_loss2: -43.8009147644043, p_loss: 0.4276605546474457, mean_rew: -0.13012829756509314, var_rew: -0.01915675401687622, lamda: 1.1395028829574585
Running avgs for agent 1: q_loss: 46.727027893066406, q_loss2: -47.488101959228516, p_loss: 2.501899003982544, mean_rew: -0.5029322383369423, var_rew: -0.03174379840493202, lamda: 1.1375715732574463
steps: 199975, episodes: 8000, mean episode reward: -10.926690098192914, agent episode reward: [-1.9017071774344492, -9.024982920758465], time: 29.128
Running avgs for agent 0: q_loss: 49.588680267333984, q_loss2: -43.69129943847656, p_loss: 0.3505629599094391, mean_rew: -0.12250662750708764, var_rew: 0.002765838522464037, lamda: 1.1646684408187866
Running avgs for agent 1: q_loss: 48.77907943725586, q_loss2: -49.6175537109375, p_loss: 2.4273171424865723, mean_rew: -0.4851276298991884, var_rew: 0.05297771468758583, lamda: 1.162610650062561
steps: 224975, episodes: 9000, mean episode reward: -10.984888450100511, agent episode reward: [-1.2845049746641184, -9.700383475436393], time: 30.513
Running avgs for agent 0: q_loss: 73.77713775634766, q_loss2: -67.36309051513672, p_loss: 0.2735978960990906, mean_rew: -0.11541425674346137, var_rew: 0.06597404181957245, lamda: 1.1897891759872437
Running avgs for agent 1: q_loss: 90.25800323486328, q_loss2: -87.31087493896484, p_loss: 2.4149129390716553, mean_rew: -0.4720795013079595, var_rew: 0.06512033939361572, lamda: 1.187644600868225
steps: 249975, episodes: 10000, mean episode reward: -11.509386630531464, agent episode reward: [-1.6652615278788674, -9.844125102652598], time: 30.522
Running avgs for agent 0: q_loss: 91.83515930175781, q_loss2: -82.50482177734375, p_loss: 0.23657527565956116, mean_rew: -0.10794369929089372, var_rew: 0.07280690222978592, lamda: 1.2148795127868652
Running avgs for agent 1: q_loss: 89.5999526977539, q_loss2: -84.22220611572266, p_loss: 2.399186134338379, mean_rew: -0.4653760285764073, var_rew: 0.13576778769493103, lamda: 1.212677240371704
steps: 274975, episodes: 11000, mean episode reward: -11.982498744925213, agent episode reward: [-1.8360403218303654, -10.146458423094847], time: 30.194
Running avgs for agent 0: q_loss: 94.24598693847656, q_loss2: -79.54911804199219, p_loss: 0.2308393269777298, mean_rew: -0.1060655817037509, var_rew: 0.00993373803794384, lamda: 1.2399460077285767
Running avgs for agent 1: q_loss: 108.69796752929688, q_loss2: -108.30865478515625, p_loss: 2.5020835399627686, mean_rew: -0.457877912434235, var_rew: 0.005915840156376362, lamda: 1.237688422203064
steps: 299975, episodes: 12000, mean episode reward: -13.297826807391166, agent episode reward: [-3.28048319013448, -10.017343617256685], time: 29.277
Running avgs for agent 0: q_loss: 80.0299072265625, q_loss2: -68.54598999023438, p_loss: 0.2477502077817917, mean_rew: -0.1052231495525541, var_rew: 0.04897112771868706, lamda: 1.2649985551834106
Running avgs for agent 1: q_loss: 85.11270904541016, q_loss2: -79.28768920898438, p_loss: 2.57405686378479, mean_rew: -0.45470390377391356, var_rew: 0.04787081480026245, lamda: 1.2626925706863403
steps: 324975, episodes: 13000, mean episode reward: -12.493386634525145, agent episode reward: [-2.146991011048544, -10.346395623476601], time: 29.173
Running avgs for agent 0: q_loss: 82.15504455566406, q_loss2: -68.70726013183594, p_loss: 0.2566596269607544, mean_rew: -0.10675907910159682, var_rew: 0.019053729251027107, lamda: 1.2900327444076538
Running avgs for agent 1: q_loss: 66.0658950805664, q_loss2: -64.21697998046875, p_loss: 2.6118710041046143, mean_rew: -0.44925562166982197, var_rew: 0.043064601719379425, lamda: 1.2876967191696167
steps: 349975, episodes: 14000, mean episode reward: -12.996624024608945, agent episode reward: [-3.5275643381377515, -9.469059686471196], time: 29.274
Running avgs for agent 0: q_loss: 90.40483093261719, q_loss2: -76.32159423828125, p_loss: 0.29625898599624634, mean_rew: -0.10786607321636792, var_rew: 0.04753829911351204, lamda: 1.3150666952133179
Running avgs for agent 1: q_loss: 102.12408447265625, q_loss2: -97.0872573852539, p_loss: 2.638848066329956, mean_rew: -0.44452901757899504, var_rew: 0.020384574308991432, lamda: 1.3127009868621826
steps: 374975, episodes: 15000, mean episode reward: -12.620602784543745, agent episode reward: [-3.5642528461096026, -9.056349938434142], time: 29.223
Running avgs for agent 0: q_loss: 51.5598030090332, q_loss2: -42.56571960449219, p_loss: 0.34651365876197815, mean_rew: -0.10929321626464766, var_rew: 0.04278605803847313, lamda: 1.3400992155075073
Running avgs for agent 1: q_loss: 75.37264251708984, q_loss2: -70.42631530761719, p_loss: 2.5943706035614014, mean_rew: -0.44058512864784494, var_rew: 0.0797916129231453, lamda: 1.337705135345459
steps: 399975, episodes: 16000, mean episode reward: -12.69717115314933, agent episode reward: [-3.297511257337807, -9.399659895811524], time: 29.108
Running avgs for agent 0: q_loss: 98.13638305664062, q_loss2: -78.9514389038086, p_loss: 0.4015490710735321, mean_rew: -0.11160670354857176, var_rew: 0.07641523331403732, lamda: 1.3651106357574463
Running avgs for agent 1: q_loss: 70.30240631103516, q_loss2: -65.0066909790039, p_loss: 2.5982677936553955, mean_rew: -0.4369863721996577, var_rew: 0.040048517286777496, lamda: 1.3627092838287354
steps: 424975, episodes: 17000, mean episode reward: -12.41465229624926, agent episode reward: [-3.1091636759222188, -9.305488620327043], time: 29.155
Running avgs for agent 0: q_loss: 84.45587921142578, q_loss2: -72.15205383300781, p_loss: 0.48370853066444397, mean_rew: -0.11066224158738933, var_rew: 0.017549335956573486, lamda: 1.390114665031433
Running avgs for agent 1: q_loss: 156.75157165527344, q_loss2: -153.4423065185547, p_loss: 2.551370620727539, mean_rew: -0.4296010345496594, var_rew: 0.14525820314884186, lamda: 1.3877134323120117
steps: 449975, episodes: 18000, mean episode reward: -12.112138152554175, agent episode reward: [-3.2483920671346094, -8.863746085419564], time: 29.297
Running avgs for agent 0: q_loss: 69.62835693359375, q_loss2: -56.89780044555664, p_loss: 0.5109381675720215, mean_rew: -0.11426866133838891, var_rew: 0.1255560964345932, lamda: 1.4151188135147095
Running avgs for agent 1: q_loss: 88.25682830810547, q_loss2: -77.38856506347656, p_loss: 2.4827358722686768, mean_rew: -0.4276385141602611, var_rew: 0.05716883763670921, lamda: 1.412717580795288
steps: 474975, episodes: 19000, mean episode reward: -11.680905759657838, agent episode reward: [-2.5318380036177617, -9.149067756040075], time: 29.019
Running avgs for agent 0: q_loss: 77.36534118652344, q_loss2: -60.129093170166016, p_loss: 0.43447357416152954, mean_rew: -0.11238341037309664, var_rew: 0.10259263962507248, lamda: 1.4401229619979858
Running avgs for agent 1: q_loss: 141.4911346435547, q_loss2: -120.77642822265625, p_loss: 2.360399007797241, mean_rew: -0.42427977666210664, var_rew: 0.07468937337398529, lamda: 1.437721610069275
steps: 499975, episodes: 20000, mean episode reward: -11.583947262153666, agent episode reward: [-2.1767299464574155, -9.407217315696249], time: 29.165
Running avgs for agent 0: q_loss: 49.025047302246094, q_loss2: -39.07009506225586, p_loss: 0.4026874899864197, mean_rew: -0.11208401600515915, var_rew: 0.08319693058729172, lamda: 1.4651269912719727
Running avgs for agent 1: q_loss: 97.48434448242188, q_loss2: -86.49586486816406, p_loss: 2.3054115772247314, mean_rew: -0.4205763334090281, var_rew: 0.12451150268316269, lamda: 1.4627258777618408
steps: 524975, episodes: 21000, mean episode reward: -12.02642125251734, agent episode reward: [-2.652197440753711, -9.37422381176363], time: 29.187
Running avgs for agent 0: q_loss: 87.7001953125, q_loss2: -69.14122772216797, p_loss: 0.4243443012237549, mean_rew: -0.11080001722162848, var_rew: 0.050561659038066864, lamda: 1.4901312589645386
Running avgs for agent 1: q_loss: 80.26461029052734, q_loss2: -67.0954818725586, p_loss: 2.2210288047790527, mean_rew: -0.42007354212862097, var_rew: 0.07463710755109787, lamda: 1.4877300262451172
steps: 549975, episodes: 22000, mean episode reward: -12.125640926703236, agent episode reward: [-2.0898067583021547, -10.035834168401081], time: 29.327
Running avgs for agent 0: q_loss: 82.8556900024414, q_loss2: -63.31624221801758, p_loss: 0.48833373188972473, mean_rew: -0.11094712288752794, var_rew: 0.028925711289048195, lamda: 1.5151355266571045
Running avgs for agent 1: q_loss: 92.43150329589844, q_loss2: -80.68851470947266, p_loss: 2.1838266849517822, mean_rew: -0.4187999659220089, var_rew: 0.031946152448654175, lamda: 1.5127341747283936
steps: 574975, episodes: 23000, mean episode reward: -12.330277810791925, agent episode reward: [-2.2198732814338595, -10.110404529358068], time: 29.378
Running avgs for agent 0: q_loss: 66.31592559814453, q_loss2: -55.66630554199219, p_loss: 0.4977101981639862, mean_rew: -0.11014125536837648, var_rew: 0.05414886400103569, lamda: 1.5401394367218018
Running avgs for agent 1: q_loss: 84.01492309570312, q_loss2: -75.5987777709961, p_loss: 2.209479808807373, mean_rew: -0.41786547996093154, var_rew: 0.08388528227806091, lamda: 1.53773832321167
steps: 599975, episodes: 24000, mean episode reward: -12.389916181816556, agent episode reward: [-2.0264395420569894, -10.363476639759567], time: 29.505
Running avgs for agent 0: q_loss: 115.60184478759766, q_loss2: -94.40029907226562, p_loss: 0.4299702048301697, mean_rew: -0.1074743605072008, var_rew: 0.11260908842086792, lamda: 1.5651437044143677
Running avgs for agent 1: q_loss: 94.67776489257812, q_loss2: -92.03091430664062, p_loss: 2.2528469562530518, mean_rew: -0.41749503905799923, var_rew: -0.04397950321435928, lamda: 1.5627424716949463
steps: 624975, episodes: 25000, mean episode reward: -12.860175788510706, agent episode reward: [-2.288967789705776, -10.57120799880493], time: 29.097
Running avgs for agent 0: q_loss: 71.2119140625, q_loss2: -55.29229736328125, p_loss: 0.37936294078826904, mean_rew: -0.10824383028909879, var_rew: 0.040524695068597794, lamda: 1.590147852897644
Running avgs for agent 1: q_loss: 82.7763900756836, q_loss2: -74.24652862548828, p_loss: 2.3370935916900635, mean_rew: -0.41621828319770704, var_rew: -0.003991021309047937, lamda: 1.587746500968933
steps: 649975, episodes: 26000, mean episode reward: -13.067444995997695, agent episode reward: [-2.4073595984643203, -10.660085397533376], time: 29.214
Running avgs for agent 0: q_loss: 57.82444381713867, q_loss2: -46.58203887939453, p_loss: 0.2464314103126526, mean_rew: -0.10676890898547807, var_rew: 0.061126209795475006, lamda: 1.6151518821716309
Running avgs for agent 1: q_loss: 100.22267150878906, q_loss2: -79.81018829345703, p_loss: 2.409369707107544, mean_rew: -0.4168178828046098, var_rew: 0.06011302024126053, lamda: 1.612750768661499
steps: 674975, episodes: 27000, mean episode reward: -12.645445212273472, agent episode reward: [-1.4470797338597454, -11.198365478413725], time: 29.206
Running avgs for agent 0: q_loss: 116.43526458740234, q_loss2: -93.09294891357422, p_loss: 0.19704195857048035, mean_rew: -0.10585608809590952, var_rew: -0.02290988340973854, lamda: 1.6401561498641968
Running avgs for agent 1: q_loss: 84.83821868896484, q_loss2: -72.07197570800781, p_loss: 2.5106167793273926, mean_rew: -0.41746407238493066, var_rew: 0.055945105850696564, lamda: 1.6377549171447754
steps: 699975, episodes: 28000, mean episode reward: -12.62999698234146, agent episode reward: [-1.3278707514216437, -11.302126230919814], time: 29.219
Running avgs for agent 0: q_loss: 65.03672790527344, q_loss2: -51.93864440917969, p_loss: 0.10338116437196732, mean_rew: -0.10308588588359174, var_rew: 0.020914901047945023, lamda: 1.6651604175567627
Running avgs for agent 1: q_loss: 63.187984466552734, q_loss2: -56.20861053466797, p_loss: 2.6116979122161865, mean_rew: -0.41979474514732823, var_rew: 0.002985494676977396, lamda: 1.6627590656280518
steps: 724975, episodes: 29000, mean episode reward: -13.090876606800764, agent episode reward: [-2.246360312524403, -10.844516294276362], time: 29.328
Running avgs for agent 0: q_loss: 74.76113891601562, q_loss2: -62.918148040771484, p_loss: -0.013826859183609486, mean_rew: -0.10356850031468548, var_rew: 0.023911235854029655, lamda: 1.69016432762146
Running avgs for agent 1: q_loss: 71.83240509033203, q_loss2: -60.94960021972656, p_loss: 2.688751220703125, mean_rew: -0.42041744420435795, var_rew: 0.03367457166314125, lamda: 1.6877632141113281
steps: 749975, episodes: 30000, mean episode reward: -12.872277828582371, agent episode reward: [-2.5423292484689517, -10.329948580113422], time: 29.33
Running avgs for agent 0: q_loss: 90.66343688964844, q_loss2: -67.22752380371094, p_loss: 0.1068897545337677, mean_rew: -0.10310935455506073, var_rew: 0.017717070877552032, lamda: 1.7151685953140259
Running avgs for agent 1: q_loss: 56.95856857299805, q_loss2: -45.03325653076172, p_loss: 2.7803523540496826, mean_rew: -0.42045445384028507, var_rew: 0.06362301856279373, lamda: 1.7127673625946045
steps: 774975, episodes: 31000, mean episode reward: -13.249038628615358, agent episode reward: [-2.4973219163162947, -10.75171671229906], time: 29.116
Running avgs for agent 0: q_loss: 72.6389389038086, q_loss2: -55.10982131958008, p_loss: 0.09270819276571274, mean_rew: -0.10398008174481516, var_rew: 0.004690697882324457, lamda: 1.7401727437973022
Running avgs for agent 1: q_loss: 60.035762786865234, q_loss2: -51.660118103027344, p_loss: 2.8586626052856445, mean_rew: -0.4206617637438934, var_rew: 0.09111236035823822, lamda: 1.7377713918685913
steps: 799975, episodes: 32000, mean episode reward: -13.454012553242668, agent episode reward: [-2.9057991417354216, -10.548213411507247], time: 29.326
Running avgs for agent 0: q_loss: 65.7515640258789, q_loss2: -52.67631912231445, p_loss: 0.10598348826169968, mean_rew: -0.10239028767214645, var_rew: 0.043732307851314545, lamda: 1.765176773071289
Running avgs for agent 1: q_loss: 62.88806915283203, q_loss2: -55.613773345947266, p_loss: 2.9005675315856934, mean_rew: -0.4224925536161263, var_rew: 0.12202900648117065, lamda: 1.7627756595611572
steps: 824975, episodes: 33000, mean episode reward: -14.021777359711534, agent episode reward: [-2.836420324914394, -11.18535703479714], time: 29.3
Running avgs for agent 0: q_loss: 102.8309555053711, q_loss2: -85.96075439453125, p_loss: 0.2186768501996994, mean_rew: -0.1036843796683596, var_rew: 0.051307931542396545, lamda: 1.790181040763855
Running avgs for agent 1: q_loss: 56.59075927734375, q_loss2: -47.91596603393555, p_loss: 2.9415385723114014, mean_rew: -0.42118984221751826, var_rew: 0.06925910711288452, lamda: 1.7877798080444336
steps: 849975, episodes: 34000, mean episode reward: -13.913917791604552, agent episode reward: [-2.5738877610342623, -11.340030030570288], time: 29.687
Running avgs for agent 0: q_loss: 53.99464797973633, q_loss2: -43.516788482666016, p_loss: 0.3061933219432831, mean_rew: -0.10319921212248521, var_rew: 0.05453373119235039, lamda: 1.8151851892471313
Running avgs for agent 1: q_loss: 62.86394119262695, q_loss2: -55.017417907714844, p_loss: 3.0125653743743896, mean_rew: -0.42094036642974414, var_rew: 0.01258376520127058, lamda: 1.8127838373184204
steps: 874975, episodes: 35000, mean episode reward: -14.81146652577379, agent episode reward: [-3.5175393649678344, -11.293927160805957], time: 29.397
Running avgs for agent 0: q_loss: 55.473045349121094, q_loss2: -43.748634338378906, p_loss: 0.4061433970928192, mean_rew: -0.10228149262080549, var_rew: 0.028959175571799278, lamda: 1.8401892185211182
Running avgs for agent 1: q_loss: 77.00135040283203, q_loss2: -64.5835189819336, p_loss: 3.0585052967071533, mean_rew: -0.4239537070591327, var_rew: 0.09324055910110474, lamda: 1.8377881050109863
steps: 899975, episodes: 36000, mean episode reward: -14.99931494344645, agent episode reward: [-4.219335013877746, -10.779979929568702], time: 29.418
Running avgs for agent 0: q_loss: 82.30525970458984, q_loss2: -65.81111907958984, p_loss: 0.5171451568603516, mean_rew: -0.10604503187174931, var_rew: 0.058621179312467575, lamda: 1.865193486213684
Running avgs for agent 1: q_loss: 70.64032745361328, q_loss2: -56.43256378173828, p_loss: 3.071229934692383, mean_rew: -0.42251442575142295, var_rew: 0.10145147144794464, lamda: 1.8627922534942627
steps: 924975, episodes: 37000, mean episode reward: -14.801530759803764, agent episode reward: [-4.236521814685325, -10.56500894511844], time: 29.652
Running avgs for agent 0: q_loss: 106.5020980834961, q_loss2: -82.38934326171875, p_loss: 0.5464818477630615, mean_rew: -0.10572230512362173, var_rew: 0.07813610881567001, lamda: 1.8901976346969604
Running avgs for agent 1: q_loss: 72.42723083496094, q_loss2: -58.49128723144531, p_loss: 3.1211283206939697, mean_rew: -0.4232991870713743, var_rew: 0.08138421177864075, lamda: 1.8877962827682495
steps: 949975, episodes: 38000, mean episode reward: -14.666061321262482, agent episode reward: [-3.975111369253306, -10.690949952009174], time: 29.78
Running avgs for agent 0: q_loss: 96.26057434082031, q_loss2: -76.62905883789062, p_loss: 0.6991947293281555, mean_rew: -0.1076778215470732, var_rew: 0.06150863319635391, lamda: 1.9152016639709473
Running avgs for agent 1: q_loss: 99.68225860595703, q_loss2: -83.57686614990234, p_loss: 3.2012016773223877, mean_rew: -0.4245329194474079, var_rew: 0.020255357027053833, lamda: 1.9128004312515259
steps: 974975, episodes: 39000, mean episode reward: -15.481899975854656, agent episode reward: [-4.965660680431923, -10.516239295422734], time: 29.45
Running avgs for agent 0: q_loss: 64.48709106445312, q_loss2: -52.000858306884766, p_loss: 0.798059344291687, mean_rew: -0.11028345535030715, var_rew: 0.07519301027059555, lamda: 1.9402059316635132
Running avgs for agent 1: q_loss: 89.86829376220703, q_loss2: -72.45528411865234, p_loss: 3.236192464828491, mean_rew: -0.42394788331033145, var_rew: 0.005491653457283974, lamda: 1.9378046989440918
steps: 999975, episodes: 40000, mean episode reward: -15.871674826421787, agent episode reward: [-5.639796400260987, -10.231878426160801], time: 29.337
Running avgs for agent 0: q_loss: 131.4695587158203, q_loss2: -117.256591796875, p_loss: 0.8712852001190186, mean_rew: -0.11206214743821477, var_rew: 0.021773600950837135, lamda: 1.9652100801467896
Running avgs for agent 1: q_loss: 118.706298828125, q_loss2: -99.99502563476562, p_loss: 3.2665457725524902, mean_rew: -0.4232327737339914, var_rew: 0.02069992385804653, lamda: 1.9628088474273682
steps: 1024975, episodes: 41000, mean episode reward: -15.925753472025386, agent episode reward: [-5.6055117834072625, -10.320241688618124], time: 30.145
Running avgs for agent 0: q_loss: 92.66607666015625, q_loss2: -73.51728820800781, p_loss: 0.9455836415290833, mean_rew: -0.11711919481142156, var_rew: 0.09904603660106659, lamda: 1.9902141094207764
Running avgs for agent 1: q_loss: 57.4644775390625, q_loss2: -46.09185791015625, p_loss: 3.2657294273376465, mean_rew: -0.4157254112570562, var_rew: 0.043682437390089035, lamda: 1.9878129959106445
steps: 1049975, episodes: 42000, mean episode reward: -15.941339083280713, agent episode reward: [-6.664294320038232, -9.27704476324248], time: 30.624
Running avgs for agent 0: q_loss: 19.60944175720215, q_loss2: -17.49907112121582, p_loss: 1.0764153003692627, mean_rew: -0.11933792340950213, var_rew: 0.0463399738073349, lamda: 2.01520037651062
Running avgs for agent 1: q_loss: 27.033430099487305, q_loss2: -26.224281311035156, p_loss: 3.1702797412872314, mean_rew: -0.3984036091167341, var_rew: 0.04665294662117958, lamda: 2.0128018856048584
steps: 1074975, episodes: 43000, mean episode reward: -15.566353355758965, agent episode reward: [-6.382531466119425, -9.183821889639539], time: 30.206
Running avgs for agent 0: q_loss: 5.158618927001953, q_loss2: -4.558870792388916, p_loss: 1.2158342599868774, mean_rew: -0.12057675580326492, var_rew: 0.04878006875514984, lamda: 2.0401744842529297
Running avgs for agent 1: q_loss: 7.132354259490967, q_loss2: -6.309218406677246, p_loss: 3.1599721908569336, mean_rew: -0.39683127108101474, var_rew: 0.039612654596567154, lamda: 2.037776231765747
steps: 1099975, episodes: 44000, mean episode reward: -15.269876700384197, agent episode reward: [-6.116361730839807, -9.153514969544391], time: 29.861
Running avgs for agent 0: q_loss: 5.120152473449707, q_loss2: -4.616882801055908, p_loss: 1.3484636545181274, mean_rew: -0.12278895709871926, var_rew: 0.04228170961141586, lamda: 2.0651488304138184
Running avgs for agent 1: q_loss: 6.715897560119629, q_loss2: -5.9337568283081055, p_loss: 3.170231819152832, mean_rew: -0.3992479860836036, var_rew: 0.04377549886703491, lamda: 2.0627505779266357
steps: 1124975, episodes: 45000, mean episode reward: -14.620306644365282, agent episode reward: [-5.585591654729347, -9.034714989635937], time: 29.831
Running avgs for agent 0: q_loss: 5.4868035316467285, q_loss2: -4.971254825592041, p_loss: 1.4678874015808105, mean_rew: -0.1250436604827841, var_rew: 0.03634228557348251, lamda: 2.090123176574707
Running avgs for agent 1: q_loss: 6.877094745635986, q_loss2: -6.143519401550293, p_loss: 3.173987627029419, mean_rew: -0.3989837936971718, var_rew: 0.05417591705918312, lamda: 2.0877249240875244
steps: 1149975, episodes: 46000, mean episode reward: -14.603692521776782, agent episode reward: [-5.29203902222865, -9.311653499548134], time: 29.852
Running avgs for agent 0: q_loss: 5.911518573760986, q_loss2: -5.386658668518066, p_loss: 1.561966061592102, mean_rew: -0.12945663878746064, var_rew: 0.03396782651543617, lamda: 2.115097761154175
Running avgs for agent 1: q_loss: 7.702096939086914, q_loss2: -6.723494529724121, p_loss: 3.2157585620880127, mean_rew: -0.40070461289268533, var_rew: 0.03381500393152237, lamda: 2.112699270248413
^[[Bsteps: 1174975, episodes: 47000, mean episode reward: -14.933063400795179, agent episode reward: [-5.292890508837114, -9.640172891958068], time: 30.344
Running avgs for agent 0: q_loss: 6.205743789672852, q_loss2: -5.609470844268799, p_loss: 1.6347978115081787, mean_rew: -0.1317671099028025, var_rew: 0.04014742374420166, lamda: 2.1400721073150635
Running avgs for agent 1: q_loss: 7.5878214836120605, q_loss2: -6.747330188751221, p_loss: 3.2413721084594727, mean_rew: -0.400823986498027, var_rew: 0.046855948865413666, lamda: 2.1376736164093018
steps: 1199975, episodes: 48000, mean episode reward: -14.785366360269757, agent episode reward: [-5.046413330548926, -9.738953029720829], time: 30.379
Running avgs for agent 0: q_loss: 6.472139835357666, q_loss2: -5.861575603485107, p_loss: 1.6730495691299438, mean_rew: -0.1346961237395036, var_rew: 0.04810788854956627, lamda: 2.165046453475952
Running avgs for agent 1: q_loss: 8.091700553894043, q_loss2: -7.203057765960693, p_loss: 3.289623975753784, mean_rew: -0.40328351712703425, var_rew: 0.0475589781999588, lamda: 2.1626479625701904
steps: 1224975, episodes: 49000, mean episode reward: -14.533856396406016, agent episode reward: [-5.117748905777063, -9.416107490628955], time: 30.058
Running avgs for agent 0: q_loss: 6.659541606903076, q_loss2: -6.049792289733887, p_loss: 1.7062609195709229, mean_rew: -0.1382201680120173, var_rew: 0.03944088891148567, lamda: 2.190020799636841
Running avgs for agent 1: q_loss: 7.9052934646606445, q_loss2: -6.986515522003174, p_loss: 3.3183164596557617, mean_rew: -0.4023695241345297, var_rew: 0.05143854022026062, lamda: 2.187622308731079
steps: 1249975, episodes: 50000, mean episode reward: -14.17991705236588, agent episode reward: [-4.731992764353727, -9.447924288012153], time: 29.931
Running avgs for agent 0: q_loss: 6.736087799072266, q_loss2: -6.112545013427734, p_loss: 1.7216532230377197, mean_rew: -0.1404588575472336, var_rew: 0.05736790597438812, lamda: 2.2149951457977295
Running avgs for agent 1: q_loss: 8.161965370178223, q_loss2: -7.223156452178955, p_loss: 3.3488852977752686, mean_rew: -0.40314899432354134, var_rew: 0.05686074495315552, lamda: 2.2125966548919678
steps: 1274975, episodes: 51000, mean episode reward: -14.1218109797734, agent episode reward: [-4.585902678532999, -9.5359083012404], time: 30.013
Running avgs for agent 0: q_loss: 7.217339992523193, q_loss2: -6.511805057525635, p_loss: 1.7337455749511719, mean_rew: -0.14480347148445774, var_rew: 0.05182715505361557, lamda: 2.239969253540039
Running avgs for agent 1: q_loss: 8.737874984741211, q_loss2: -7.692986011505127, p_loss: 3.360041856765747, mean_rew: -0.40284906422553796, var_rew: 0.04093015566468239, lamda: 2.2375710010528564
steps: 1299975, episodes: 52000, mean episode reward: -13.211967574206515, agent episode reward: [-4.163709692249097, -9.048257881957419], time: 29.784
Running avgs for agent 0: q_loss: 6.712343692779541, q_loss2: -6.045853614807129, p_loss: 1.7507234811782837, mean_rew: -0.14724224015690066, var_rew: 0.046184614300727844, lamda: 2.264943838119507
Running avgs for agent 1: q_loss: 8.527650833129883, q_loss2: -7.528331756591797, p_loss: 3.3462655544281006, mean_rew: -0.40170111543868936, var_rew: 0.03929542377591133, lamda: 2.262545347213745
steps: 1324975, episodes: 53000, mean episode reward: -12.8666642057643, agent episode reward: [-3.9507393631968375, -8.915924842567463], time: 29.855
Running avgs for agent 0: q_loss: 6.85231351852417, q_loss2: -6.151660442352295, p_loss: 1.7566221952438354, mean_rew: -0.1488106050005434, var_rew: 0.05589083954691887, lamda: 2.2899181842803955
Running avgs for agent 1: q_loss: 8.301203727722168, q_loss2: -7.471074104309082, p_loss: 3.3284378051757812, mean_rew: -0.3991692304575981, var_rew: 0.041421882808208466, lamda: 2.2875194549560547
steps: 1349975, episodes: 54000, mean episode reward: -12.333639999842427, agent episode reward: [-3.7198193285427097, -8.61382067129972], time: 29.507
Running avgs for agent 0: q_loss: 6.850957870483398, q_loss2: -6.175820350646973, p_loss: 1.7675799131393433, mean_rew: -0.14925284215075782, var_rew: 0.04264426603913307, lamda: 2.314892292022705
Running avgs for agent 1: q_loss: 8.764509201049805, q_loss2: -7.730443477630615, p_loss: 3.3073112964630127, mean_rew: -0.39830570669644555, var_rew: 0.06021277606487274, lamda: 2.3124940395355225
steps: 1374975, episodes: 55000, mean episode reward: -12.128651140678052, agent episode reward: [-3.566058254268185, -8.562592886409869], time: 29.789
Running avgs for agent 0: q_loss: 7.6395955085754395, q_loss2: -6.861753940582275, p_loss: 1.792224645614624, mean_rew: -0.15061963093755862, var_rew: 0.05558660998940468, lamda: 2.339866876602173
Running avgs for agent 1: q_loss: 8.379003524780273, q_loss2: -7.427220344543457, p_loss: 3.2826504707336426, mean_rew: -0.39836006792767387, var_rew: 0.03831668943166733, lamda: 2.337468147277832
steps: 1399975, episodes: 56000, mean episode reward: -11.935215119552032, agent episode reward: [-3.093662593161593, -8.841552526390439], time: 29.635
Running avgs for agent 0: q_loss: 7.5281829833984375, q_loss2: -6.740353107452393, p_loss: 1.8146109580993652, mean_rew: -0.14924625938295463, var_rew: 0.049759238958358765, lamda: 2.3648412227630615
Running avgs for agent 1: q_loss: 9.749617576599121, q_loss2: -8.627254486083984, p_loss: 3.2414052486419678, mean_rew: -0.3977517185548463, var_rew: 0.05276910215616226, lamda: 2.362442970275879
steps: 1424975, episodes: 57000, mean episode reward: -11.88819583293942, agent episode reward: [-3.3146916464776255, -8.573504186461793], time: 30.306
Running avgs for agent 0: q_loss: 8.039719581604004, q_loss2: -7.222890853881836, p_loss: 1.8461055755615234, mean_rew: -0.14779640245313494, var_rew: 0.044727858155965805, lamda: 2.389815330505371
Running avgs for agent 1: q_loss: 9.271454811096191, q_loss2: -8.216072082519531, p_loss: 3.206064462661743, mean_rew: -0.39876632093087794, var_rew: 0.037678804248571396, lamda: 2.3874170780181885
steps: 1449975, episodes: 58000, mean episode reward: -11.799183913013232, agent episode reward: [-2.914111807672831, -8.8850721053404], time: 30.525
Running avgs for agent 0: q_loss: 8.023045539855957, q_loss2: -7.200406074523926, p_loss: 1.871606469154358, mean_rew: -0.14976301065928246, var_rew: 0.04431619122624397, lamda: 2.414790153503418
Running avgs for agent 1: q_loss: 10.031229019165039, q_loss2: -8.950066566467285, p_loss: 3.147125005722046, mean_rew: -0.39695718065605035, var_rew: 0.05373844876885414, lamda: 2.412391424179077
steps: 1474975, episodes: 59000, mean episode reward: -11.465964726439466, agent episode reward: [-2.823486888401641, -8.642477838037827], time: 30.093
Running avgs for agent 0: q_loss: 7.922424793243408, q_loss2: -7.130762577056885, p_loss: 1.8575327396392822, mean_rew: -0.14873811675029414, var_rew: 0.05674229934811592, lamda: 2.4397642612457275
Running avgs for agent 1: q_loss: 10.426142692565918, q_loss2: -9.09752368927002, p_loss: 3.0915791988372803, mean_rew: -0.3969138312297338, var_rew: 0.0496133454144001, lamda: 2.437365770339966
steps: 1499975, episodes: 60000, mean episode reward: -11.679335323816499, agent episode reward: [-2.5018460139052254, -9.177489309911273], time: 29.555
Running avgs for agent 0: q_loss: 8.140700340270996, q_loss2: -7.349964618682861, p_loss: 1.8339985609054565, mean_rew: -0.14841382210572385, var_rew: 0.046667374670505524, lamda: 2.464738607406616
Running avgs for agent 1: q_loss: 11.8656005859375, q_loss2: -10.373993873596191, p_loss: 3.0291037559509277, mean_rew: -0.39656681860955695, var_rew: 0.04571547731757164, lamda: 2.4623401165008545
Traceback (most recent call last):
  File "train.py", line 237, in <module>
    train(arglist)
  File "train.py", line 226, in train

