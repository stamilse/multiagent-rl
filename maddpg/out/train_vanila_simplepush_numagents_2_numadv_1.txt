python train_vanila.py --scenario simple_push --num-adversaries 1 
2018-10-27 18:42:25.446104: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -26.366741058621205, agent episode reward: [1.2436236894113066, -27.610364748032513], time: 18.621
/anaconda2/envs/py36/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/anaconda2/envs/py36/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, var_rew: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, var_rew: nan
steps: 49975, episodes: 2000, mean episode reward: -21.1012389943877, agent episode reward: [-4.427844294821421, -16.673394699566277], time: 28.767
Running avgs for agent 0: q_loss: 0.026784373447299004, p_loss: -0.10645344108343124, mean_rew: 0.005272561683414244, var_rew: 0.5579937036832788
Running avgs for agent 1: q_loss: 0.03621755912899971, p_loss: 1.6727737188339233, mean_rew: -1.0540859485682745, var_rew: 0.4397616840629427
steps: 74975, episodes: 3000, mean episode reward: -10.786398643595927, agent episode reward: [-3.935371212478964, -6.851027431116964], time: 28.214
Running avgs for agent 0: q_loss: 0.016491230577230453, p_loss: 0.126361683011055, mean_rew: -0.08451412671736162, var_rew: 0.4270700441524909
Running avgs for agent 1: q_loss: 0.004573238547891378, p_loss: 2.286156177520752, mean_rew: -0.7704268761084185, var_rew: 0.4732315865517387
steps: 99975, episodes: 4000, mean episode reward: -9.543228523233232, agent episode reward: [-2.740418503613048, -6.802810019620185], time: 28.853
Running avgs for agent 0: q_loss: 0.031745001673698425, p_loss: 0.3203635513782501, mean_rew: -0.09776930802832101, var_rew: 0.3406209555203675
Running avgs for agent 1: q_loss: 0.004723051097244024, p_loss: 2.530327558517456, mean_rew: -0.623104384025899, var_rew: 0.43483864765406605
steps: 124975, episodes: 5000, mean episode reward: -9.474407035739889, agent episode reward: [-2.34226128596319, -7.132145749776698], time: 28.337
Running avgs for agent 0: q_loss: 0.03483320772647858, p_loss: 0.4553575813770294, mean_rew: -0.09855725633645855, var_rew: 0.292131410214801
Running avgs for agent 1: q_loss: 0.007466866169124842, p_loss: 2.5715627670288086, mean_rew: -0.547240266328901, var_rew: 0.4014117595627079
steps: 149975, episodes: 6000, mean episode reward: -9.186676964540252, agent episode reward: [-2.4280024309435655, -6.758674533596686], time: 28.496
Running avgs for agent 0: q_loss: 0.03557892516255379, p_loss: 0.5612055063247681, mean_rew: -0.0962099689062137, var_rew: 0.2612016958060106
Running avgs for agent 1: q_loss: 0.013268250972032547, p_loss: 2.5084095001220703, mean_rew: -0.49910061209714307, var_rew: 0.37445029385474904
steps: 174975, episodes: 7000, mean episode reward: -9.219297820785531, agent episode reward: [-2.146266552370433, -7.073031268415098], time: 28.551
Running avgs for agent 0: q_loss: 0.03581496328115463, p_loss: 0.6438243389129639, mean_rew: -0.09655772053130013, var_rew: 0.2361157450246513
Running avgs for agent 1: q_loss: 0.004701128229498863, p_loss: 2.419891595840454, mean_rew: -0.46485332348448666, var_rew: 0.3492476463851416
steps: 199975, episodes: 8000, mean episode reward: -9.296955998245135, agent episode reward: [-2.503115533514747, -6.7938404647303905], time: 29.42
Running avgs for agent 0: q_loss: 0.04145912826061249, p_loss: 0.711628794670105, mean_rew: -0.09634748819931672, var_rew: 0.22329570478695282
Running avgs for agent 1: q_loss: 0.005914940964430571, p_loss: 2.335689067840576, mean_rew: -0.43937692009590296, var_rew: 0.3315925706731429
steps: 224975, episodes: 9000, mean episode reward: -9.125508536853205, agent episode reward: [-2.6243393980703864, -6.50116913878282], time: 28.984
Running avgs for agent 0: q_loss: 0.03585013002157211, p_loss: 0.7525486946105957, mean_rew: -0.09717739469012121, var_rew: 0.21134258109183618
Running avgs for agent 1: q_loss: 0.007033473812043667, p_loss: 2.2596189975738525, mean_rew: -0.41943156088031164, var_rew: 0.3172428286718241
steps: 249975, episodes: 10000, mean episode reward: -8.962085758679278, agent episode reward: [-2.258273022532792, -6.703812736146485], time: 28.285
Running avgs for agent 0: q_loss: 0.03788500279188156, p_loss: 0.7803788781166077, mean_rew: -0.09691910722270888, var_rew: 0.20339052245875486
Running avgs for agent 1: q_loss: 0.004788857884705067, p_loss: 2.1783857345581055, mean_rew: -0.40146043920364227, var_rew: 0.30095548080797563
steps: 274975, episodes: 11000, mean episode reward: -9.059311121940135, agent episode reward: [-2.2742580106846937, -6.785053111255442], time: 28.973
Running avgs for agent 0: q_loss: 0.039486438035964966, p_loss: 0.7954022884368896, mean_rew: -0.0965164270462809, var_rew: 0.19494090932669808
Running avgs for agent 1: q_loss: 0.004235685802996159, p_loss: 2.1262736320495605, mean_rew: -0.3897452094805507, var_rew: 0.29000348800528036
steps: 299975, episodes: 12000, mean episode reward: -8.915542498124443, agent episode reward: [-2.0965276300363946, -6.819014868088049], time: 28.748
Running avgs for agent 0: q_loss: 0.03677225485444069, p_loss: 0.811286449432373, mean_rew: -0.09585655861638594, var_rew: 0.18441057447360112
Running avgs for agent 1: q_loss: 0.009596460498869419, p_loss: 2.09122371673584, mean_rew: -0.38035022216999326, var_rew: 0.2841545231538573
steps: 324975, episodes: 13000, mean episode reward: -9.07442925650932, agent episode reward: [-2.2423267838696677, -6.8321024726396535], time: 28.437
Running avgs for agent 0: q_loss: 0.036810535937547684, p_loss: 0.8158950209617615, mean_rew: -0.09489405528965308, var_rew: 0.1821828950977885
Running avgs for agent 1: q_loss: 0.015468195080757141, p_loss: 2.04494047164917, mean_rew: -0.37203380131816943, var_rew: 0.2743783531228082
steps: 349975, episodes: 14000, mean episode reward: -8.956305417963817, agent episode reward: [-2.1977730553384163, -6.7585323626254], time: 28.41
Running avgs for agent 0: q_loss: 0.03315017744898796, p_loss: 0.820608377456665, mean_rew: -0.09501348489848821, var_rew: 0.1748314246635865
Running avgs for agent 1: q_loss: 0.004865784663707018, p_loss: 2.003645420074463, mean_rew: -0.3623645516676288, var_rew: 0.266896359260208
steps: 374975, episodes: 15000, mean episode reward: -8.817583547360204, agent episode reward: [-2.115277760176756, -6.702305787183447], time: 28.326
Running avgs for agent 0: q_loss: 0.03904798999428749, p_loss: 0.812645435333252, mean_rew: -0.09401094615569705, var_rew: 0.17080097586426965
Running avgs for agent 1: q_loss: 0.0064615048468112946, p_loss: 1.9817191362380981, mean_rew: -0.35733186714364273, var_rew: 0.26038946439023253
steps: 399975, episodes: 16000, mean episode reward: -8.605495969237596, agent episode reward: [-1.8807975758615363, -6.72469839337606], time: 28.381
Running avgs for agent 0: q_loss: 0.040244121104478836, p_loss: 0.8131775856018066, mean_rew: -0.09324416967708216, var_rew: 0.16777569313000407
Running avgs for agent 1: q_loss: 0.00955386832356453, p_loss: 1.954058051109314, mean_rew: -0.35100216021600433, var_rew: 0.2551092924104712
steps: 424975, episodes: 17000, mean episode reward: -8.684825971558942, agent episode reward: [-2.3258832810860435, -6.358942690472897], time: 28.434
Running avgs for agent 0: q_loss: 0.0376155860722065, p_loss: 0.7995222210884094, mean_rew: -0.09179395142623281, var_rew: 0.1643702782940786
Running avgs for agent 1: q_loss: 0.005947509780526161, p_loss: 1.9364951848983765, mean_rew: -0.3462038336191444, var_rew: 0.25248264084117295
steps: 449975, episodes: 18000, mean episode reward: -8.710576123154892, agent episode reward: [-1.9774368059334646, -6.7331393172214264], time: 28.691
Running avgs for agent 0: q_loss: 0.03457086905837059, p_loss: 0.8120707273483276, mean_rew: -0.09313527884844384, var_rew: 0.16105364583656445
Running avgs for agent 1: q_loss: 0.008493415080010891, p_loss: 1.9117625951766968, mean_rew: -0.3406948847992037, var_rew: 0.2461996623026135
steps: 474975, episodes: 19000, mean episode reward: -8.868648617456198, agent episode reward: [-2.108499308548264, -6.760149308907936], time: 28.381
Running avgs for agent 0: q_loss: 0.036069612950086594, p_loss: 0.8075506687164307, mean_rew: -0.09283310723803286, var_rew: 0.15806387646610062
Running avgs for agent 1: q_loss: 0.010322482325136662, p_loss: 1.906090497970581, mean_rew: -0.3388287225932322, var_rew: 0.24497878740759382
steps: 499975, episodes: 20000, mean episode reward: -8.919662994747382, agent episode reward: [-2.1265246799071473, -6.793138314840233], time: 28.187
Running avgs for agent 0: q_loss: 0.040897436439991, p_loss: 0.7964463829994202, mean_rew: -0.09064001784785863, var_rew: 0.1589372617643529
Running avgs for agent 1: q_loss: 0.006205383688211441, p_loss: 1.874560832977295, mean_rew: -0.33324399304531493, var_rew: 0.23864513701400136
steps: 524975, episodes: 21000, mean episode reward: -8.82542046148855, agent episode reward: [-2.3003277475484043, -6.525092713940147], time: 28.309
Running avgs for agent 0: q_loss: 0.03260045498609543, p_loss: 0.7929163575172424, mean_rew: -0.09144996573778576, var_rew: 0.15527795320781143
Running avgs for agent 1: q_loss: 0.006299424916505814, p_loss: 1.8728302717208862, mean_rew: -0.33174108983746786, var_rew: 0.23671252653078712
steps: 549975, episodes: 22000, mean episode reward: -8.765024625005532, agent episode reward: [-2.1343143754842506, -6.630710249521282], time: 28.731
Running avgs for agent 0: q_loss: 0.036103256046772, p_loss: 0.7911292314529419, mean_rew: -0.09140102920581206, var_rew: 0.15299021750424432
Running avgs for agent 1: q_loss: 0.005758272018283606, p_loss: 1.8659818172454834, mean_rew: -0.3285157488169105, var_rew: 0.23568381904943286
steps: 574975, episodes: 23000, mean episode reward: -8.634030173124739, agent episode reward: [-2.022101523698423, -6.611928649426315], time: 29.281
Running avgs for agent 0: q_loss: 0.0320044681429863, p_loss: 0.7862631678581238, mean_rew: -0.09105872001199919, var_rew: 0.15112748071314194
Running avgs for agent 1: q_loss: 0.0037791032809764147, p_loss: 1.845812201499939, mean_rew: -0.32492030564616703, var_rew: 0.23028592200211345
steps: 599975, episodes: 24000, mean episode reward: -8.563157519206495, agent episode reward: [-1.882700858098831, -6.680456661107665], time: 28.855
Running avgs for agent 0: q_loss: 0.04235551506280899, p_loss: 0.7739731669425964, mean_rew: -0.08904106939499318, var_rew: 0.1519775744673264
Running avgs for agent 1: q_loss: 0.017138095572590828, p_loss: 1.8361366987228394, mean_rew: -0.32287370130519605, var_rew: 0.22855877557753593
steps: 624975, episodes: 25000, mean episode reward: -8.821277502486573, agent episode reward: [-2.0564910192250414, -6.764786483261533], time: 28.681
Running avgs for agent 0: q_loss: 0.03451577201485634, p_loss: 0.7826903462409973, mean_rew: -0.09171664530462117, var_rew: 0.14843088053738554
Running avgs for agent 1: q_loss: 0.01019494328647852, p_loss: 1.8050024509429932, mean_rew: -0.3210221706797629, var_rew: 0.22558383188306808
steps: 649975, episodes: 26000, mean episode reward: -8.831627003250098, agent episode reward: [-2.0759074472282926, -6.7557195560218055], time: 28.664
Running avgs for agent 0: q_loss: 0.03372916579246521, p_loss: 0.7555460333824158, mean_rew: -0.08870891854811014, var_rew: 0.14811015744470665
Running avgs for agent 1: q_loss: 0.007539344951510429, p_loss: 1.7811174392700195, mean_rew: -0.3161496104109467, var_rew: 0.22197018799144158
steps: 674975, episodes: 27000, mean episode reward: -8.594701263132054, agent episode reward: [-1.870171606612678, -6.724529656519376], time: 28.207
Running avgs for agent 0: q_loss: 0.03558062016963959, p_loss: 0.7489439845085144, mean_rew: -0.09083858222353992, var_rew: 0.14650222146791103
Running avgs for agent 1: q_loss: 0.011081954464316368, p_loss: 1.7858073711395264, mean_rew: -0.3166589373667671, var_rew: 0.22138074879663674
steps: 699975, episodes: 28000, mean episode reward: -8.427901812868553, agent episode reward: [-1.7052192589186945, -6.7226825539498565], time: 28.326
Running avgs for agent 0: q_loss: 0.04156697914004326, p_loss: 0.7301506996154785, mean_rew: -0.08843729614496494, var_rew: 0.14691268013197561
Running avgs for agent 1: q_loss: 0.0036796408239752054, p_loss: 1.780839204788208, mean_rew: -0.3152544982669172, var_rew: 0.21933458968078703
steps: 724975, episodes: 29000, mean episode reward: -8.67413137871005, agent episode reward: [-1.9275690787757132, -6.746562299934336], time: 28.271
Running avgs for agent 0: q_loss: 0.03792060166597366, p_loss: 0.702328085899353, mean_rew: -0.08769023519946577, var_rew: 0.14473459420401938
Running avgs for agent 1: q_loss: 0.00455046072602272, p_loss: 1.7763586044311523, mean_rew: -0.31383597326508517, var_rew: 0.21567827865055605
steps: 749975, episodes: 30000, mean episode reward: -8.744170128230108, agent episode reward: [-2.0342745679693794, -6.709895560260727], time: 28.307
Running avgs for agent 0: q_loss: 0.035631366074085236, p_loss: 0.6749971508979797, mean_rew: -0.0870503960778152, var_rew: 0.14422621837147395
Running avgs for agent 1: q_loss: 0.008934743702411652, p_loss: 1.7756801843643188, mean_rew: -0.3109961213605395, var_rew: 0.2150523530452174
steps: 774975, episodes: 31000, mean episode reward: -8.918152089248476, agent episode reward: [-1.7379910983787121, -7.180160990869765], time: 27.69
Running avgs for agent 0: q_loss: 0.0340568944811821, p_loss: 0.6562483906745911, mean_rew: -0.08729055739075585, var_rew: 0.14415370280493667
Running avgs for agent 1: q_loss: 0.013316160999238491, p_loss: 1.7699638605117798, mean_rew: -0.3092600578030448, var_rew: 0.21377369809652538
steps: 799975, episodes: 32000, mean episode reward: -8.76358354337218, agent episode reward: [-1.6206128582343544, -7.142970685137828], time: 27.665
Running avgs for agent 0: q_loss: 0.03533441200852394, p_loss: 0.641578733921051, mean_rew: -0.08688470920186722, var_rew: 0.1424274580590615
Running avgs for agent 1: q_loss: 0.009572315029799938, p_loss: 1.763904094696045, mean_rew: -0.30879467800497445, var_rew: 0.21161410227371355
steps: 824975, episodes: 33000, mean episode reward: -8.492503898625849, agent episode reward: [-0.9852640796936999, -7.507239818932148], time: 27.607
Running avgs for agent 0: q_loss: 0.039986517280340195, p_loss: 0.6118860840797424, mean_rew: -0.08655508966840057, var_rew: 0.14200210750074427
Running avgs for agent 1: q_loss: 0.004785978235304356, p_loss: 1.7613054513931274, mean_rew: -0.30834879319515607, var_rew: 0.21003184859441426
steps: 849975, episodes: 34000, mean episode reward: -8.532351182919232, agent episode reward: [-0.776896769485365, -7.755454413433868], time: 28.172
Running avgs for agent 0: q_loss: 0.03979695960879326, p_loss: 0.5339482426643372, mean_rew: -0.08361787814833661, var_rew: 0.1412452611971154
Running avgs for agent 1: q_loss: 0.005301442928612232, p_loss: 1.7727950811386108, mean_rew: -0.3103998141092048, var_rew: 0.21008936041104442
steps: 874975, episodes: 35000, mean episode reward: -8.719657961079863, agent episode reward: [-1.0890632009340966, -7.630594760145764], time: 28.436
Running avgs for agent 0: q_loss: 0.04412480816245079, p_loss: 0.4469664990901947, mean_rew: -0.08366419696137721, var_rew: 0.14121979423356576
Running avgs for agent 1: q_loss: 0.005453598219901323, p_loss: 1.7612816095352173, mean_rew: -0.3083886199217127, var_rew: 0.20694681125678918
steps: 899975, episodes: 36000, mean episode reward: -8.47737184027306, agent episode reward: [-0.4692478738647333, -8.008123966408325], time: 27.743
Running avgs for agent 0: q_loss: 0.04016651585698128, p_loss: 0.352015882730484, mean_rew: -0.08238334034594885, var_rew: 0.13982781351873153
Running avgs for agent 1: q_loss: 0.006862645037472248, p_loss: 1.762341022491455, mean_rew: -0.3085247254211643, var_rew: 0.20471202737355285
steps: 924975, episodes: 37000, mean episode reward: -8.812044799097253, agent episode reward: [-0.33381700941784614, -8.478227789679407], time: 27.827
Running avgs for agent 0: q_loss: 0.04469430446624756, p_loss: 0.24461515247821808, mean_rew: -0.07885355132266259, var_rew: 0.13888968425760612
Running avgs for agent 1: q_loss: 0.006293150130659342, p_loss: 1.7701069116592407, mean_rew: -0.3101855602132968, var_rew: 0.20618108334385105
steps: 949975, episodes: 38000, mean episode reward: -8.739853881915012, agent episode reward: [-0.7659200348173183, -7.973933847097694], time: 28.828
Running avgs for agent 0: q_loss: 0.04364486038684845, p_loss: 0.14035135507583618, mean_rew: -0.07725402221846298, var_rew: 0.13854039442703292
Running avgs for agent 1: q_loss: 0.005176118109375238, p_loss: 1.7728395462036133, mean_rew: -0.311567915301515, var_rew: 0.20323296045036154
steps: 974975, episodes: 39000, mean episode reward: -8.582349389055965, agent episode reward: [-0.8390774588814254, -7.743271930174541], time: 27.851
Running avgs for agent 0: q_loss: 0.0461231991648674, p_loss: 0.08979585766792297, mean_rew: -0.07693854262567588, var_rew: 0.13918479163288755
Running avgs for agent 1: q_loss: 0.00565775390714407, p_loss: 1.7843447923660278, mean_rew: -0.3107891080220476, var_rew: 0.20174758098128856
steps: 999975, episodes: 40000, mean episode reward: -9.054076484234646, agent episode reward: [-1.3618179725054955, -7.6922585117291495], time: 27.737
Running avgs for agent 0: q_loss: 0.045985788106918335, p_loss: 0.0655369907617569, mean_rew: -0.07589933143272269, var_rew: 0.13768886130627317
Running avgs for agent 1: q_loss: 0.012507704086601734, p_loss: 1.7907569408416748, mean_rew: -0.31076579793206494, var_rew: 0.20091704746113254
steps: 1024975, episodes: 41000, mean episode reward: -8.651402927011553, agent episode reward: [-0.6216611257892591, -8.029741801222297], time: 27.81
Running avgs for agent 0: q_loss: 0.04813380539417267, p_loss: 0.01766456663608551, mean_rew: -0.07673580077159262, var_rew: 0.13498512109855362
Running avgs for agent 1: q_loss: 0.007740521803498268, p_loss: 1.7417733669281006, mean_rew: -0.3000443652530736, var_rew: 0.18895383110114877
steps: 1049975, episodes: 42000, mean episode reward: -8.838439890253033, agent episode reward: [-0.18876921927341586, -8.649670670979619], time: 27.629
Running avgs for agent 0: q_loss: 0.04047464206814766, p_loss: -0.07162867486476898, mean_rew: -0.07684291125781724, var_rew: 0.12183876872049894
Running avgs for agent 1: q_loss: 0.006154629867523909, p_loss: 1.656890630722046, mean_rew: -0.28325178443919585, var_rew: 0.16760270784402806
steps: 1074975, episodes: 43000, mean episode reward: -8.907492289783317, agent episode reward: [-0.1491491321363, -8.758343157647014], time: 27.843
Running avgs for agent 0: q_loss: 0.04199562594294548, p_loss: -0.19373074173927307, mean_rew: -0.0719403757462942, var_rew: 0.11854159922256571
Running avgs for agent 1: q_loss: 0.006801066920161247, p_loss: 1.6745588779449463, mean_rew: -0.2846071796650765, var_rew: 0.16628010254431905
steps: 1099975, episodes: 44000, mean episode reward: -9.049156352163584, agent episode reward: [-0.25848195820143927, -8.790674393962144], time: 27.655
Running avgs for agent 0: q_loss: 0.041957512497901917, p_loss: -0.2809024155139923, mean_rew: -0.06816276308673581, var_rew: 0.1194490634739302
Running avgs for agent 1: q_loss: 0.007533223833888769, p_loss: 1.6865956783294678, mean_rew: -0.28516580633651684, var_rew: 0.1643761545920143
steps: 1124975, episodes: 45000, mean episode reward: -8.855670070767886, agent episode reward: [-0.13492322812976584, -8.720746842638118], time: 27.633
Running avgs for agent 0: q_loss: 0.04425464943051338, p_loss: -0.3610931634902954, mean_rew: -0.06553238896738872, var_rew: 0.11768073668014187
Running avgs for agent 1: q_loss: 0.006984414532780647, p_loss: 1.6785907745361328, mean_rew: -0.287131927868919, var_rew: 0.16471397479476543
steps: 1149975, episodes: 46000, mean episode reward: -8.859752519056743, agent episode reward: [-1.3262907376340496, -7.533461781422694], time: 27.723
Running avgs for agent 0: q_loss: 0.04449177533388138, p_loss: -0.44520238041877747, mean_rew: -0.06301344431109429, var_rew: 0.11831194417086119
Running avgs for agent 1: q_loss: 0.006170772481709719, p_loss: 1.6485487222671509, mean_rew: -0.28849709114200134, var_rew: 0.16411314831328933
steps: 1174975, episodes: 47000, mean episode reward: -8.856703809858208, agent episode reward: [-1.1928034474173186, -7.663900362440891], time: 27.646
Running avgs for agent 0: q_loss: 0.04135284572839737, p_loss: -0.4551079571247101, mean_rew: -0.06318542497643044, var_rew: 0.11825791580262844
Running avgs for agent 1: q_loss: 0.00671803392469883, p_loss: 1.5897501707077026, mean_rew: -0.28988354067479044, var_rew: 0.1637728582978265
steps: 1199975, episodes: 48000, mean episode reward: -8.50600629127726, agent episode reward: [-0.8094658427490434, -7.696540448528217], time: 27.608
Running avgs for agent 0: q_loss: 0.03906187787652016, p_loss: -0.4927837550640106, mean_rew: -0.06285528909923147, var_rew: 0.11806611760121291
Running avgs for agent 1: q_loss: 0.006892487406730652, p_loss: 1.561272144317627, mean_rew: -0.2893498812852273, var_rew: 0.1623547790999486
steps: 1224975, episodes: 49000, mean episode reward: -8.776245051428832, agent episode reward: [-0.9812640316433229, -7.794981019785509], time: 27.713
Running avgs for agent 0: q_loss: 0.039682745933532715, p_loss: -0.5181838870048523, mean_rew: -0.06173713233789343, var_rew: 0.11741037568998551
Running avgs for agent 1: q_loss: 0.005630314815789461, p_loss: 1.597387433052063, mean_rew: -0.2893478127625691, var_rew: 0.15991241522048075
steps: 1249975, episodes: 50000, mean episode reward: -9.155488649424461, agent episode reward: [-1.5823971759425282, -7.5730914734819335], time: 27.733
Running avgs for agent 0: q_loss: 0.039465874433517456, p_loss: -0.5476459860801697, mean_rew: -0.05837365819755986, var_rew: 0.1172462325212367
Running avgs for agent 1: q_loss: 0.00597401661798358, p_loss: 1.6148285865783691, mean_rew: -0.29141950881790535, var_rew: 0.1603965034260494
steps: 1274975, episodes: 51000, mean episode reward: -9.189941478739765, agent episode reward: [-2.2229868071841516, -6.966954671555613], time: 27.866
Running avgs for agent 0: q_loss: 0.03873727470636368, p_loss: -0.541740357875824, mean_rew: -0.058700171045432244, var_rew: 0.11758585629233573
Running avgs for agent 1: q_loss: 0.006524452939629555, p_loss: 1.6001574993133545, mean_rew: -0.2920354949571161, var_rew: 0.1593041465611431
steps: 1299975, episodes: 52000, mean episode reward: -8.863711447430592, agent episode reward: [-1.8620925854056336, -7.0016188620249595], time: 27.551
Running avgs for agent 0: q_loss: 0.04021984338760376, p_loss: -0.5536441802978516, mean_rew: -0.05882581345792317, var_rew: 0.1178015904796165
Running avgs for agent 1: q_loss: 0.008612411096692085, p_loss: 1.5419154167175293, mean_rew: -0.2929019844549513, var_rew: 0.1604601197564773
steps: 1324975, episodes: 53000, mean episode reward: -8.773363122905899, agent episode reward: [-1.5741378138927453, -7.199225309013152], time: 27.631
Running avgs for agent 0: q_loss: 0.039152275770902634, p_loss: -0.5748291015625, mean_rew: -0.058463169157025915, var_rew: 0.11811407645325968
Running avgs for agent 1: q_loss: 0.010631789453327656, p_loss: 1.4486217498779297, mean_rew: -0.2927872738092616, var_rew: 0.16007476322346004
steps: 1349975, episodes: 54000, mean episode reward: -8.758173151230363, agent episode reward: [-1.308040190454855, -7.450132960775507], time: 27.563
Running avgs for agent 0: q_loss: 0.037395186722278595, p_loss: -0.6022464036941528, mean_rew: -0.056635458486844543, var_rew: 0.11753110193191522
Running avgs for agent 1: q_loss: 0.011585508473217487, p_loss: 1.4054386615753174, mean_rew: -0.29358197205515024, var_rew: 0.15958946641009317
steps: 1374975, episodes: 55000, mean episode reward: -8.370487904628794, agent episode reward: [-1.3172285246725548, -7.053259379956237], time: 27.586
Running avgs for agent 0: q_loss: 0.035640135407447815, p_loss: -0.5637117028236389, mean_rew: -0.05574485733413773, var_rew: 0.11683862260757359
Running avgs for agent 1: q_loss: 0.010657249018549919, p_loss: 1.390985131263733, mean_rew: -0.29467957390855226, var_rew: 0.16073068616942243
steps: 1399975, episodes: 56000, mean episode reward: -8.735912152586936, agent episode reward: [-1.595652254871688, -7.140259897715249], time: 27.642
Running avgs for agent 0: q_loss: 0.03665592893958092, p_loss: -0.5005874037742615, mean_rew: -0.05521233569691428, var_rew: 0.11747549298530478
Running avgs for agent 1: q_loss: 0.010180125012993813, p_loss: 1.360453486442566, mean_rew: -0.29562737181132037, var_rew: 0.16005897768404878
steps: 1424975, episodes: 57000, mean episode reward: -8.74145182921546, agent episode reward: [-1.6852015051748408, -7.0562503240406205], time: 27.65
Running avgs for agent 0: q_loss: 0.03561544790863991, p_loss: -0.45566490292549133, mean_rew: -0.055489210697335506, var_rew: 0.11765525335690015
Running avgs for agent 1: q_loss: 0.010685469023883343, p_loss: 1.3565291166305542, mean_rew: -0.29427352670640117, var_rew: 0.1584061641917607
steps: 1449975, episodes: 58000, mean episode reward: -8.761987262418497, agent episode reward: [-1.4979323244868505, -7.264054937931647], time: 27.621
Running avgs for agent 0: q_loss: 0.03606613725423813, p_loss: -0.42593109607696533, mean_rew: -0.055459576799312994, var_rew: 0.11718798013476342
Running avgs for agent 1: q_loss: 0.010421360842883587, p_loss: 1.360975980758667, mean_rew: -0.29664244664133405, var_rew: 0.16083460099290967
steps: 1474975, episodes: 59000, mean episode reward: -9.160006849754623, agent episode reward: [-2.0675486107893177, -7.092458238965304], time: 27.702
Running avgs for agent 0: q_loss: 0.0353146530687809, p_loss: -0.4279087483882904, mean_rew: -0.05437823247308109, var_rew: 0.11695719958566465
Running avgs for agent 1: q_loss: 0.012264348566532135, p_loss: 1.3067384958267212, mean_rew: -0.2960601920960402, var_rew: 0.1597626875454783
steps: 1499975, episodes: 60000, mean episode reward: -9.0005526496624, agent episode reward: [-1.9898305576164486, -7.010722092045952], time: 27.676
Running avgs for agent 0: q_loss: 0.03726309910416603, p_loss: -0.4287645220756531, mean_rew: -0.05576224198429792, var_rew: 0.11743659135572745
Running avgs for agent 1: q_loss: 0.01400342583656311, p_loss: 1.2215224504470825, mean_rew: -0.2962572950770317, var_rew: 0.15924306780794872
