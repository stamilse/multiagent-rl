python train.py --scenario simple_push --num-adversaries 0
2018-10-11 12:08:46.598029: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -54.48168641162879, time: 22.074
steps: 49975, episodes: 2000, mean episode reward: -28.46172227337128, time: 33.192
steps: 74975, episodes: 3000, mean episode reward: -14.14923849186563, time: 31.645
steps: 99975, episodes: 4000, mean episode reward: -13.600355622164217, time: 31.786
steps: 124975, episodes: 5000, mean episode reward: -13.233822130424306, time: 31.651
steps: 149975, episodes: 6000, mean episode reward: -13.672790787372652, time: 31.941
steps: 174975, episodes: 7000, mean episode reward: -13.234014306884445, time: 31.975
steps: 199975, episodes: 8000, mean episode reward: -13.740668773183897, time: 32.226
steps: 224975, episodes: 9000, mean episode reward: -13.414839202199547, time: 31.949
steps: 249975, episodes: 10000, mean episode reward: -13.372957357324871, time: 34.588
steps: 274975, episodes: 11000, mean episode reward: -13.64948384344524, time: 32.271
steps: 299975, episodes: 12000, mean episode reward: -13.630887757705255, time: 32.736
steps: 324975, episodes: 13000, mean episode reward: -13.949086132192503, time: 32.093
steps: 349975, episodes: 14000, mean episode reward: -13.893521178569317, time: 31.948
steps: 374975, episodes: 15000, mean episode reward: -13.654109124881515, time: 32.258
steps: 399975, episodes: 16000, mean episode reward: -13.386325245855236, time: 32.187
steps: 424975, episodes: 17000, mean episode reward: -13.675682298902462, time: 32.458
steps: 449975, episodes: 18000, mean episode reward: -13.666902239957526, time: 32.513
steps: 474975, episodes: 19000, mean episode reward: -13.74428975193549, time: 32.475
steps: 499975, episodes: 20000, mean episode reward: -13.897838272488611, time: 32.466
steps: 524975, episodes: 21000, mean episode reward: -14.083801329635346, time: 32.363
steps: 549975, episodes: 22000, mean episode reward: -13.914092021051449, time: 32.898
steps: 574975, episodes: 23000, mean episode reward: -14.067950179654948, time: 32.604
steps: 599975, episodes: 24000, mean episode reward: -13.901969298720845, time: 33.727
steps: 624975, episodes: 25000, mean episode reward: -14.39822613542616, time: 33.662
steps: 649975, episodes: 26000, mean episode reward: -15.062866727251748, time: 33.913
steps: 674975, episodes: 27000, mean episode reward: -15.02927248898615, time: 34.188
steps: 699975, episodes: 28000, mean episode reward: -14.294518423175964, time: 32.557
steps: 724975, episodes: 29000, mean episode reward: -13.694258740311945, time: 35.234
steps: 749975, episodes: 30000, mean episode reward: -14.241866074252618, time: 32.311
steps: 774975, episodes: 31000, mean episode reward: -13.910032196196246, time: 32.831
steps: 799975, episodes: 32000, mean episode reward: -14.205595553021867, time: 40.037
steps: 824975, episodes: 33000, mean episode reward: -14.219117032806885, time: 37.174
steps: 849975, episodes: 34000, mean episode reward: -14.765575787840644, time: 32.931
steps: 874975, episodes: 35000, mean episode reward: -14.256208384049966, time: 34.075
steps: 899975, episodes: 36000, mean episode reward: -14.170622340674687, time: 30.851
steps: 924975, episodes: 37000, mean episode reward: -14.489063767644783, time: 31.081
steps: 949975, episodes: 38000, mean episode reward: -14.700537458957074, time: 30.393
steps: 974975, episodes: 39000, mean episode reward: -14.415924122564068, time: 32.024
steps: 999975, episodes: 40000, mean episode reward: -14.372934309803364, time: 32.317
steps: 1024975, episodes: 41000, mean episode reward: -15.55701506492446, time: 32.006
steps: 1049975, episodes: 42000, mean episode reward: -14.519688467113044, time: 31.697
steps: 1074975, episodes: 43000, mean episode reward: -14.014556953171926, time: 32.946
steps: 1099975, episodes: 44000, mean episode reward: -13.93595947120168, time: 32.435
steps: 1124975, episodes: 45000, mean episode reward: -14.31505120035897, time: 36.365
steps: 1149975, episodes: 46000, mean episode reward: -14.151588196740347, time: 32.765
steps: 1174975, episodes: 47000, mean episode reward: -14.096809673590364, time: 33.208
steps: 1199975, episodes: 48000, mean episode reward: -14.206461023578232, time: 36.459
steps: 1224975, episodes: 49000, mean episode reward: -14.55878720872659, time: 35.233
steps: 1249975, episodes: 50000, mean episode reward: -13.87843497504362, time: 37.77
steps: 1274975, episodes: 51000, mean episode reward: -14.539515122934393, time: 33.528
steps: 1299975, episodes: 52000, mean episode reward: -15.175906028603508, time: 34.078
steps: 1324975, episodes: 53000, mean episode reward: -15.448388509571869, time: 33.512
steps: 1349975, episodes: 54000, mean episode reward: -15.671080487165279, time: 34.166
steps: 1374975, episodes: 55000, mean episode reward: -14.615858582094814, time: 31.385
steps: 1399975, episodes: 56000, mean episode reward: -14.367906336719988, time: 31.509
steps: 1424975, episodes: 57000, mean episode reward: -14.511190897476462, time: 30.523
steps: 1449975, episodes: 58000, mean episode reward: -14.441416779857486, time: 30.019
steps: 1474975, episodes: 59000, mean episode reward: -16.109465395247682, time: 30.089
steps: 1499975, episodes: 60000, mean episode reward: -16.840842658914568, time: 30.041
