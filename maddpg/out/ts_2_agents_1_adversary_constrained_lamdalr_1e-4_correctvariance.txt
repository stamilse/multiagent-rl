python train.py --scenario simple_push --num-adversaries 1
2018-10-25 16:49:19.646531: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -27.05544427370523, agent episode reward: [-0.06694161243420214, -26.988502661271028], time: 18.624
/anaconda2/envs/py36/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/anaconda2/envs/py36/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
Running avgs for agent 0: q_loss: nan, q_loss2: nan, p_loss: nan, mean_rew: nan, var_rew: nan, lamda: nan
Running avgs for agent 1: q_loss: nan, q_loss2: nan, p_loss: nan, mean_rew: nan, var_rew: nan, lamda: nan
steps: 49975, episodes: 2000, mean episode reward: -46.23460256493921, agent episode reward: [-32.264353256112166, -13.97024930882704], time: 29.805
Running avgs for agent 0: q_loss: 1.1828783750534058, q_loss2: -1.1111468076705933, p_loss: -0.8476036190986633, mean_rew: -0.3519871578266518, var_rew: -0.7751508951187134, lamda: 1.0078095197677612
Running avgs for agent 1: q_loss: 0.3378463089466095, q_loss2: -0.2843087315559387, p_loss: 1.4032843112945557, mean_rew: -0.963968494541895, var_rew: 0.008709010668098927, lamda: 1.0074371099472046
steps: 74975, episodes: 3000, mean episode reward: -47.64962644085195, agent episode reward: [-40.34617764781854, -7.303448793033415], time: 29.464
Running avgs for agent 0: q_loss: 0.6354945302009583, q_loss2: -0.5686879754066467, p_loss: -0.9727180600166321, mean_rew: -0.8466700650019603, var_rew: -0.8034235835075378, lamda: 1.028022289276123
Running avgs for agent 1: q_loss: 0.3882470726966858, q_loss2: -0.38789209723472595, p_loss: 2.0416910648345947, mean_rew: -0.7192618503629632, var_rew: 0.0001515479088993743, lamda: 1.0326417684555054
steps: 99975, episodes: 4000, mean episode reward: -36.502472558067474, agent episode reward: [-29.43815453206147, -7.064318026005998], time: 29.502
Running avgs for agent 0: q_loss: 0.9619242548942566, q_loss2: -0.856857419013977, p_loss: -0.9809211492538452, mean_rew: -0.9934345928150178, var_rew: -0.7706788778305054, lamda: 1.0534570217132568
Running avgs for agent 1: q_loss: 1.1819126605987549, q_loss2: -1.1484605073928833, p_loss: 2.31284236907959, mean_rew: -0.5942959120347362, var_rew: -0.010277051478624344, lamda: 1.0655159950256348
steps: 124975, episodes: 5000, mean episode reward: -42.82715665142461, agent episode reward: [-35.54147813927044, -7.285678512154167], time: 29.913
Running avgs for agent 0: q_loss: 4.4087324142456055, q_loss2: -4.212794780731201, p_loss: -0.6468498706817627, mean_rew: -1.064836471263743, var_rew: -0.5839859843254089, lamda: 1.0801656246185303
Running avgs for agent 1: q_loss: 1.844120740890503, q_loss2: -1.7753818035125732, p_loss: 2.3042616844177246, mean_rew: -0.5257430171961673, var_rew: 0.02096874639391899, lamda: 1.094648838043213
steps: 149975, episodes: 6000, mean episode reward: -41.352320674757394, agent episode reward: [-33.865084477888594, -7.487236196868797], time: 29.645
Running avgs for agent 0: q_loss: 10.4104585647583, q_loss2: -10.356463432312012, p_loss: 0.20692004263401031, mean_rew: -1.1260903389918266, var_rew: -0.4521400034427643, lamda: 1.1063603162765503
Running avgs for agent 1: q_loss: 3.0554466247558594, q_loss2: -2.8321893215179443, p_loss: 2.2360386848449707, mean_rew: -0.48422463251626746, var_rew: 0.03396645188331604, lamda: 1.122060775756836
steps: 174975, episodes: 7000, mean episode reward: -36.153102351917184, agent episode reward: [-28.8560728689822, -7.297029482934987], time: 29.617
Running avgs for agent 0: q_loss: 21.595420837402344, q_loss2: -20.754915237426758, p_loss: 1.0729377269744873, mean_rew: -1.148483927405903, var_rew: -0.27985161542892456, lamda: 1.1321609020233154
Running avgs for agent 1: q_loss: 3.904648780822754, q_loss2: -3.6424477100372314, p_loss: 2.158372163772583, mean_rew: -0.45546051986457153, var_rew: 0.037239182740449905, lamda: 1.1486376523971558
steps: 199975, episodes: 8000, mean episode reward: -25.27586199548781, agent episode reward: [-17.967450317772347, -7.308411677715464], time: 29.484
Running avgs for agent 0: q_loss: 32.158023834228516, q_loss2: -31.541311264038086, p_loss: 2.345790386199951, mean_rew: -1.128013350796638, var_rew: 0.025893695652484894, lamda: 1.1577234268188477
Running avgs for agent 1: q_loss: 5.7881269454956055, q_loss2: -5.156017780303955, p_loss: 2.097463846206665, mean_rew: -0.43053140267866813, var_rew: 0.023360591381788254, lamda: 1.1747305393218994
steps: 224975, episodes: 9000, mean episode reward: -15.004303253505915, agent episode reward: [-7.468404421815027, -7.535898831690889], time: 29.414
Running avgs for agent 0: q_loss: 33.64940643310547, q_loss2: -32.76661682128906, p_loss: 3.971353054046631, mean_rew: -1.0452384748529193, var_rew: -0.11838145554065704, lamda: 1.1831306219100952
Running avgs for agent 1: q_loss: 5.741731643676758, q_loss2: -5.066298961639404, p_loss: 2.085355043411255, mean_rew: -0.4177980625461087, var_rew: 0.02025359682738781, lamda: 1.200514793395996
steps: 249975, episodes: 10000, mean episode reward: -11.767198976012308, agent episode reward: [-3.51981217453294, -8.24738680147937], time: 29.305
Running avgs for agent 0: q_loss: 46.26278305053711, q_loss2: -46.23854064941406, p_loss: 5.1569647789001465, mean_rew: -0.9620259847833289, var_rew: -0.11031842231750488, lamda: 1.208431363105774
Running avgs for agent 1: q_loss: 6.779084205627441, q_loss2: -5.618490219116211, p_loss: 2.0589935779571533, mean_rew: -0.4048281103402134, var_rew: 0.023664681240916252, lamda: 1.2260903120040894
steps: 274975, episodes: 11000, mean episode reward: -11.900806063843996, agent episode reward: [-3.7129703831057617, -8.187835680738235], time: 29.227
Running avgs for agent 0: q_loss: 86.49470520019531, q_loss2: -83.21150207519531, p_loss: 5.819077014923096, mean_rew: -0.8828660685056732, var_rew: -0.11159256100654602, lamda: 1.233656406402588
Running avgs for agent 1: q_loss: 7.739152431488037, q_loss2: -6.5393500328063965, p_loss: 2.05987548828125, mean_rew: -0.39963455243512375, var_rew: 0.018363839015364647, lamda: 1.2515193223953247
steps: 299975, episodes: 12000, mean episode reward: -12.589673953557616, agent episode reward: [-4.379939958998603, -8.209733994559015], time: 29.933
Running avgs for agent 0: q_loss: 97.9338607788086, q_loss2: -94.63333129882812, p_loss: 6.052123069763184, mean_rew: -0.8197938812807954, var_rew: -0.17671844363212585, lamda: 1.2588266134262085
Running avgs for agent 1: q_loss: 6.036005020141602, q_loss2: -5.072264671325684, p_loss: 2.064257860183716, mean_rew: -0.3924933795190116, var_rew: 0.0496644452214241, lamda: 1.2768425941467285
steps: 324975, episodes: 13000, mean episode reward: -13.513905540119644, agent episode reward: [-5.097698880369972, -8.416206659749673], time: 29.823
Running avgs for agent 0: q_loss: 111.65726470947266, q_loss2: -108.24801635742188, p_loss: 6.001333236694336, mean_rew: -0.7682072948247283, var_rew: -0.11829593777656555, lamda: 1.2839560508728027
Running avgs for agent 1: q_loss: 7.544902324676514, q_loss2: -6.2510762214660645, p_loss: 2.0802645683288574, mean_rew: -0.38855093425060117, var_rew: 0.031165195629000664, lamda: 1.3020883798599243
steps: 349975, episodes: 14000, mean episode reward: -13.48649076526567, agent episode reward: [-4.853576292342686, -8.632914472922982], time: 29.3
Running avgs for agent 0: q_loss: 123.12227630615234, q_loss2: -117.60697174072266, p_loss: 5.86268424987793, mean_rew: -0.7247708787164786, var_rew: 0.03080594912171364, lamda: 1.3090554475784302
Running avgs for agent 1: q_loss: 6.531031131744385, q_loss2: -5.549376964569092, p_loss: 2.087965250015259, mean_rew: -0.3835781070116857, var_rew: 0.023779423907399178, lamda: 1.3272758722305298
steps: 374975, episodes: 15000, mean episode reward: -12.64727398023371, agent episode reward: [-4.217653818375245, -8.429620161858464], time: 29.171
Running avgs for agent 0: q_loss: 155.08912658691406, q_loss2: -146.25733947753906, p_loss: 5.673353672027588, mean_rew: -0.689351424499173, var_rew: -0.00657143397256732, lamda: 1.3341301679611206
Running avgs for agent 1: q_loss: 6.999603748321533, q_loss2: -5.790095329284668, p_loss: 2.134230136871338, mean_rew: -0.3826682592214217, var_rew: 0.04533808305859566, lamda: 1.3524202108383179
steps: 399975, episodes: 16000, mean episode reward: -13.350471852075898, agent episode reward: [-4.4021477540743925, -8.948324098001505], time: 29.181
Running avgs for agent 0: q_loss: 170.51649475097656, q_loss2: -162.611328125, p_loss: 5.493839740753174, mean_rew: -0.655683991850112, var_rew: -0.12241372466087341, lamda: 1.3591923713684082
Running avgs for agent 1: q_loss: 6.080297946929932, q_loss2: -5.326924800872803, p_loss: 2.169440984725952, mean_rew: -0.37731096104277445, var_rew: 0.04322486370801926, lamda: 1.377530813217163
steps: 424975, episodes: 17000, mean episode reward: -13.803987819471757, agent episode reward: [-4.90032462461197, -8.903663194859789], time: 29.369
Running avgs for agent 0: q_loss: 204.5331268310547, q_loss2: -187.52096557617188, p_loss: 5.237332344055176, mean_rew: -0.6234898114482974, var_rew: 0.029170522466301918, lamda: 1.3842333555221558
Running avgs for agent 1: q_loss: 5.668545246124268, q_loss2: -4.76669454574585, p_loss: 2.2098734378814697, mean_rew: -0.3783576470417421, var_rew: 0.03898204118013382, lamda: 1.402617335319519
steps: 449975, episodes: 18000, mean episode reward: -14.410580108500735, agent episode reward: [-5.253410828036127, -9.157169280464608], time: 29.666
Running avgs for agent 0: q_loss: 184.31292724609375, q_loss2: -168.2545166015625, p_loss: 5.0823445320129395, mean_rew: -0.6053337346468547, var_rew: 0.1279372274875641, lamda: 1.4092674255371094
Running avgs for agent 1: q_loss: 5.596831798553467, q_loss2: -4.734498977661133, p_loss: 2.252331018447876, mean_rew: -0.37758811579436824, var_rew: 0.028417829424142838, lamda: 1.4276823997497559
steps: 474975, episodes: 19000, mean episode reward: -13.95021944353027, agent episode reward: [-5.1259601955752485, -8.824259247955021], time: 29.284
Running avgs for agent 0: q_loss: 225.51211547851562, q_loss2: -213.22332763671875, p_loss: 4.906383991241455, mean_rew: -0.5839878792020609, var_rew: 0.023869328200817108, lamda: 1.4343012571334839
Running avgs for agent 1: q_loss: 5.784564971923828, q_loss2: -5.030074596405029, p_loss: 2.2651641368865967, mean_rew: -0.37619057247680965, var_rew: 0.0327533483505249, lamda: 1.4527361392974854
steps: 499975, episodes: 20000, mean episode reward: -13.92674408246725, agent episode reward: [-5.61192922101241, -8.31481486145484], time: 29.349
Running avgs for agent 0: q_loss: 288.87689208984375, q_loss2: -265.47674560546875, p_loss: 4.685507297515869, mean_rew: -0.5634598487466631, var_rew: 0.10913650691509247, lamda: 1.4593297243118286
Running avgs for agent 1: q_loss: 6.0156450271606445, q_loss2: -5.0072245597839355, p_loss: 2.2669270038604736, mean_rew: -0.37521111779710564, var_rew: 0.03559386357665062, lamda: 1.4777705669403076
steps: 524975, episodes: 21000, mean episode reward: -14.35176757799247, agent episode reward: [-6.194920969431707, -8.156846608560762], time: 29.489
Running avgs for agent 0: q_loss: 284.8607177734375, q_loss2: -249.25279235839844, p_loss: 4.532609462738037, mean_rew: -0.5460437325511671, var_rew: 0.10851646959781647, lamda: 1.4843358993530273
Running avgs for agent 1: q_loss: 6.367743492126465, q_loss2: -5.414517402648926, p_loss: 2.258607864379883, mean_rew: -0.37374365278403454, var_rew: 0.02678868919610977, lamda: 1.5028043985366821
steps: 549975, episodes: 22000, mean episode reward: -13.971148071759938, agent episode reward: [-6.013050173016534, -7.958097898743403], time: 29.545
Running avgs for agent 0: q_loss: 261.7735595703125, q_loss2: -231.94036865234375, p_loss: 4.484423637390137, mean_rew: -0.5335170022568543, var_rew: 0.13580447435379028, lamda: 1.5093401670455933
Running avgs for agent 1: q_loss: 6.566239833831787, q_loss2: -5.505741119384766, p_loss: 2.2113089561462402, mean_rew: -0.3698979345093836, var_rew: 0.025948630645871162, lamda: 1.527838110923767
steps: 574975, episodes: 23000, mean episode reward: -13.889293532779668, agent episode reward: [-6.624690257429634, -7.264603275350032], time: 30.06
Running avgs for agent 0: q_loss: 238.8656005859375, q_loss2: -210.5584259033203, p_loss: 4.425492286682129, mean_rew: -0.5209729543362748, var_rew: 0.11529118567705154, lamda: 1.5343444347381592
Running avgs for agent 1: q_loss: 7.6124444007873535, q_loss2: -6.423058986663818, p_loss: 2.1823437213897705, mean_rew: -0.3661596050880211, var_rew: 0.0025512948632240295, lamda: 1.5528545379638672
steps: 599975, episodes: 24000, mean episode reward: -14.071935908025361, agent episode reward: [-6.798309561643247, -7.273626346382114], time: 29.638
Running avgs for agent 0: q_loss: 265.35614013671875, q_loss2: -235.1204071044922, p_loss: 4.431013584136963, mean_rew: -0.509684548998163, var_rew: -0.03798522800207138, lamda: 1.559348464012146
Running avgs for agent 1: q_loss: 7.470311641693115, q_loss2: -6.188334941864014, p_loss: 2.1454684734344482, mean_rew: -0.36604368828251155, var_rew: 0.016449423506855965, lamda: 1.5778584480285645
steps: 624975, episodes: 25000, mean episode reward: -14.860580829480034, agent episode reward: [-7.422491196814752, -7.438089632665279], time: 29.591
Running avgs for agent 0: q_loss: 224.3292236328125, q_loss2: -191.3327178955078, p_loss: 4.4723076820373535, mean_rew: -0.49964456051369704, var_rew: 0.0012983741471543908, lamda: 1.5843524932861328
Running avgs for agent 1: q_loss: 7.113603591918945, q_loss2: -5.9012603759765625, p_loss: 2.0643630027770996, mean_rew: -0.36122398620303237, var_rew: 0.016601217910647392, lamda: 1.6028627157211304
steps: 649975, episodes: 26000, mean episode reward: -15.915574061599264, agent episode reward: [-8.05086366189964, -7.864710399699624], time: 29.684
Running avgs for agent 0: q_loss: 307.6607360839844, q_loss2: -273.6718444824219, p_loss: 4.526154041290283, mean_rew: -0.4946043810457502, var_rew: 0.030002547428011894, lamda: 1.6093568801879883
Running avgs for agent 1: q_loss: 6.496397495269775, q_loss2: -5.442515850067139, p_loss: 1.9489264488220215, mean_rew: -0.3579777062994415, var_rew: 0.023235950618982315, lamda: 1.6278669834136963
steps: 674975, episodes: 27000, mean episode reward: -16.3919693251053, agent episode reward: [-9.005678344708839, -7.386290980396461], time: 30.037
Running avgs for agent 0: q_loss: 351.51910400390625, q_loss2: -308.6346130371094, p_loss: 4.517940521240234, mean_rew: -0.48523610363080355, var_rew: 0.08120638132095337, lamda: 1.6343607902526855
Running avgs for agent 1: q_loss: 6.958416938781738, q_loss2: -5.730779647827148, p_loss: 1.8416005373001099, mean_rew: -0.3567817651165034, var_rew: 0.01328441221266985, lamda: 1.652871012687683
steps: 699975, episodes: 28000, mean episode reward: -13.60563688611859, agent episode reward: [-5.982721310278363, -7.6229155758402305], time: 30.08
Running avgs for agent 0: q_loss: 222.13987731933594, q_loss2: -194.9263458251953, p_loss: 4.631842613220215, mean_rew: -0.48097287684594386, var_rew: 0.06571926176548004, lamda: 1.6593650579452515
Running avgs for agent 1: q_loss: 6.107390403747559, q_loss2: -5.1906232833862305, p_loss: 1.6896592378616333, mean_rew: -0.354555530379562, var_rew: 0.026958325877785683, lamda: 1.6778751611709595
steps: 724975, episodes: 29000, mean episode reward: -13.998525983504459, agent episode reward: [-5.988506612918792, -8.010019370585667], time: 29.523
Running avgs for agent 0: q_loss: 280.4005126953125, q_loss2: -249.1341552734375, p_loss: 4.699975490570068, mean_rew: -0.4745557792855835, var_rew: 0.0891600027680397, lamda: 1.6843693256378174
Running avgs for agent 1: q_loss: 6.799581050872803, q_loss2: -5.699894428253174, p_loss: 1.5471110343933105, mean_rew: -0.35176730000678746, var_rew: 0.005037165712565184, lamda: 1.7028794288635254
steps: 749975, episodes: 30000, mean episode reward: -15.297314106126962, agent episode reward: [-7.00631544381391, -8.290998662313052], time: 29.422
Running avgs for agent 0: q_loss: 252.79986572265625, q_loss2: -215.72247314453125, p_loss: 4.755088806152344, mean_rew: -0.4667400137210053, var_rew: 0.10004977136850357, lamda: 1.7093732357025146
Running avgs for agent 1: q_loss: 6.786781311035156, q_loss2: -5.685879707336426, p_loss: 1.4598698616027832, mean_rew: -0.352075573648146, var_rew: 0.005429674871265888, lamda: 1.7278833389282227
steps: 774975, episodes: 31000, mean episode reward: -15.139434321505968, agent episode reward: [-6.954677934941861, -8.184756386564107], time: 29.568
Running avgs for agent 0: q_loss: 328.8560485839844, q_loss2: -287.1392517089844, p_loss: 4.807870388031006, mean_rew: -0.46063962908420875, var_rew: 0.07035389542579651, lamda: 1.734377384185791
Running avgs for agent 1: q_loss: 6.680573463439941, q_loss2: -5.672769069671631, p_loss: 1.351076602935791, mean_rew: -0.351772640108789, var_rew: 0.014066629111766815, lamda: 1.7528876066207886
steps: 799975, episodes: 32000, mean episode reward: -15.570927413375793, agent episode reward: [-7.106799602458208, -8.464127810917583], time: 29.496
Running avgs for agent 0: q_loss: 277.0496826171875, q_loss2: -249.39266967773438, p_loss: 4.879690170288086, mean_rew: -0.45332048155444915, var_rew: 0.04690808802843094, lamda: 1.7593817710876465
Running avgs for agent 1: q_loss: 6.398622989654541, q_loss2: -5.40963888168335, p_loss: 1.2010937929153442, mean_rew: -0.3514222416129403, var_rew: 0.024459293112158775, lamda: 1.7778918743133545
steps: 824975, episodes: 33000, mean episode reward: -15.715692806906073, agent episode reward: [-6.693473686697479, -9.022219120208597], time: 30.628
Running avgs for agent 0: q_loss: 262.7967834472656, q_loss2: -228.67068481445312, p_loss: 4.901183128356934, mean_rew: -0.4482567287186669, var_rew: 0.06062091886997223, lamda: 1.7843858003616333
Running avgs for agent 1: q_loss: 6.107722759246826, q_loss2: -5.210342884063721, p_loss: 1.0059822797775269, mean_rew: -0.35042243260634615, var_rew: 0.00643672700971365, lamda: 1.8028959035873413
steps: 849975, episodes: 34000, mean episode reward: -15.540504133676642, agent episode reward: [-5.786336290938843, -9.754167842737795], time: 30.002
Running avgs for agent 0: q_loss: 299.2200622558594, q_loss2: -265.59844970703125, p_loss: 4.891993999481201, mean_rew: -0.4405412188957878, var_rew: 0.13899211585521698, lamda: 1.8093900680541992
Running avgs for agent 1: q_loss: 6.369444370269775, q_loss2: -5.502326965332031, p_loss: 0.8372547626495361, mean_rew: -0.35284092732102385, var_rew: -0.0035838556941598654, lamda: 1.8279000520706177
steps: 874975, episodes: 35000, mean episode reward: -14.89568738839989, agent episode reward: [-4.68413729733723, -10.21155009106266], time: 29.862
Running avgs for agent 0: q_loss: 223.9643096923828, q_loss2: -191.86282348632812, p_loss: 4.942303657531738, mean_rew: -0.43629499284213885, var_rew: 0.031563859432935715, lamda: 1.8343942165374756
Running avgs for agent 1: q_loss: 5.6611738204956055, q_loss2: -4.845206260681152, p_loss: 0.689833402633667, mean_rew: -0.3522649223007184, var_rew: -0.03328714892268181, lamda: 1.8529043197631836
steps: 899975, episodes: 36000, mean episode reward: -15.519228151269912, agent episode reward: [-4.600629523714954, -10.918598627554958], time: 29.801
Running avgs for agent 0: q_loss: 290.6729431152344, q_loss2: -257.31805419921875, p_loss: 4.911669731140137, mean_rew: -0.42842311123305377, var_rew: 0.11216249316930771, lamda: 1.8593982458114624
Running avgs for agent 1: q_loss: 6.434701442718506, q_loss2: -5.582787036895752, p_loss: 0.49872857332229614, mean_rew: -0.3549403540058056, var_rew: -0.03413994610309601, lamda: 1.8779082298278809
steps: 924975, episodes: 37000, mean episode reward: -15.819819948242312, agent episode reward: [-4.376553161741187, -11.443266786501125], time: 29.341
Running avgs for agent 0: q_loss: 249.3733367919922, q_loss2: -218.31362915039062, p_loss: 4.913494110107422, mean_rew: -0.42267771123781445, var_rew: 0.050753917545080185, lamda: 1.8844023942947388
Running avgs for agent 1: q_loss: 6.910335063934326, q_loss2: -5.820742607116699, p_loss: 0.36772826313972473, mean_rew: -0.3577596072774668, var_rew: -0.04819415137171745, lamda: 1.9029124975204468
steps: 949975, episodes: 38000, mean episode reward: -15.493524959155494, agent episode reward: [-3.1997110876861736, -12.293813871469318], time: 29.361
Running avgs for agent 0: q_loss: 229.53695678710938, q_loss2: -217.49044799804688, p_loss: 4.911176681518555, mean_rew: -0.4145921387813072, var_rew: 0.0950375646352768, lamda: 1.9094066619873047
Running avgs for agent 1: q_loss: 7.950062274932861, q_loss2: -6.934514999389648, p_loss: 0.1790212094783783, mean_rew: -0.3605217836919603, var_rew: -0.026916412636637688, lamda: 1.9279167652130127
steps: 974975, episodes: 39000, mean episode reward: -15.497809340835778, agent episode reward: [-3.3521430316378917, -12.145666309197884], time: 29.558
Running avgs for agent 0: q_loss: 227.67062377929688, q_loss2: -197.70057678222656, p_loss: 4.909511566162109, mean_rew: -0.40822245620332, var_rew: 0.1291525661945343, lamda: 1.9344106912612915
Running avgs for agent 1: q_loss: 7.862636566162109, q_loss2: -6.851963043212891, p_loss: -0.02387256547808647, mean_rew: -0.36384430321028494, var_rew: -0.0020445832051336765, lamda: 1.9529207944869995
steps: 999975, episodes: 40000, mean episode reward: -15.543864508424502, agent episode reward: [-3.563224613270145, -11.980639895154358], time: 29.628
Running avgs for agent 0: q_loss: 259.47833251953125, q_loss2: -220.0670623779297, p_loss: 4.85434103012085, mean_rew: -0.39843122843723083, var_rew: 0.17626601457595825, lamda: 1.9594149589538574
Running avgs for agent 1: q_loss: 8.9443941116333, q_loss2: -7.746852874755859, p_loss: -0.29185009002685547, mean_rew: -0.36772263717277165, var_rew: 0.03489072620868683, lamda: 1.9779249429702759
steps: 1024975, episodes: 41000, mean episode reward: -16.045919729120104, agent episode reward: [-4.801769792632198, -11.244149936487908], time: 29.483
Running avgs for agent 0: q_loss: 241.05224609375, q_loss2: -209.97900390625, p_loss: 4.905942440032959, mean_rew: -0.39985280790070177, var_rew: 0.06580284982919693, lamda: 1.9844189882278442
Running avgs for agent 1: q_loss: 8.047636985778809, q_loss2: -7.233626365661621, p_loss: -0.6487536430358887, mean_rew: -0.36067970801452637, var_rew: 0.09908728301525116, lamda: 2.0029234886169434
steps: 1049975, episodes: 42000, mean episode reward: -16.046743122448863, agent episode reward: [-5.819888264276256, -10.22685485817261], time: 29.465
Running avgs for agent 0: q_loss: 279.03399658203125, q_loss2: -257.3979187011719, p_loss: 4.890021324157715, mean_rew: -0.3928791248554701, var_rew: 0.09447664022445679, lamda: 2.009411573410034
Running avgs for agent 1: q_loss: 7.302260875701904, q_loss2: -6.693090915679932, p_loss: -1.0011968612670898, mean_rew: -0.3491225044275942, var_rew: 0.18714725971221924, lamda: 2.027899980545044
steps: 1074975, episodes: 43000, mean episode reward: -16.025402673839555, agent episode reward: [-6.662089659832822, -9.363313014006733], time: 30.967
Running avgs for agent 0: q_loss: 232.61753845214844, q_loss2: -198.75962829589844, p_loss: 4.627748012542725, mean_rew: -0.35419375881465337, var_rew: 0.02989213541150093, lamda: 2.034386396408081
Running avgs for agent 1: q_loss: 9.449962615966797, q_loss2: -9.07674789428711, p_loss: -1.2135165929794312, mean_rew: -0.3499370173870343, var_rew: 0.22974519431591034, lamda: 2.0528743267059326
steps: 1099975, episodes: 44000, mean episode reward: -16.670611084825783, agent episode reward: [-7.401783371339694, -9.268827713486088], time: 29.506
Running avgs for agent 0: q_loss: 135.89511108398438, q_loss2: -118.30823516845703, p_loss: 4.482390403747559, mean_rew: -0.3283740483253348, var_rew: -0.0036663170903921127, lamda: 2.0593607425689697
Running avgs for agent 1: q_loss: 11.75813102722168, q_loss2: -11.241732597351074, p_loss: -1.332647442817688, mean_rew: -0.3526095215458541, var_rew: 0.25374048948287964, lamda: 2.0778486728668213
steps: 1124975, episodes: 45000, mean episode reward: -16.874417971409887, agent episode reward: [-7.521152044111031, -9.353265927298853], time: 29.258
Running avgs for agent 0: q_loss: 139.67172241210938, q_loss2: -127.57559204101562, p_loss: 4.362655162811279, mean_rew: -0.3053099366305326, var_rew: 0.027583012357354164, lamda: 2.0843348503112793
Running avgs for agent 1: q_loss: 12.522703170776367, q_loss2: -11.994230270385742, p_loss: -1.418994665145874, mean_rew: -0.3549782801748131, var_rew: 0.26073122024536133, lamda: 2.102822780609131
steps: 1149975, episodes: 46000, mean episode reward: -15.706261010995691, agent episode reward: [-6.470220148884707, -9.236040862110986], time: 29.801
Running avgs for agent 0: q_loss: 101.54808044433594, q_loss2: -94.70652770996094, p_loss: 4.161367416381836, mean_rew: -0.27796537459077597, var_rew: 0.09083312004804611, lamda: 2.109309673309326
Running avgs for agent 1: q_loss: 13.352788925170898, q_loss2: -12.763589859008789, p_loss: -1.4891765117645264, mean_rew: -0.35607882930647655, var_rew: 0.25502458214759827, lamda: 2.1277973651885986
steps: 1174975, episodes: 47000, mean episode reward: -14.259933353282348, agent episode reward: [-5.32790781379475, -8.932025539487597], time: 29.444
Running avgs for agent 0: q_loss: 62.03325653076172, q_loss2: -60.0352668762207, p_loss: 3.9567222595214844, mean_rew: -0.2515214414559454, var_rew: 0.017070362344384193, lamda: 2.1342837810516357
Running avgs for agent 1: q_loss: 15.328455924987793, q_loss2: -14.895060539245605, p_loss: -1.6199816465377808, mean_rew: -0.35756731965761845, var_rew: 0.28355783224105835, lamda: 2.1527717113494873
steps: 1199975, episodes: 48000, mean episode reward: -12.640907995487401, agent episode reward: [-3.9910981682329245, -8.649809827254478], time: 29.477
Running avgs for agent 0: q_loss: 55.13612365722656, q_loss2: -40.65299987792969, p_loss: 3.729180335998535, mean_rew: -0.23240534067364976, var_rew: 0.008283127099275589, lamda: 2.1592581272125244
Running avgs for agent 1: q_loss: 17.3220272064209, q_loss2: -16.6133975982666, p_loss: -1.77397620677948, mean_rew: -0.3596241879077937, var_rew: 0.26917704939842224, lamda: 2.177745819091797
steps: 1224975, episodes: 49000, mean episode reward: -12.354321009301199, agent episode reward: [-3.467487017622191, -8.886833991679008], time: 30.091
Running avgs for agent 0: q_loss: 13.309611320495605, q_loss2: -12.260964393615723, p_loss: 3.509533166885376, mean_rew: -0.22482928319743598, var_rew: 0.06679792702198029, lamda: 2.184232473373413
Running avgs for agent 1: q_loss: 16.475576400756836, q_loss2: -15.841131210327148, p_loss: -1.8898049592971802, mean_rew: -0.3596901843248197, var_rew: 0.2527514398097992, lamda: 2.2027204036712646
steps: 1249975, episodes: 50000, mean episode reward: -12.447530888321948, agent episode reward: [-2.6666638592783194, -9.780867029043627], time: 30.437
Running avgs for agent 0: q_loss: 11.649309158325195, q_loss2: -10.763384819030762, p_loss: 3.272972583770752, mean_rew: -0.22169516983170853, var_rew: 0.061465196311473846, lamda: 2.2092068195343018
Running avgs for agent 1: q_loss: 13.737332344055176, q_loss2: -13.035310745239258, p_loss: -1.9353300333023071, mean_rew: -0.3622882186217601, var_rew: 0.17032064497470856, lamda: 2.2276947498321533
steps: 1274975, episodes: 51000, mean episode reward: -12.249664915061926, agent episode reward: [-1.841794467289203, -10.407870447772723], time: 30.026
Running avgs for agent 0: q_loss: 11.061802864074707, q_loss2: -10.121927261352539, p_loss: 3.018810272216797, mean_rew: -0.2197128465017534, var_rew: 0.04794077202677727, lamda: 2.2341811656951904
Running avgs for agent 1: q_loss: 12.908764839172363, q_loss2: -12.237470626831055, p_loss: -1.8485511541366577, mean_rew: -0.36243955416229456, var_rew: 0.15047508478164673, lamda: 2.252668857574463
steps: 1299975, episodes: 52000, mean episode reward: -12.267884237775654, agent episode reward: [-0.9005161545432389, -11.367368083232416], time: 30.089
Running avgs for agent 0: q_loss: 10.824207305908203, q_loss2: -9.851320266723633, p_loss: 2.6759860515594482, mean_rew: -0.21854504625376564, var_rew: 0.06761332601308823, lamda: 2.259155511856079
Running avgs for agent 1: q_loss: 10.678507804870605, q_loss2: -9.968067169189453, p_loss: -1.6717637777328491, mean_rew: -0.36489576964116804, var_rew: 0.1440742164850235, lamda: 2.2776434421539307
steps: 1324975, episodes: 53000, mean episode reward: -11.917794985788293, agent episode reward: [0.6226915293108924, -12.540486515099182], time: 30.19
Running avgs for agent 0: q_loss: 10.768232345581055, q_loss2: -9.764326095581055, p_loss: 2.2562358379364014, mean_rew: -0.21371713111610546, var_rew: 0.06975025683641434, lamda: 2.2841298580169678
Running avgs for agent 1: q_loss: 10.668076515197754, q_loss2: -9.908970832824707, p_loss: -1.45607328414917, mean_rew: -0.3707819639625269, var_rew: 0.19801214337348938, lamda: 2.3026177883148193
steps: 1349975, episodes: 54000, mean episode reward: -12.059448454559305, agent episode reward: [1.0593052215994652, -13.118753676158773], time: 29.967
Running avgs for agent 0: q_loss: 10.291223526000977, q_loss2: -9.300060272216797, p_loss: 1.8897883892059326, mean_rew: -0.20649920035316324, var_rew: 0.05175839364528656, lamda: 2.3091042041778564
Running avgs for agent 1: q_loss: 7.7755446434021, q_loss2: -7.167342662811279, p_loss: -1.2927067279815674, mean_rew: -0.374642148436012, var_rew: 0.17662441730499268, lamda: 2.327592372894287
steps: 1374975, episodes: 55000, mean episode reward: -12.18275754364099, agent episode reward: [1.3199368893630312, -13.502694433004022], time: 30.211
Running avgs for agent 0: q_loss: 10.014226913452148, q_loss2: -9.075510025024414, p_loss: 1.5627063512802124, mean_rew: -0.2038858480544429, var_rew: 0.05440925806760788, lamda: 2.334078550338745
Running avgs for agent 1: q_loss: 6.602565288543701, q_loss2: -5.914120197296143, p_loss: -1.0832675695419312, mean_rew: -0.37785020680863557, var_rew: 0.15263736248016357, lamda: 2.3525664806365967
steps: 1399975, episodes: 56000, mean episode reward: -12.425567925175635, agent episode reward: [2.0959930985598083, -14.521561023735442], time: 30.207
Running avgs for agent 0: q_loss: 9.616279602050781, q_loss2: -8.7146635055542, p_loss: 1.2313332557678223, mean_rew: -0.1958670625246666, var_rew: 0.05370139330625534, lamda: 2.3590526580810547
Running avgs for agent 1: q_loss: 6.430304050445557, q_loss2: -5.675255298614502, p_loss: -0.8922554850578308, mean_rew: -0.38400459375454216, var_rew: 0.09999940544366837, lamda: 2.3775408267974854
steps: 1424975, episodes: 57000, mean episode reward: -12.457425172856267, agent episode reward: [2.564483196628357, -15.021908369484622], time: 30.073
Running avgs for agent 0: q_loss: 9.200943946838379, q_loss2: -8.35973072052002, p_loss: 0.8821009993553162, mean_rew: -0.1889040787221545, var_rew: 0.05836945399641991, lamda: 2.3840272426605225
Running avgs for agent 1: q_loss: 6.434699535369873, q_loss2: -5.732893466949463, p_loss: -0.6512314677238464, mean_rew: -0.3912828258705755, var_rew: 0.058502234518527985, lamda: 2.402515172958374
steps: 1449975, episodes: 58000, mean episode reward: -13.204723456187537, agent episode reward: [0.6750170361849578, -13.879740492372497], time: 30.091
Running avgs for agent 0: q_loss: 8.308252334594727, q_loss2: -7.456905364990234, p_loss: 0.49486243724823, mean_rew: -0.18200295116656928, var_rew: 0.0545356385409832, lamda: 2.409001350402832
Running avgs for agent 1: q_loss: 5.801056861877441, q_loss2: -5.012688636779785, p_loss: -0.44645655155181885, mean_rew: -0.3950153525769162, var_rew: 0.009280014783143997, lamda: 2.4274895191192627
steps: 1474975, episodes: 59000, mean episode reward: -13.778219601406192, agent episode reward: [1.052334291831261, -14.830553893237454], time: 29.96
Running avgs for agent 0: q_loss: 8.375727653503418, q_loss2: -7.464041233062744, p_loss: 0.28076493740081787, mean_rew: -0.17651089198047407, var_rew: 0.06669130176305771, lamda: 2.433976173400879
Running avgs for agent 1: q_loss: 4.680085182189941, q_loss2: -4.078951835632324, p_loss: -0.35691720247268677, mean_rew: -0.4004884746774937, var_rew: -0.017367294058203697, lamda: 2.4524638652801514
steps: 1499975, episodes: 60000, mean episode reward: -13.774662647115026, agent episode reward: [0.5306665592119739, -14.305329206326999], time: 29.995
Running avgs for agent 0: q_loss: 8.370089530944824, q_loss2: -7.488306522369385, p_loss: 0.13399280607700348, mean_rew: -0.16842365685143318, var_rew: 0.05992867425084114, lamda: 2.4589502811431885
Running avgs for agent 1: q_loss: 4.1881489753723145, q_loss2: -3.739643096923828, p_loss: -0.2808733582496643, mean_rew: -0.4051018634601961, var_rew: -0.023875610902905464, lamda: 2.477437973022461
Traceback (most recent call last):
  File "train.py", line 235, in <module>
    train(arglist)
  File "train.py", line 224, in train
    rew_file_name = arglist.plots_dir + arglist.exp_name + '_rewards.pkl'
TypeError: must be str, not NoneType

