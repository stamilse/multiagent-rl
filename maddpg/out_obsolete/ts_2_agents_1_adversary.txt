python train_ts.py --scenario simple_push --num-adversaries 1
2018-10-11 16:29:49.008638: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -28.172988238958904, agent episode reward: [-2.705696264458809, -25.467291974500096], time: 19.559
steps: 49975, episodes: 2000, mean episode reward: -15.580432348835844, agent episode reward: [-2.283280310818852, -13.297152038016995], time: 67.168
steps: 74975, episodes: 3000, mean episode reward: -10.55762299981363, agent episode reward: [-3.9530603607433177, -6.604562639070313], time: 67.595
steps: 99975, episodes: 4000, mean episode reward: -9.710534823053264, agent episode reward: [-2.863811694275003, -6.846723128778261], time: 70.487
steps: 124975, episodes: 5000, mean episode reward: -8.995004477261041, agent episode reward: [-2.1577113113952477, -6.837293165865795], time: 73.912
steps: 149975, episodes: 6000, mean episode reward: -8.725871245683788, agent episode reward: [-1.7507118021932913, -6.975159443490496], time: 74.447
steps: 174975, episodes: 7000, mean episode reward: -8.947470759795767, agent episode reward: [-1.9469171540653356, -7.00055360573043], time: 76.923
steps: 199975, episodes: 8000, mean episode reward: -9.087349566904372, agent episode reward: [-1.4564331701765296, -7.63091639672784], time: 82.45
steps: 224975, episodes: 9000, mean episode reward: -9.087335503600865, agent episode reward: [-0.26680160930624897, -8.820533894294618], time: 88.898
steps: 249975, episodes: 10000, mean episode reward: -9.46463007299227, agent episode reward: [0.4109155421438333, -9.875545615136103], time: 81.307
steps: 274975, episodes: 11000, mean episode reward: -9.053741300755327, agent episode reward: [-0.037604676201722156, -9.016136624553605], time: 79.474
steps: 299975, episodes: 12000, mean episode reward: -9.511676970047029, agent episode reward: [-0.776421230782894, -8.735255739264135], time: 81.815
steps: 324975, episodes: 13000, mean episode reward: -9.562077124566187, agent episode reward: [-0.862226981021621, -8.699850143544564], time: 88.277
steps: 349975, episodes: 14000, mean episode reward: -9.863781689340083, agent episode reward: [-0.45779634620992316, -9.40598534313016], time: 85.664
steps: 374975, episodes: 15000, mean episode reward: -9.900688055247517, agent episode reward: [1.4994647673202588, -11.400152822567776], time: 83.419
steps: 399975, episodes: 16000, mean episode reward: -10.013718219711201, agent episode reward: [0.7508159166430525, -10.764534136354253], time: 84.191
steps: 424975, episodes: 17000, mean episode reward: -10.081860035556051, agent episode reward: [-1.0288803280702892, -9.052979707485763], time: 90.804
steps: 449975, episodes: 18000, mean episode reward: -10.01629091761348, agent episode reward: [-0.1115927457034278, -9.904698171910052], time: 86.387
steps: 474975, episodes: 19000, mean episode reward: -9.992874899281679, agent episode reward: [-0.19182179829391058, -9.801053100987767], time: 87.214
steps: 499975, episodes: 20000, mean episode reward: -10.258383120482332, agent episode reward: [-1.3339280903533552, -8.924455030128977], time: 87.717
steps: 524975, episodes: 21000, mean episode reward: -10.035603558784459, agent episode reward: [-0.6428621858629373, -9.39274137292152], time: 97.828
steps: 549975, episodes: 22000, mean episode reward: -9.7791985423763, agent episode reward: [-0.3158207280139317, -9.463377814362367], time: 106.454
steps: 574975, episodes: 23000, mean episode reward: -10.1013272858059, agent episode reward: [-0.44790184321298293, -9.653425442592914], time: 124.899
steps: 599975, episodes: 24000, mean episode reward: -10.086041504672412, agent episode reward: [-1.2371629636611503, -8.848878541011262], time: 115.503
steps: 624975, episodes: 25000, mean episode reward: -10.02498135477972, agent episode reward: [-1.770486058733062, -8.254495296046658], time: 89.624
steps: 649975, episodes: 26000, mean episode reward: -10.086721221809624, agent episode reward: [-1.207739622322581, -8.878981599487044], time: 104.748
steps: 674975, episodes: 27000, mean episode reward: -10.463915449414708, agent episode reward: [1.6994252909291807, -12.163340740343891], time: 94.634
steps: 699975, episodes: 28000, mean episode reward: -11.225601400797377, agent episode reward: [0.8223007064159095, -12.047902107213286], time: 127.643
steps: 724975, episodes: 29000, mean episode reward: -10.76394577455946, agent episode reward: [2.1411614391301708, -12.905107213689629], time: 126.509
steps: 749975, episodes: 30000, mean episode reward: -9.677568973904327, agent episode reward: [-0.046619115764946456, -9.630949858139378], time: 110.919
steps: 774975, episodes: 31000, mean episode reward: -10.426906491227077, agent episode reward: [2.223736918044001, -12.650643409271076], time: 76.845
steps: 799975, episodes: 32000, mean episode reward: -10.420790111501784, agent episode reward: [0.9359340439543395, -11.356724155456124], time: 77.092
steps: 824975, episodes: 33000, mean episode reward: -10.096823299479999, agent episode reward: [0.8478221358067588, -10.944645435286759], time: 79.797
steps: 849975, episodes: 34000, mean episode reward: -10.024772159114407, agent episode reward: [1.6075023833579005, -11.632274542472308], time: 77.452
steps: 874975, episodes: 35000, mean episode reward: -10.061528821609704, agent episode reward: [-1.6858102536541, -8.375718567955602], time: 77.308
steps: 899975, episodes: 36000, mean episode reward: -9.822321377241012, agent episode reward: [-1.996707736012742, -7.82561364122827], time: 78.428
steps: 924975, episodes: 37000, mean episode reward: -10.166309859064988, agent episode reward: [-2.980016234805188, -7.186293624259799], time: 75.599
steps: 949975, episodes: 38000, mean episode reward: -9.858710072573267, agent episode reward: [-1.4941116954081306, -8.364598377165137], time: 73.892
steps: 974975, episodes: 39000, mean episode reward: -9.956580404681302, agent episode reward: [0.004401975263853529, -9.960982379945158], time: 74.376
steps: 999975, episodes: 40000, mean episode reward: -9.898091760707487, agent episode reward: [-0.9909224685694896, -8.907169292137999], time: 74.094
steps: 1024975, episodes: 41000, mean episode reward: -9.937896202444062, agent episode reward: [-2.5849494327033704, -7.352946769740692], time: 76.799
steps: 1049975, episodes: 42000, mean episode reward: -10.045312285079318, agent episode reward: [-2.238840537654438, -7.80647174742488], time: 75.306
steps: 1074975, episodes: 43000, mean episode reward: -10.245714317412807, agent episode reward: [-3.077640583170606, -7.168073734242202], time: 75.567
steps: 1099975, episodes: 44000, mean episode reward: -10.317372365381166, agent episode reward: [-2.7994216505739398, -7.517950714807227], time: 77.348
steps: 1124975, episodes: 45000, mean episode reward: -10.540498623371638, agent episode reward: [-2.2640945128909915, -8.276404110480648], time: 74.812
steps: 1149975, episodes: 46000, mean episode reward: -10.73975265103563, agent episode reward: [-2.258807270688276, -8.480945380347352], time: 75.492
steps: 1174975, episodes: 47000, mean episode reward: -10.11437677695439, agent episode reward: [-0.9197108849152072, -9.194665892039184], time: 77.431
steps: 1199975, episodes: 48000, mean episode reward: -9.889163954888318, agent episode reward: [0.14091334599140018, -10.030077300879716], time: 75.169
steps: 1224975, episodes: 49000, mean episode reward: -9.697792637788003, agent episode reward: [-0.43512158538427126, -9.262671052403732], time: 74.551
steps: 1249975, episodes: 50000, mean episode reward: -9.092531243264446, agent episode reward: [0.6259986792110476, -9.718529922475494], time: 75.469
steps: 1274975, episodes: 51000, mean episode reward: -9.269746058552224, agent episode reward: [0.7046139809207052, -9.974360039472929], time: 75.719
steps: 1299975, episodes: 52000, mean episode reward: -9.332620224954814, agent episode reward: [0.26268576892958734, -9.595305993884402], time: 75.535
steps: 1324975, episodes: 53000, mean episode reward: -9.168075893073812, agent episode reward: [-0.6149705750213299, -8.553105318052483], time: 75.573
steps: 1349975, episodes: 54000, mean episode reward: -9.308811420012303, agent episode reward: [-2.174298706293533, -7.134512713718769], time: 75.27
steps: 1374975, episodes: 55000, mean episode reward: -8.984144638716819, agent episode reward: [-2.0421255765232016, -6.942019062193617], time: 81.389
steps: 1399975, episodes: 56000, mean episode reward: -9.434889067884557, agent episode reward: [-2.4270821528313498, -7.007806915053208], time: 75.762
steps: 1424975, episodes: 57000, mean episode reward: -9.277861072266038, agent episode reward: [-2.161918880969456, -7.115942191296583], time: 74.191
steps: 1449975, episodes: 58000, mean episode reward: -9.239725055000802, agent episode reward: [-2.1853452706707714, -7.054379784330032], time: 74.039
steps: 1474975, episodes: 59000, mean episode reward: -9.5097596001535, agent episode reward: [-2.176240774860878, -7.33351882529262], time: 72.646
steps: 1499975, episodes: 60000, mean episode reward: -9.511722236118336, agent episode reward: [-2.4935663998609967, -7.018155836257338], time: 73.448
