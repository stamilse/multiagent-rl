python train.py --scenario simple_push --num-adversaries 1
2018-10-23 10:58:37.224369: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -26.440095913668955, agent episode reward: [0.4382548136868161, -26.878350727355773], time: 20.475
/anaconda2/envs/py36/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/anaconda2/envs/py36/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
Running avgs for agent 0: q_loss: nan, q_loss2: nan, p_loss: nan, mean_rew: nan, var_rew: nan
Running avgs for agent 1: q_loss: nan, q_loss2: nan, p_loss: nan, mean_rew: nan, var_rew: nan
steps: 49975, episodes: 2000, mean episode reward: -20.703296509084904, agent episode reward: [-4.25298816477673, -16.45030834430817], time: 34.731
Running avgs for agent 0: q_loss: 0.9044933915138245, q_loss2: -0.9006969332695007, p_loss: -0.08970381319522858, mean_rew: -0.0083363698819225, var_rew: 0.593975209980207
Running avgs for agent 1: q_loss: 0.5845112800598145, q_loss2: -0.5787656903266907, p_loss: 1.5728001594543457, mean_rew: -1.0290561419527677, var_rew: 0.3883058006950266
steps: 74975, episodes: 3000, mean episode reward: -10.918511590977888, agent episode reward: [-3.6980438534145517, -7.220467737563336], time: 37.055
Running avgs for agent 0: q_loss: 1.0863416194915771, q_loss2: -1.0861289501190186, p_loss: 0.06508894264698029, mean_rew: -0.09366925881770928, var_rew: 0.4408789599156508
Running avgs for agent 1: q_loss: 1.1088284254074097, q_loss2: -1.1087087392807007, p_loss: 2.341914176940918, mean_rew: -0.7566328509514587, var_rew: 0.42257221380643795
steps: 99975, episodes: 4000, mean episode reward: -8.845148162908691, agent episode reward: [-2.153981028154666, -6.691167134754025], time: 35.876
Running avgs for agent 0: q_loss: 1.1632107496261597, q_loss2: -1.161507487297058, p_loss: 0.16535937786102295, mean_rew: -0.09753758326046374, var_rew: 0.3478643271560667
Running avgs for agent 1: q_loss: 1.497857928276062, q_loss2: -1.497410774230957, p_loss: 2.980161666870117, mean_rew: -0.616621854622193, var_rew: 0.39791624093803973
steps: 124975, episodes: 5000, mean episode reward: -8.456375570704436, agent episode reward: [-1.6405107478250913, -6.815864822879345], time: 31.454
Running avgs for agent 0: q_loss: 1.2387208938598633, q_loss2: -1.2341829538345337, p_loss: 0.25003328919410706, mean_rew: -0.0940086435347416, var_rew: 0.295672467301451
Running avgs for agent 1: q_loss: 1.784380316734314, q_loss2: -1.7827656269073486, p_loss: 3.508915424346924, mean_rew: -0.5411393476817858, var_rew: 0.3703878822424755
steps: 149975, episodes: 6000, mean episode reward: -8.85919997048381, agent episode reward: [-1.673262057414157, -7.185937913069653], time: 31.478
Running avgs for agent 0: q_loss: 1.3404408693313599, q_loss2: -1.3330069780349731, p_loss: 0.30509883165359497, mean_rew: -0.08776333388271086, var_rew: 0.2639776813712971
Running avgs for agent 1: q_loss: 1.9737260341644287, q_loss2: -1.9706261157989502, p_loss: 3.963778257369995, mean_rew: -0.4926142349716106, var_rew: 0.3388386805522397
steps: 174975, episodes: 7000, mean episode reward: -8.613014189884098, agent episode reward: [-0.9590708176276507, -7.653943372256446], time: 31.561
Running avgs for agent 0: q_loss: 1.449195146560669, q_loss2: -1.4386464357376099, p_loss: 0.33021092414855957, mean_rew: -0.08251489722758285, var_rew: 0.24162111248508794
Running avgs for agent 1: q_loss: 2.127089738845825, q_loss2: -2.1229474544525146, p_loss: 4.407107830047607, mean_rew: -0.46182046032544133, var_rew: 0.3146974085710908
steps: 199975, episodes: 8000, mean episode reward: -8.725200568103839, agent episode reward: [-0.8520392853865248, -7.873161282717316], time: 31.413
Running avgs for agent 0: q_loss: 1.5557329654693604, q_loss2: -1.542686104774475, p_loss: 0.33098268508911133, mean_rew: -0.07575217425511706, var_rew: 0.22547444471205744
Running avgs for agent 1: q_loss: 2.2840282917022705, q_loss2: -2.2783243656158447, p_loss: 4.848236560821533, mean_rew: -0.44277091297398513, var_rew: 0.2970436194879165
steps: 224975, episodes: 9000, mean episode reward: -8.603227652731352, agent episode reward: [-0.501557823633103, -8.101669829098249], time: 31.188
Running avgs for agent 0: q_loss: 1.679746150970459, q_loss2: -1.6657606363296509, p_loss: 0.3328809142112732, mean_rew: -0.07258167156762534, var_rew: 0.21319041295375324
Running avgs for agent 1: q_loss: 2.439805746078491, q_loss2: -2.433145046234131, p_loss: 5.27008581161499, mean_rew: -0.4280402994817009, var_rew: 0.28295650501839525
steps: 249975, episodes: 10000, mean episode reward: -8.971118374320964, agent episode reward: [-0.8845320133374641, -8.0865863609835], time: 35.851
Running avgs for agent 0: q_loss: 1.78657865524292, q_loss2: -1.7683652639389038, p_loss: 0.3323538601398468, mean_rew: -0.06747455416278293, var_rew: 0.20399715114741127
Running avgs for agent 1: q_loss: 2.5677950382232666, q_loss2: -2.5610454082489014, p_loss: 5.670790195465088, mean_rew: -0.41589752126869656, var_rew: 0.27004870625280764
steps: 274975, episodes: 11000, mean episode reward: -8.525614422702779, agent episode reward: [-0.3076874492954211, -8.217926973407357], time: 34.884
Running avgs for agent 0: q_loss: 1.8839759826660156, q_loss2: -1.8650295734405518, p_loss: 0.2949569821357727, mean_rew: -0.06304120766393703, var_rew: 0.19481145132488922
Running avgs for agent 1: q_loss: 2.678553581237793, q_loss2: -2.6705987453460693, p_loss: 6.066473960876465, mean_rew: -0.4088989690641574, var_rew: 0.25850122248009993
steps: 299975, episodes: 12000, mean episode reward: -8.861943155893623, agent episode reward: [-0.4345306916588662, -8.427412464234754], time: 36.238
Running avgs for agent 0: q_loss: 2.0098156929016113, q_loss2: -1.9904249906539917, p_loss: 0.2577798366546631, mean_rew: -0.05810010944819081, var_rew: 0.18996818252200023
Running avgs for agent 1: q_loss: 2.8005969524383545, q_loss2: -2.792590618133545, p_loss: 6.421536922454834, mean_rew: -0.40169369841141844, var_rew: 0.24861345522171582
steps: 324975, episodes: 13000, mean episode reward: -8.57915705470174, agent episode reward: [-0.040477431118352086, -8.538679623583386], time: 34.324
Running avgs for agent 0: q_loss: 2.098184585571289, q_loss2: -2.0757367610931396, p_loss: 0.22360312938690186, mean_rew: -0.055122261339399216, var_rew: 0.18347911107404374
Running avgs for agent 1: q_loss: 2.89856219291687, q_loss2: -2.8891005516052246, p_loss: 6.785953998565674, mean_rew: -0.3966778545670172, var_rew: 0.2397969753392299
steps: 349975, episodes: 14000, mean episode reward: -8.990290349886633, agent episode reward: [-0.26150898161073144, -8.7287813682759], time: 33.318
Running avgs for agent 0: q_loss: 2.1923468112945557, q_loss2: -2.1693122386932373, p_loss: 0.1471618264913559, mean_rew: -0.051022025727466516, var_rew: 0.177623271136789
Running avgs for agent 1: q_loss: 3.0229618549346924, q_loss2: -3.01492977142334, p_loss: 7.137165069580078, mean_rew: -0.39356334652423464, var_rew: 0.2333017504560994
steps: 374975, episodes: 15000, mean episode reward: -8.89145489011257, agent episode reward: [-0.004772947787780766, -8.886681942324788], time: 32.847
Running avgs for agent 0: q_loss: 2.3104889392852783, q_loss2: -2.2865688800811768, p_loss: 0.06404624879360199, mean_rew: -0.04785703347928019, var_rew: 0.1746195615968703
Running avgs for agent 1: q_loss: 3.1356201171875, q_loss2: -3.1258704662323, p_loss: 7.429853439331055, mean_rew: -0.3896959198078799, var_rew: 0.2275956273401089
steps: 399975, episodes: 16000, mean episode reward: -9.235865262376102, agent episode reward: [0.03834864035383014, -9.274213902729931], time: 32.781
Running avgs for agent 0: q_loss: 2.39401912689209, q_loss2: -2.368434190750122, p_loss: -0.00561123201623559, mean_rew: -0.04510032383018523, var_rew: 0.1698872679207816
Running avgs for agent 1: q_loss: 3.2395853996276855, q_loss2: -3.2287323474884033, p_loss: 7.729554653167725, mean_rew: -0.38792703397233613, var_rew: 0.2216439869966432
steps: 424975, episodes: 17000, mean episode reward: -9.008176674769947, agent episode reward: [0.42071743435883524, -9.428894109128782], time: 32.349
Running avgs for agent 0: q_loss: 2.49995756149292, q_loss2: -2.4753732681274414, p_loss: -0.1117006242275238, mean_rew: -0.04210918848037357, var_rew: 0.16696936432188889
Running avgs for agent 1: q_loss: 3.350691795349121, q_loss2: -3.3406119346618652, p_loss: 8.008512496948242, mean_rew: -0.3880779566556662, var_rew: 0.21637710052540388
steps: 449975, episodes: 18000, mean episode reward: -8.989221133972867, agent episode reward: [0.6874948380083132, -9.676715971981183], time: 33.616
Running avgs for agent 0: q_loss: 2.609743118286133, q_loss2: -2.584044933319092, p_loss: -0.23310421407222748, mean_rew: -0.03738960267692525, var_rew: 0.16458757326618376
Running avgs for agent 1: q_loss: 3.470242738723755, q_loss2: -3.459505796432495, p_loss: 8.28797435760498, mean_rew: -0.3884535270544492, var_rew: 0.21238540414562262
steps: 474975, episodes: 19000, mean episode reward: -9.306162013730315, agent episode reward: [0.6834863860368844, -9.989648399767198], time: 34.342
Running avgs for agent 0: q_loss: 2.672905683517456, q_loss2: -2.647932529449463, p_loss: -0.32209643721580505, mean_rew: -0.033591128005238426, var_rew: 0.16128983985954573
Running avgs for agent 1: q_loss: 3.586832046508789, q_loss2: -3.5767343044281006, p_loss: 8.553508758544922, mean_rew: -0.3891531653914096, var_rew: 0.20876994083642195
steps: 499975, episodes: 20000, mean episode reward: -9.089875287560034, agent episode reward: [1.0169692037422484, -10.106844491302283], time: 34.773
Running avgs for agent 0: q_loss: 2.8057405948638916, q_loss2: -2.780277967453003, p_loss: -0.3705213665962219, mean_rew: -0.03124487209898177, var_rew: 0.1609684256306648
Running avgs for agent 1: q_loss: 3.650392532348633, q_loss2: -3.639688730239868, p_loss: 8.751774787902832, mean_rew: -0.38743350323700965, var_rew: 0.2030702611320994
steps: 524975, episodes: 21000, mean episode reward: -9.317779528957356, agent episode reward: [0.821910449097803, -10.13968997805516], time: 31.609
Running avgs for agent 0: q_loss: 2.9183549880981445, q_loss2: -2.893188953399658, p_loss: -0.480747252702713, mean_rew: -0.026996757646426, var_rew: 0.16054374616982853
Running avgs for agent 1: q_loss: 3.744511604309082, q_loss2: -3.7348732948303223, p_loss: 9.010523796081543, mean_rew: -0.38850361659899035, var_rew: 0.19907252663364328
steps: 549975, episodes: 22000, mean episode reward: -9.165786698029887, agent episode reward: [0.9174037363469124, -10.0831904343768], time: 31.171
Running avgs for agent 0: q_loss: 3.0037524700164795, q_loss2: -2.977416515350342, p_loss: -0.5260864496231079, mean_rew: -0.0253922613684406, var_rew: 0.15782486262477488
Running avgs for agent 1: q_loss: 3.849137544631958, q_loss2: -3.838886022567749, p_loss: 9.200357437133789, mean_rew: -0.38974484986804664, var_rew: 0.19630597060087268
steps: 574975, episodes: 23000, mean episode reward: -8.989001180684177, agent episode reward: [1.4465941286338926, -10.435595309318071], time: 31.723
Running avgs for agent 0: q_loss: 3.0960958003997803, q_loss2: -3.0694854259490967, p_loss: -0.5948277115821838, mean_rew: -0.02196205888570043, var_rew: 0.15622017379899922
Running avgs for agent 1: q_loss: 3.92549991607666, q_loss2: -3.915530204772949, p_loss: 9.3770112991333, mean_rew: -0.38926454891221823, var_rew: 0.1918868198179376
steps: 599975, episodes: 24000, mean episode reward: -9.198109362011731, agent episode reward: [1.431067536431995, -10.629176898443726], time: 32.279
Running avgs for agent 0: q_loss: 3.193986415863037, q_loss2: -3.1687095165252686, p_loss: -0.6915730834007263, mean_rew: -0.018541508130679653, var_rew: 0.15513737416562726
Running avgs for agent 1: q_loss: 4.037933826446533, q_loss2: -4.026759147644043, p_loss: 9.576865196228027, mean_rew: -0.39173944016283546, var_rew: 0.1900143554859197
steps: 624975, episodes: 25000, mean episode reward: -9.703837152742862, agent episode reward: [1.2325853546671524, -10.936422507410017], time: 31.135
Running avgs for agent 0: q_loss: 3.330688714981079, q_loss2: -3.3061187267303467, p_loss: -0.7666300535202026, mean_rew: -0.016056443447349692, var_rew: 0.15537193383731135
Running avgs for agent 1: q_loss: 4.159251689910889, q_loss2: -4.149102210998535, p_loss: 9.789144515991211, mean_rew: -0.39413103746252093, var_rew: 0.1881127408694821
steps: 649975, episodes: 26000, mean episode reward: -9.402410350712499, agent episode reward: [1.6940390093648212, -11.09644936007732], time: 32.041
Running avgs for agent 0: q_loss: 3.3814141750335693, q_loss2: -3.357167959213257, p_loss: -0.8107436299324036, mean_rew: -0.011961659707260033, var_rew: 0.1529011730036225
Running avgs for agent 1: q_loss: 4.190558433532715, q_loss2: -4.178545951843262, p_loss: 9.931673049926758, mean_rew: -0.39544824832260694, var_rew: 0.18391339818355032
steps: 674975, episodes: 27000, mean episode reward: -9.41969129142523, agent episode reward: [1.6632642562413649, -11.082955547666595], time: 31.611
Running avgs for agent 0: q_loss: 3.5161659717559814, q_loss2: -3.49165940284729, p_loss: -0.90312260389328, mean_rew: -0.009041992444653625, var_rew: 0.1529553506311385
Running avgs for agent 1: q_loss: 4.278201580047607, q_loss2: -4.266292095184326, p_loss: 10.070443153381348, mean_rew: -0.39787210084117947, var_rew: 0.18169241381934143
steps: 699975, episodes: 28000, mean episode reward: -9.351694609188144, agent episode reward: [1.5460581622606255, -10.89775277144877], time: 31.653
Running avgs for agent 0: q_loss: 3.6115787029266357, q_loss2: -3.5857675075531006, p_loss: -0.9920256733894348, mean_rew: -0.005147858138752006, var_rew: 0.15142638469640454
Running avgs for agent 1: q_loss: 4.4045209884643555, q_loss2: -4.392772197723389, p_loss: 10.231468200683594, mean_rew: -0.3987942766006667, var_rew: 0.1807355949374662
steps: 724975, episodes: 29000, mean episode reward: -9.622620844255232, agent episode reward: [1.6089302348814976, -11.23155107913673], time: 32.861
Running avgs for agent 0: q_loss: 3.69174861907959, q_loss2: -3.666491985321045, p_loss: -1.0494107007980347, mean_rew: -0.004494183632122903, var_rew: 0.1503140375056375
Running avgs for agent 1: q_loss: 4.475675106048584, q_loss2: -4.463608741760254, p_loss: 10.353981971740723, mean_rew: -0.4001375772118887, var_rew: 0.17808743746694183
steps: 749975, episodes: 30000, mean episode reward: -9.869126197706569, agent episode reward: [1.2593821400112888, -11.128508337717857], time: 35.869
Running avgs for agent 0: q_loss: 3.853456497192383, q_loss2: -3.8284659385681152, p_loss: -1.0996108055114746, mean_rew: -0.0025589823934716836, var_rew: 0.15161691721322082
Running avgs for agent 1: q_loss: 4.62680721282959, q_loss2: -4.614563465118408, p_loss: 10.505147933959961, mean_rew: -0.40406371578362166, var_rew: 0.1779072927602934
steps: 774975, episodes: 31000, mean episode reward: -9.704726621815494, agent episode reward: [1.3627065716942297, -11.067433193509725], time: 35.113
Running avgs for agent 0: q_loss: 3.884551525115967, q_loss2: -3.8603198528289795, p_loss: -1.2025361061096191, mean_rew: -0.0002482177034513814, var_rew: 0.14876421822023883
Running avgs for agent 1: q_loss: 4.629052639007568, q_loss2: -4.617180347442627, p_loss: 10.56856632232666, mean_rew: -0.4040067947307598, var_rew: 0.1736995183102967
steps: 799975, episodes: 32000, mean episode reward: -9.607445103198602, agent episode reward: [1.6135868612739148, -11.221031964472516], time: 34.965
Running avgs for agent 0: q_loss: 3.9741945266723633, q_loss2: -3.9502079486846924, p_loss: -1.2836474180221558, mean_rew: 0.0020913050478305736, var_rew: 0.14817850669180002
Running avgs for agent 1: q_loss: 4.747280120849609, q_loss2: -4.7342000007629395, p_loss: 10.695293426513672, mean_rew: -0.4053450024175505, var_rew: 0.17283920315604878
steps: 824975, episodes: 33000, mean episode reward: -9.654562191407832, agent episode reward: [1.3220820496128658, -10.976644241020699], time: 33.483
Running avgs for agent 0: q_loss: 4.065563201904297, q_loss2: -4.0409770011901855, p_loss: -1.3053909540176392, mean_rew: 0.0029231122762487065, var_rew: 0.14724520746296538
Running avgs for agent 1: q_loss: 4.724883079528809, q_loss2: -4.712357521057129, p_loss: 10.792972564697266, mean_rew: -0.4048801355312713, var_rew: 0.16821861457169415
steps: 849975, episodes: 34000, mean episode reward: -9.995056841293554, agent episode reward: [1.3988134162097783, -11.39387025750333], time: 34.346
Running avgs for agent 0: q_loss: 4.159802436828613, q_loss2: -4.13610315322876, p_loss: -1.3531951904296875, mean_rew: 0.00446666056080936, var_rew: 0.1462037374547018
Running avgs for agent 1: q_loss: 4.881250858306885, q_loss2: -4.868599891662598, p_loss: 10.903607368469238, mean_rew: -0.4066110939717808, var_rew: 0.16865239852649963
steps: 874975, episodes: 35000, mean episode reward: -9.786806503378253, agent episode reward: [1.5550174091897888, -11.34182391256804], time: 43.553
Running avgs for agent 0: q_loss: 4.239134311676025, q_loss2: -4.214864253997803, p_loss: -1.4077013731002808, mean_rew: 0.0064276815627083966, var_rew: 0.14580451904679093
Running avgs for agent 1: q_loss: 5.01116418838501, q_loss2: -4.998236179351807, p_loss: 11.036638259887695, mean_rew: -0.4086729467574877, var_rew: 0.16799473511919327
steps: 899975, episodes: 36000, mean episode reward: -10.014817548531912, agent episode reward: [1.3773655644926754, -11.392183113024585], time: 38.542
Running avgs for agent 0: q_loss: 4.382894039154053, q_loss2: -4.358447551727295, p_loss: -1.4683325290679932, mean_rew: 0.007403226178432737, var_rew: 0.14631007675661095
Running avgs for agent 1: q_loss: 5.14515495300293, q_loss2: -5.132237434387207, p_loss: 11.151925086975098, mean_rew: -0.41020251531957563, var_rew: 0.16771953617240115
steps: 924975, episodes: 37000, mean episode reward: -9.71793936155415, agent episode reward: [1.5621771710478807, -11.280116532602031], time: 38.123
Running avgs for agent 0: q_loss: 4.436898231506348, q_loss2: -4.4135565757751465, p_loss: -1.5260483026504517, mean_rew: 0.009912175879322405, var_rew: 0.14445871011510367
Running avgs for agent 1: q_loss: 5.185849189758301, q_loss2: -5.171875953674316, p_loss: 11.215856552124023, mean_rew: -0.4110350495867785, var_rew: 0.16567947700312613
steps: 949975, episodes: 38000, mean episode reward: -9.704598397099751, agent episode reward: [1.5709295978408455, -11.275527994940596], time: 32.577
Running avgs for agent 0: q_loss: 4.54561185836792, q_loss2: -4.522381782531738, p_loss: -1.5998644828796387, mean_rew: 0.012064105922375085, var_rew: 0.1445348726981469
Running avgs for agent 1: q_loss: 5.233056545257568, q_loss2: -5.2197346687316895, p_loss: 11.25232219696045, mean_rew: -0.41236593101675295, var_rew: 0.16379452351366394
steps: 974975, episodes: 39000, mean episode reward: -9.695251139313129, agent episode reward: [1.721050748822581, -11.41630188813571], time: 31.269
Running avgs for agent 0: q_loss: 4.610483169555664, q_loss2: -4.587494373321533, p_loss: -1.5841103792190552, mean_rew: 0.012735705126980091, var_rew: 0.14304316280115523
Running avgs for agent 1: q_loss: 5.255702972412109, q_loss2: -5.243265628814697, p_loss: 11.262862205505371, mean_rew: -0.41243440936276216, var_rew: 0.1613022005355374
steps: 999975, episodes: 40000, mean episode reward: -10.07496425153357, agent episode reward: [0.9630708172516693, -11.038035068785241], time: 31.067
Running avgs for agent 0: q_loss: 4.707943439483643, q_loss2: -4.685198783874512, p_loss: -1.5667016506195068, mean_rew: 0.012891455267037971, var_rew: 0.14314391090372425
Running avgs for agent 1: q_loss: 5.409707546234131, q_loss2: -5.397019386291504, p_loss: 11.337275505065918, mean_rew: -0.4145147729371747, var_rew: 0.16182639167780394
steps: 1024975, episodes: 41000, mean episode reward: -9.993370199698568, agent episode reward: [1.4645028440937518, -11.457873043792318], time: 31.4
Running avgs for agent 0: q_loss: 4.638210773468018, q_loss2: -4.615111351013184, p_loss: -1.5246286392211914, mean_rew: 0.01482011600767634, var_rew: 0.13941240322724274
Running avgs for agent 1: q_loss: 5.110563278198242, q_loss2: -5.098613262176514, p_loss: 11.18367862701416, mean_rew: -0.4069123528706451, var_rew: 0.15283985977896894
steps: 1049975, episodes: 42000, mean episode reward: -9.896691203634619, agent episode reward: [1.23244352131699, -11.129134724951607], time: 30.975
Running avgs for agent 0: q_loss: 4.106650352478027, q_loss2: -4.0868000984191895, p_loss: -1.3968249559402466, mean_rew: 0.015740076879155143, var_rew: 0.12629547838109234
Running avgs for agent 1: q_loss: 4.433109283447266, q_loss2: -4.4210524559021, p_loss: 10.881896018981934, mean_rew: -0.3935617739569403, var_rew: 0.13658293180983863
steps: 1074975, episodes: 43000, mean episode reward: -9.965003482549047, agent episode reward: [1.2127679812912906, -11.177771463840338], time: 31.183
Running avgs for agent 0: q_loss: 4.062038421630859, q_loss2: -4.042713165283203, p_loss: -1.3993926048278809, mean_rew: 0.02315409292416732, var_rew: 0.12395738242277221
Running avgs for agent 1: q_loss: 4.350156307220459, q_loss2: -4.340655326843262, p_loss: 10.854148864746094, mean_rew: -0.39612991247228685, var_rew: 0.13355221264222866
steps: 1099975, episodes: 44000, mean episode reward: -9.898219588807848, agent episode reward: [1.4841812762638547, -11.382400865071704], time: 30.924
Running avgs for agent 0: q_loss: 4.093947887420654, q_loss2: -4.074537754058838, p_loss: -1.431053876876831, mean_rew: 0.027873652261662305, var_rew: 0.12345606073265898
Running avgs for agent 1: q_loss: 4.297609329223633, q_loss2: -4.286398410797119, p_loss: 10.846437454223633, mean_rew: -0.3999275089080827, var_rew: 0.13126089178266942
steps: 1124975, episodes: 45000, mean episode reward: -9.998649968433813, agent episode reward: [1.2820352043064855, -11.2806851727403], time: 31.805
Running avgs for agent 0: q_loss: 4.130451202392578, q_loss2: -4.1118693351745605, p_loss: -1.4580498933792114, mean_rew: 0.029790294417852102, var_rew: 0.12281407727947964
Running avgs for agent 1: q_loss: 4.288664817810059, q_loss2: -4.278756618499756, p_loss: 10.833206176757812, mean_rew: -0.4048036808375803, var_rew: 0.13016838886838583
steps: 1149975, episodes: 46000, mean episode reward: -9.661459359890891, agent episode reward: [1.2655074560402335, -10.926966815931124], time: 31.3
Running avgs for agent 0: q_loss: 4.230476379394531, q_loss2: -4.211511611938477, p_loss: -1.5425828695297241, mean_rew: 0.032641359152713864, var_rew: 0.12345427261190235
Running avgs for agent 1: q_loss: 4.353994846343994, q_loss2: -4.3446502685546875, p_loss: 10.864152908325195, mean_rew: -0.40927249236954827, var_rew: 0.12984865762624548
steps: 1174975, episodes: 47000, mean episode reward: -9.991486910023927, agent episode reward: [1.62292509016037, -11.614412000184297], time: 31.182
Running avgs for agent 0: q_loss: 4.294669151306152, q_loss2: -4.275816440582275, p_loss: -1.5990768671035767, mean_rew: 0.03524915371361904, var_rew: 0.1232890141427696
Running avgs for agent 1: q_loss: 4.2415547370910645, q_loss2: -4.232687473297119, p_loss: 10.867349624633789, mean_rew: -0.4118064979591245, var_rew: 0.12634543569573328
steps: 1199975, episodes: 48000, mean episode reward: -9.851005652843899, agent episode reward: [1.4863877432830928, -11.337393396126993], time: 31.296
Running avgs for agent 0: q_loss: 4.338023662567139, q_loss2: -4.319186687469482, p_loss: -1.675937533378601, mean_rew: 0.03793439125410728, var_rew: 0.12286696010022077
Running avgs for agent 1: q_loss: 4.404170036315918, q_loss2: -4.395554065704346, p_loss: 10.958845138549805, mean_rew: -0.4177182641070105, var_rew: 0.12780480986692672
steps: 1224975, episodes: 49000, mean episode reward: -9.619473194208393, agent episode reward: [1.9814442856760743, -11.600917479884467], time: 31.088
Running avgs for agent 0: q_loss: 4.436901569366455, q_loss2: -4.418357849121094, p_loss: -1.7722697257995605, mean_rew: 0.04064567432080758, var_rew: 0.12311770060689556
Running avgs for agent 1: q_loss: 4.364416599273682, q_loss2: -4.3557281494140625, p_loss: 11.008225440979004, mean_rew: -0.4203294815490841, var_rew: 0.12557057672176994
steps: 1249975, episodes: 50000, mean episode reward: -9.750006194075562, agent episode reward: [1.8335195841132126, -11.583525778188775], time: 31.144
Running avgs for agent 0: q_loss: 4.542961597442627, q_loss2: -4.524821758270264, p_loss: -1.921829342842102, mean_rew: 0.042895333823418716, var_rew: 0.12340286185283725
Running avgs for agent 1: q_loss: 4.37106466293335, q_loss2: -4.363076686859131, p_loss: 11.083677291870117, mean_rew: -0.42332437656271554, var_rew: 0.12405607838820135
steps: 1274975, episodes: 51000, mean episode reward: -9.473210068072458, agent episode reward: [2.3762038219620574, -11.849413890034514], time: 31.266
Running avgs for agent 0: q_loss: 4.617686748504639, q_loss2: -4.600200176239014, p_loss: -2.021430015563965, mean_rew: 0.04488697327621725, var_rew: 0.12358341364696367
Running avgs for agent 1: q_loss: 4.3504157066345215, q_loss2: -4.342967987060547, p_loss: 11.157761573791504, mean_rew: -0.4261165307696362, var_rew: 0.12243104660762053
steps: 1299975, episodes: 52000, mean episode reward: -9.785649581786936, agent episode reward: [2.271278980061345, -12.056928561848279], time: 31.025
Running avgs for agent 0: q_loss: 4.743057727813721, q_loss2: -4.725520610809326, p_loss: -2.1449978351593018, mean_rew: 0.048351704493522815, var_rew: 0.12422204971713804
Running avgs for agent 1: q_loss: 4.410376071929932, q_loss2: -4.403043746948242, p_loss: 11.249471664428711, mean_rew: -0.43099187069770706, var_rew: 0.12229859192024864
steps: 1324975, episodes: 53000, mean episode reward: -9.677871997424917, agent episode reward: [2.3472449502513095, -12.025116947676226], time: 31.153
Running avgs for agent 0: q_loss: 4.769472122192383, q_loss2: -4.752737522125244, p_loss: -2.2422289848327637, mean_rew: 0.05145690364122694, var_rew: 0.1233699210846864
Running avgs for agent 1: q_loss: 4.479123115539551, q_loss2: -4.471312522888184, p_loss: 11.337931632995605, mean_rew: -0.43502639251391567, var_rew: 0.12225723969410471
steps: 1349975, episodes: 54000, mean episode reward: -9.828475253479061, agent episode reward: [2.1529483334763655, -11.981423586955424], time: 31.147
Running avgs for agent 0: q_loss: 4.872016429901123, q_loss2: -4.854724407196045, p_loss: -2.282722234725952, mean_rew: 0.05308838499132972, var_rew: 0.12374386979410605
Running avgs for agent 1: q_loss: 4.4499993324279785, q_loss2: -4.441938400268555, p_loss: 11.39222240447998, mean_rew: -0.4380371624528899, var_rew: 0.12064161538070368
steps: 1374975, episodes: 55000, mean episode reward: -9.764950544964007, agent episode reward: [2.459303268995638, -12.224253813959645], time: 31.306
Running avgs for agent 0: q_loss: 4.972935676574707, q_loss2: -4.955798625946045, p_loss: -2.3365981578826904, mean_rew: 0.05629697116123924, var_rew: 0.12385126211120805
Running avgs for agent 1: q_loss: 4.531515598297119, q_loss2: -4.525124549865723, p_loss: 11.47208023071289, mean_rew: -0.44237425243353484, var_rew: 0.12071244265281396
steps: 1399975, episodes: 56000, mean episode reward: -9.518365600259415, agent episode reward: [2.6998666738608748, -12.21823227412029], time: 32.399
Running avgs for agent 0: q_loss: 5.141749858856201, q_loss2: -5.125129699707031, p_loss: -2.3763420581817627, mean_rew: 0.05797217537880475, var_rew: 0.1250829465261715
Running avgs for agent 1: q_loss: 4.43023157119751, q_loss2: -4.423213958740234, p_loss: 11.497536659240723, mean_rew: -0.44289835269087063, var_rew: 0.11792085470833348
steps: 1424975, episodes: 57000, mean episode reward: -9.901845568671822, agent episode reward: [2.349712601123552, -12.251558169795375], time: 33.057
Running avgs for agent 0: q_loss: 5.218566417694092, q_loss2: -5.201420783996582, p_loss: -2.4060068130493164, mean_rew: 0.061150843852081735, var_rew: 0.12487844638840478
Running avgs for agent 1: q_loss: 4.479763507843018, q_loss2: -4.472460746765137, p_loss: 11.579302787780762, mean_rew: -0.44751819263387077, var_rew: 0.11744314028434633
steps: 1449975, episodes: 58000, mean episode reward: -9.557460790743804, agent episode reward: [2.504378512432431, -12.061839303176235], time: 32.513
Running avgs for agent 0: q_loss: 5.316605091094971, q_loss2: -5.299772262573242, p_loss: -2.4435415267944336, mean_rew: 0.06344534995303984, var_rew: 0.1250102740709156
Running avgs for agent 1: q_loss: 4.622341632843018, q_loss2: -4.615479469299316, p_loss: 11.618897438049316, mean_rew: -0.4508610939744855, var_rew: 0.11841973140002324
steps: 1474975, episodes: 59000, mean episode reward: -9.768757408919127, agent episode reward: [2.3706001839539024, -12.13935759287303], time: 33.663
Running avgs for agent 0: q_loss: 5.426254749298096, q_loss2: -5.409756660461426, p_loss: -2.4926867485046387, mean_rew: 0.06479380550680128, var_rew: 0.12527385577600217
Running avgs for agent 1: q_loss: 4.604839324951172, q_loss2: -4.597508907318115, p_loss: 11.671072006225586, mean_rew: -0.4514884742952931, var_rew: 0.11684604862025347
steps: 1499975, episodes: 60000, mean episode reward: -9.861079229100795, agent episode reward: [2.2386340096447035, -12.0997132387455], time: 32.81
Running avgs for agent 0: q_loss: 5.510596752166748, q_loss2: -5.49443244934082, p_loss: -2.5050435066223145, mean_rew: 0.06554811180313519, var_rew: 0.12540374431770154
Running avgs for agent 1: q_loss: 4.5850510597229, q_loss2: -4.57806396484375, p_loss: 11.748957633972168, mean_rew: -0.4536686161854843, var_rew: 0.1151774689297871
