 python train_vanila.py --scenario simple_push --num-adversaries 1
2018-10-23 11:31:48.892561: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Using good policy maddpg and adv policy maddpg
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -28.18647799141112, agent episode reward: [-1.3168829587380328, -26.86959503267309], time: 22.077
/anaconda2/envs/py36/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/anaconda2/envs/py36/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
Running avgs for agent 0: q_loss: nan, p_loss: nan, mean_rew: nan, var_rew: nan
Running avgs for agent 1: q_loss: nan, p_loss: nan, mean_rew: nan, var_rew: nan
steps: 49975, episodes: 2000, mean episode reward: -22.313058411929386, agent episode reward: [-7.894145175151188, -14.4189132367782], time: 30.585
Running avgs for agent 0: q_loss: 0.03525283560156822, p_loss: -0.10129864513874054, mean_rew: -0.14963328115857696, var_rew: 0.5822638988873832
Running avgs for agent 1: q_loss: 0.020021848380565643, p_loss: 1.3678656816482544, mean_rew: -0.9849218736864195, var_rew: 0.33752553193997104
steps: 74975, episodes: 3000, mean episode reward: -10.393179349815458, agent episode reward: [-3.365547477767672, -7.027631872047786], time: 32.161
Running avgs for agent 0: q_loss: 0.032166846096515656, p_loss: 0.14122717082500458, mean_rew: -0.1792159775092847, var_rew: 0.424146112588673
Running avgs for agent 1: q_loss: 0.007771767675876617, p_loss: 2.135852575302124, mean_rew: -0.7241378064657074, var_rew: 0.3816256258773197
steps: 99975, episodes: 4000, mean episode reward: -9.099580146273347, agent episode reward: [-2.1692366045289226, -6.930343541744424], time: 30.454
Running avgs for agent 0: q_loss: 0.05684465169906616, p_loss: 0.380936861038208, mean_rew: -0.15951781901597314, var_rew: 0.3367824557337467
Running avgs for agent 1: q_loss: 0.02712586335837841, p_loss: 2.7954344749450684, mean_rew: -0.5949726625275451, var_rew: 0.36582638127082034
steps: 124975, episodes: 5000, mean episode reward: -8.830672841117297, agent episode reward: [-1.9120758519827603, -6.918596989134538], time: 34.401
Running avgs for agent 0: q_loss: 0.10509492456912994, p_loss: 0.500662088394165, mean_rew: -0.13856113438576487, var_rew: 0.2930724767317987
Running avgs for agent 1: q_loss: 0.05507975444197655, p_loss: 3.33358097076416, mean_rew: -0.5227180662075942, var_rew: 0.33712349592876883
steps: 149975, episodes: 6000, mean episode reward: -8.91048905241934, agent episode reward: [-1.960651751244351, -6.9498373011749885], time: 35.83
Running avgs for agent 0: q_loss: 0.17579442262649536, p_loss: 0.6198859810829163, mean_rew: -0.1285499309758414, var_rew: 0.26206993366283565
Running avgs for agent 1: q_loss: 0.0911981463432312, p_loss: 3.8164560794830322, mean_rew: -0.47747701324009395, var_rew: 0.3150808086669439
steps: 174975, episodes: 7000, mean episode reward: -9.070050964039158, agent episode reward: [-2.0630525796986348, -7.006998384340524], time: 32.083
Running avgs for agent 0: q_loss: 0.25299912691116333, p_loss: 0.720586895942688, mean_rew: -0.12268301294361718, var_rew: 0.241057990176857
Running avgs for agent 1: q_loss: 0.12303750962018967, p_loss: 4.269453048706055, mean_rew: -0.4470079550269376, var_rew: 0.29798937484991855
steps: 199975, episodes: 8000, mean episode reward: -8.858341161783574, agent episode reward: [-1.6953455501860522, -7.16299561159752], time: 37.258
Running avgs for agent 0: q_loss: 0.32582688331604004, p_loss: 0.8058258295059204, mean_rew: -0.11554891765096306, var_rew: 0.2227761070668938
Running avgs for agent 1: q_loss: 0.1587703377008438, p_loss: 4.696128845214844, mean_rew: -0.4254230497643361, var_rew: 0.2820166907564501
steps: 224975, episodes: 9000, mean episode reward: -9.020442600693478, agent episode reward: [-1.8211949542705024, -7.199247646422974], time: 38.058
Running avgs for agent 0: q_loss: 0.3890064060688019, p_loss: 0.8506275415420532, mean_rew: -0.10935909868088194, var_rew: 0.2133836357358736
Running avgs for agent 1: q_loss: 0.19690865278244019, p_loss: 5.0926737785339355, mean_rew: -0.4090692383275397, var_rew: 0.27074953188548306
steps: 249975, episodes: 10000, mean episode reward: -9.237428273976679, agent episode reward: [-2.0739515766654804, -7.163476697311198], time: 31.623
Running avgs for agent 0: q_loss: 0.4559134840965271, p_loss: 0.9047821164131165, mean_rew: -0.10621715653317158, var_rew: 0.20351501039306116
Running avgs for agent 1: q_loss: 0.22697864472866058, p_loss: 5.504982948303223, mean_rew: -0.39826218203949226, var_rew: 0.26289353139657345
^[[Bsteps: 274975, episodes: 11000, mean episode reward: -9.49097806206846, agent episode reward: [-2.008398909338972, -7.482579152729489], time: 34.93
Running avgs for agent 0: q_loss: 0.5116381645202637, p_loss: 0.9461246728897095, mean_rew: -0.10325858060735839, var_rew: 0.19369602284408807
Running avgs for agent 1: q_loss: 0.24988782405853271, p_loss: 5.865879058837891, mean_rew: -0.390311176640245, var_rew: 0.2539302567475375
steps: 299975, episodes: 12000, mean episode reward: -8.988951642847722, agent episode reward: [-1.7063839245268078, -7.282567718320913], time: 33.715
Running avgs for agent 0: q_loss: 0.5675275325775146, p_loss: 0.9942927360534668, mean_rew: -0.10195078067378863, var_rew: 0.18752606920652612
Running avgs for agent 1: q_loss: 0.2743278741836548, p_loss: 6.146364212036133, mean_rew: -0.3796511024401099, var_rew: 0.24450866023679174
steps: 324975, episodes: 13000, mean episode reward: -8.952612357951086, agent episode reward: [-1.354443632001934, -7.5981687259491535], time: 31.579
Running avgs for agent 0: q_loss: 0.6128759980201721, p_loss: 0.9834864139556885, mean_rew: -0.09763029428674051, var_rew: 0.1803640940787815
Running avgs for agent 1: q_loss: 0.2924029529094696, p_loss: 6.427776336669922, mean_rew: -0.3721260304779662, var_rew: 0.23491701813102658
steps: 349975, episodes: 14000, mean episode reward: -9.071341092564438, agent episode reward: [-1.46326356031071, -7.608077532253728], time: 31.885
Running avgs for agent 0: q_loss: 0.6472017765045166, p_loss: 1.0232667922973633, mean_rew: -0.09724776747319862, var_rew: 0.17716842231149324
Running avgs for agent 1: q_loss: 0.30664077401161194, p_loss: 6.733878135681152, mean_rew: -0.3670229689725776, var_rew: 0.23128963219273788
steps: 374975, episodes: 15000, mean episode reward: -9.492761900439243, agent episode reward: [-1.6814672941864914, -7.811294606252751], time: 31.632
Running avgs for agent 0: q_loss: 0.6909728050231934, p_loss: 1.0225950479507446, mean_rew: -0.09382779533310752, var_rew: 0.17267696503691662
Running avgs for agent 1: q_loss: 0.31868690252304077, p_loss: 6.990539073944092, mean_rew: -0.3626489936579457, var_rew: 0.22561309821099695
steps: 399975, episodes: 16000, mean episode reward: -9.49118292816192, agent episode reward: [-1.649935993931179, -7.841246934230742], time: 36.575
Running avgs for agent 0: q_loss: 0.7193412184715271, p_loss: 1.0218170881271362, mean_rew: -0.09162680675702922, var_rew: 0.16775421033537197
Running avgs for agent 1: q_loss: 0.33663806319236755, p_loss: 7.22155237197876, mean_rew: -0.35884683830466124, var_rew: 0.22167224821858386
steps: 424975, episodes: 17000, mean episode reward: -9.386163966425235, agent episode reward: [-1.8320755188721833, -7.5540884475530525], time: 31.214
Running avgs for agent 0: q_loss: 0.7387548685073853, p_loss: 1.0262986421585083, mean_rew: -0.08939831740434948, var_rew: 0.16433475842544512
Running avgs for agent 1: q_loss: 0.35155028104782104, p_loss: 7.467841625213623, mean_rew: -0.3576736175036227, var_rew: 0.21908021140846307
steps: 449975, episodes: 18000, mean episode reward: -9.739160317429555, agent episode reward: [-1.7683750128518263, -7.970785304577727], time: 31.086
Running avgs for agent 0: q_loss: 0.748001217842102, p_loss: 1.072641372680664, mean_rew: -0.08958447192725051, var_rew: 0.164238973949975
Running avgs for agent 1: q_loss: 0.3496488630771637, p_loss: 7.643881320953369, mean_rew: -0.3542627982489794, var_rew: 0.2148087906413535
steps: 474975, episodes: 19000, mean episode reward: -9.309888350347915, agent episode reward: [-1.5130145325182656, -7.79687381782965], time: 30.995
Running avgs for agent 0: q_loss: 0.7738865613937378, p_loss: 1.081814169883728, mean_rew: -0.08709723295139488, var_rew: 0.1595641295196577
Running avgs for agent 1: q_loss: 0.36938291788101196, p_loss: 7.855033874511719, mean_rew: -0.35383953952653746, var_rew: 0.21244778455143723
steps: 499975, episodes: 20000, mean episode reward: -9.654760147524549, agent episode reward: [-1.7151470175152714, -7.939613130009275], time: 30.91
Running avgs for agent 0: q_loss: 0.7940423488616943, p_loss: 1.1235917806625366, mean_rew: -0.08600016604448577, var_rew: 0.15771390663576634
Running avgs for agent 1: q_loss: 0.36390820145606995, p_loss: 7.9928693771362305, mean_rew: -0.3494799092210235, var_rew: 0.2083712426729133
steps: 524975, episodes: 21000, mean episode reward: -9.639652758875913, agent episode reward: [-1.869507184978124, -7.770145573897788], time: 30.957
Running avgs for agent 0: q_loss: 0.8229902982711792, p_loss: 1.1514084339141846, mean_rew: -0.08446648861654023, var_rew: 0.15493499669295024
Running avgs for agent 1: q_loss: 0.3718864321708679, p_loss: 8.151921272277832, mean_rew: -0.34813680203723224, var_rew: 0.20564458605778582
steps: 549975, episodes: 22000, mean episode reward: -9.965806882768211, agent episode reward: [-1.9986545263841387, -7.967152356384072], time: 31.308
Running avgs for agent 0: q_loss: 0.8429587483406067, p_loss: 1.1956584453582764, mean_rew: -0.08618287006580302, var_rew: 0.15157810795791488
Running avgs for agent 1: q_loss: 0.37128064036369324, p_loss: 8.298009872436523, mean_rew: -0.3478532308183927, var_rew: 0.20366881567598674
steps: 574975, episodes: 23000, mean episode reward: -10.201645545741796, agent episode reward: [-1.81424612553592, -8.387399420205877], time: 31.276
Running avgs for agent 0: q_loss: 0.8402050733566284, p_loss: 1.2412749528884888, mean_rew: -0.08600126350898014, var_rew: 0.1505836874987129
Running avgs for agent 1: q_loss: 0.3803029954433441, p_loss: 8.402693748474121, mean_rew: -0.34649039486718697, var_rew: 0.20049030148512634
steps: 599975, episodes: 24000, mean episode reward: -10.149998665500686, agent episode reward: [-2.1187663981998903, -8.031232267300794], time: 31.063
Running avgs for agent 0: q_loss: 0.8724864721298218, p_loss: 1.2401031255722046, mean_rew: -0.08483421700146705, var_rew: 0.1484058446348679
Running avgs for agent 1: q_loss: 0.378243088722229, p_loss: 8.498739242553711, mean_rew: -0.3441442081969101, var_rew: 0.19737951615601446
steps: 624975, episodes: 25000, mean episode reward: -10.040668561171167, agent episode reward: [-1.8072393525671655, -8.233429208604003], time: 31.549
Running avgs for agent 0: q_loss: 0.8836029171943665, p_loss: 1.2594718933105469, mean_rew: -0.08486940858613348, var_rew: 0.14877323306721799
Running avgs for agent 1: q_loss: 0.385212242603302, p_loss: 8.609294891357422, mean_rew: -0.34284083075451466, var_rew: 0.19487386621574554
steps: 649975, episodes: 26000, mean episode reward: -10.090256774765312, agent episode reward: [-1.9990499212315689, -8.091206853533743], time: 31.327
Running avgs for agent 0: q_loss: 0.8950694799423218, p_loss: 1.2520750761032104, mean_rew: -0.08476292994679022, var_rew: 0.1455608862480297
Running avgs for agent 1: q_loss: 0.39011481404304504, p_loss: 8.744587898254395, mean_rew: -0.34332467481163564, var_rew: 0.19425235688046785
steps: 674975, episodes: 27000, mean episode reward: -10.134485911542301, agent episode reward: [-1.9720309403294078, -8.162454971212892], time: 31.106
Running avgs for agent 0: q_loss: 0.9028431177139282, p_loss: 1.250842809677124, mean_rew: -0.08470982282263854, var_rew: 0.14406441172413162
Running avgs for agent 1: q_loss: 0.3907933831214905, p_loss: 8.846057891845703, mean_rew: -0.3426073706552296, var_rew: 0.19197483982948346
steps: 699975, episodes: 28000, mean episode reward: -10.274856543910555, agent episode reward: [-2.0010001534277926, -8.273856390482765], time: 32.134
Running avgs for agent 0: q_loss: 0.9256144762039185, p_loss: 1.2502765655517578, mean_rew: -0.08479445093622859, var_rew: 0.14371992835662373
Running avgs for agent 1: q_loss: 0.38212594389915466, p_loss: 8.946330070495605, mean_rew: -0.3444754272511756, var_rew: 0.19196857576711782
steps: 724975, episodes: 29000, mean episode reward: -10.011341817271324, agent episode reward: [-1.7670810443796476, -8.244260772891677], time: 30.856
Running avgs for agent 0: q_loss: 0.9229223132133484, p_loss: 1.202345609664917, mean_rew: -0.08356275746207587, var_rew: 0.14148393736834203
Running avgs for agent 1: q_loss: 0.39066192507743835, p_loss: 8.984455108642578, mean_rew: -0.3416412935055487, var_rew: 0.1895389286099455
steps: 749975, episodes: 30000, mean episode reward: -10.200291676093293, agent episode reward: [-1.877624300119608, -8.322667375973683], time: 30.737
Running avgs for agent 0: q_loss: 0.9238889217376709, p_loss: 1.1898794174194336, mean_rew: -0.08277825215288684, var_rew: 0.14006699776737333
Running avgs for agent 1: q_loss: 0.39364320039749146, p_loss: 9.056448936462402, mean_rew: -0.34214271408343006, var_rew: 0.18839783354996456
steps: 774975, episodes: 31000, mean episode reward: -10.494346327306372, agent episode reward: [-2.1597771697348396, -8.334569157571531], time: 31.149
Running avgs for agent 0: q_loss: 0.9291332364082336, p_loss: 1.1746809482574463, mean_rew: -0.08284414170815775, var_rew: 0.13919600709771685
Running avgs for agent 1: q_loss: 0.39919978380203247, p_loss: 9.104466438293457, mean_rew: -0.34110967912976153, var_rew: 0.18670248314053425
steps: 799975, episodes: 32000, mean episode reward: -10.702214636746584, agent episode reward: [-2.348806985844365, -8.35340765090222], time: 31.011
Running avgs for agent 0: q_loss: 0.9274422526359558, p_loss: 1.2071926593780518, mean_rew: -0.0834699410145793, var_rew: 0.13697603798488017
Running avgs for agent 1: q_loss: 0.3941105604171753, p_loss: 9.156720161437988, mean_rew: -0.3414692767583581, var_rew: 0.184718603885273
steps: 824975, episodes: 33000, mean episode reward: -10.461707232011808, agent episode reward: [-2.2583431192592123, -8.203364112752595], time: 34.536
Running avgs for agent 0: q_loss: 0.9225627779960632, p_loss: 1.1715798377990723, mean_rew: -0.08102649798013409, var_rew: 0.13730885708792903
Running avgs for agent 1: q_loss: 0.39085328578948975, p_loss: 9.183415412902832, mean_rew: -0.34117617577712755, var_rew: 0.18324383220011264
steps: 849975, episodes: 34000, mean episode reward: -10.592646469154065, agent episode reward: [-2.0462289001816383, -8.546417568972426], time: 39.317
Running avgs for agent 0: q_loss: 0.9336655139923096, p_loss: 1.2356493473052979, mean_rew: -0.08372136942418051, var_rew: 0.13545559359747572
Running avgs for agent 1: q_loss: 0.3965816795825958, p_loss: 9.215950965881348, mean_rew: -0.34041021070949523, var_rew: 0.18205936614207918
steps: 874975, episodes: 35000, mean episode reward: -10.52252690081325, agent episode reward: [-2.0834460581382817, -8.439080842674969], time: 31.157
Running avgs for agent 0: q_loss: 0.9384332895278931, p_loss: 1.2120381593704224, mean_rew: -0.08427092026379793, var_rew: 0.13499872930218765
Running avgs for agent 1: q_loss: 0.402079701423645, p_loss: 9.252128601074219, mean_rew: -0.34087185831650646, var_rew: 0.18241764743954605
steps: 899975, episodes: 36000, mean episode reward: -10.63728847643306, agent episode reward: [-2.2283197360926597, -8.408968740340399], time: 30.401
Running avgs for agent 0: q_loss: 0.9452722072601318, p_loss: 1.2243738174438477, mean_rew: -0.08403362746331768, var_rew: 0.13432817520593215
Running avgs for agent 1: q_loss: 0.40266433358192444, p_loss: 9.272997856140137, mean_rew: -0.3416177762400499, var_rew: 0.18198667883190534
steps: 924975, episodes: 37000, mean episode reward: -10.606281081046587, agent episode reward: [-1.7737554077045738, -8.832525673342014], time: 31.519
Running avgs for agent 0: q_loss: 0.9507207870483398, p_loss: 1.2414458990097046, mean_rew: -0.08393143049717845, var_rew: 0.1340313171628057
Running avgs for agent 1: q_loss: 0.41232749819755554, p_loss: 9.28152084350586, mean_rew: -0.3414165237895179, var_rew: 0.1800883964973474
steps: 949975, episodes: 38000, mean episode reward: -10.558605863807642, agent episode reward: [-1.892863597709943, -8.665742266097702], time: 30.47
Running avgs for agent 0: q_loss: 0.9600532054901123, p_loss: 1.2521963119506836, mean_rew: -0.08258588721369158, var_rew: 0.13365671443355293
Running avgs for agent 1: q_loss: 0.42517584562301636, p_loss: 9.309457778930664, mean_rew: -0.3424268508123272, var_rew: 0.17981532411863388
steps: 974975, episodes: 39000, mean episode reward: -10.61322691954328, agent episode reward: [-2.1247527739851306, -8.48847414555815], time: 31.66
Running avgs for agent 0: q_loss: 0.9698131084442139, p_loss: 1.2582335472106934, mean_rew: -0.08285008272805863, var_rew: 0.1328261886473618
Running avgs for agent 1: q_loss: 0.41841840744018555, p_loss: 9.334585189819336, mean_rew: -0.34240692161500746, var_rew: 0.17842115700352237
steps: 999975, episodes: 40000, mean episode reward: -11.011814274167971, agent episode reward: [-2.409328378150469, -8.602485896017502], time: 31.459
Running avgs for agent 0: q_loss: 0.9782451391220093, p_loss: 1.2395662069320679, mean_rew: -0.08294057926356035, var_rew: 0.13175560909293302
Running avgs for agent 1: q_loss: 0.4166335463523865, p_loss: 9.33988094329834, mean_rew: -0.34161729015527054, var_rew: 0.1776488861420438
steps: 1024975, episodes: 41000, mean episode reward: -10.857638747165275, agent episode reward: [-2.6380659894683776, -8.219572757696897], time: 30.73
Running avgs for agent 0: q_loss: 0.9519051909446716, p_loss: 1.307851791381836, mean_rew: -0.08442881757595919, var_rew: 0.12827450086522782
Running avgs for agent 1: q_loss: 0.4076407551765442, p_loss: 9.201417922973633, mean_rew: -0.33213875475022897, var_rew: 0.1679408537032468
steps: 1049975, episodes: 42000, mean episode reward: -10.813021142135499, agent episode reward: [-1.9314831315435077, -8.881538010591994], time: 30.713
Running avgs for agent 0: q_loss: 0.916085422039032, p_loss: 1.2872233390808105, mean_rew: -0.08100157231097845, var_rew: 0.1138927482629922
Running avgs for agent 1: q_loss: 0.38137102127075195, p_loss: 8.984841346740723, mean_rew: -0.3161265416710505, var_rew: 0.15263485894592452
steps: 1074975, episodes: 43000, mean episode reward: -10.654394629826953, agent episode reward: [-2.10538587178314, -8.549008758043811], time: 31.244
Running avgs for agent 0: q_loss: 0.9142310619354248, p_loss: 1.2768460512161255, mean_rew: -0.07741526863054736, var_rew: 0.1119857675412836
Running avgs for agent 1: q_loss: 0.3638107180595398, p_loss: 9.013858795166016, mean_rew: -0.3195071608905561, var_rew: 0.15341437249745535
steps: 1099975, episodes: 44000, mean episode reward: -10.74206210740136, agent episode reward: [-2.046710589017875, -8.695351518383482], time: 30.749
Running avgs for agent 0: q_loss: 0.919090986251831, p_loss: 1.3492969274520874, mean_rew: -0.0782438727726176, var_rew: 0.1115313749621956
Running avgs for agent 1: q_loss: 0.3530954420566559, p_loss: 8.978787422180176, mean_rew: -0.31954902133401764, var_rew: 0.1521001405781263
steps: 1124975, episodes: 45000, mean episode reward: -10.732300431336958, agent episode reward: [-2.2194455124555543, -8.512854918881406], time: 30.945
Running avgs for agent 0: q_loss: 0.9010401368141174, p_loss: 1.3847277164459229, mean_rew: -0.07782011397734617, var_rew: 0.10937781371883411
Running avgs for agent 1: q_loss: 0.3467008173465729, p_loss: 8.969152450561523, mean_rew: -0.3219117922456219, var_rew: 0.151137002987603
steps: 1149975, episodes: 46000, mean episode reward: -10.958603337681447, agent episode reward: [-2.275375221361799, -8.683228116319649], time: 30.836
Running avgs for agent 0: q_loss: 0.8724915385246277, p_loss: 1.4649324417114258, mean_rew: -0.07860974409198251, var_rew: 0.1106367368450128
Running avgs for agent 1: q_loss: 0.3363461196422577, p_loss: 8.953461647033691, mean_rew: -0.323192178130891, var_rew: 0.15054760990267274
steps: 1174975, episodes: 47000, mean episode reward: -10.873535101895765, agent episode reward: [-2.361368707923393, -8.51216639397237], time: 31.091
Running avgs for agent 0: q_loss: 0.8501251339912415, p_loss: 1.538891315460205, mean_rew: -0.07827733267707716, var_rew: 0.1104797466787079
Running avgs for agent 1: q_loss: 0.34608909487724304, p_loss: 8.94414234161377, mean_rew: -0.32423481090242956, var_rew: 0.14956429695972565
steps: 1199975, episodes: 48000, mean episode reward: -10.714392577808521, agent episode reward: [-2.1971578666662315, -8.51723471114229], time: 30.963
Running avgs for agent 0: q_loss: 0.8189468383789062, p_loss: 1.6293113231658936, mean_rew: -0.07828024934090365, var_rew: 0.10962399323222421
Running avgs for agent 1: q_loss: 0.3458641469478607, p_loss: 8.975115776062012, mean_rew: -0.3263219755747805, var_rew: 0.14965144907542977
steps: 1224975, episodes: 49000, mean episode reward: -10.68013700691808, agent episode reward: [-1.993338020714328, -8.686798986203755], time: 30.465
Running avgs for agent 0: q_loss: 0.8024411201477051, p_loss: 1.7212904691696167, mean_rew: -0.07937670760451095, var_rew: 0.10989793216296022
Running avgs for agent 1: q_loss: 0.33831682801246643, p_loss: 8.975229263305664, mean_rew: -0.326552627596271, var_rew: 0.1485646496099107
steps: 1249975, episodes: 50000, mean episode reward: -10.564739251740543, agent episode reward: [-2.1677132386305193, -8.397026013110022], time: 31.368
Running avgs for agent 0: q_loss: 0.7651291489601135, p_loss: 1.7626855373382568, mean_rew: -0.0780011359436221, var_rew: 0.1081614621166014
Running avgs for agent 1: q_loss: 0.33621513843536377, p_loss: 8.987847328186035, mean_rew: -0.32839773434233266, var_rew: 0.1483073101584143
steps: 1274975, episodes: 51000, mean episode reward: -10.924064732346684, agent episode reward: [-2.4170486401181104, -8.507016092228572], time: 31.952
Running avgs for agent 0: q_loss: 0.7326574921607971, p_loss: 1.862437129020691, mean_rew: -0.07898404861176289, var_rew: 0.1074566724807983
Running avgs for agent 1: q_loss: 0.32682064175605774, p_loss: 9.000768661499023, mean_rew: -0.32868602191271756, var_rew: 0.1481288876439545
steps: 1299975, episodes: 52000, mean episode reward: -10.791924648032062, agent episode reward: [-2.36730665791288, -8.42461799011918], time: 31.018
Running avgs for agent 0: q_loss: 0.7151185274124146, p_loss: 1.9662929773330688, mean_rew: -0.07985836142644559, var_rew: 0.10762343946809107
Running avgs for agent 1: q_loss: 0.3208577334880829, p_loss: 9.026103973388672, mean_rew: -0.33138647215189393, var_rew: 0.14781664096710076
steps: 1324975, episodes: 53000, mean episode reward: -10.662020068217949, agent episode reward: [-2.140656889449821, -8.52136317876813], time: 31.278
Running avgs for agent 0: q_loss: 0.6880447864532471, p_loss: 2.0864076614379883, mean_rew: -0.08122694472336116, var_rew: 0.1080617874226343
Running avgs for agent 1: q_loss: 0.3184417188167572, p_loss: 8.999810218811035, mean_rew: -0.33082343214482707, var_rew: 0.1464156583767994
steps: 1349975, episodes: 54000, mean episode reward: -10.891657540885422, agent episode reward: [-2.0326626554868312, -8.858994885398591], time: 30.668
Running avgs for agent 0: q_loss: 0.6746666431427002, p_loss: 2.1707358360290527, mean_rew: -0.08183812831045863, var_rew: 0.107198351029075
Running avgs for agent 1: q_loss: 0.3236965835094452, p_loss: 9.010152816772461, mean_rew: -0.3321622896799803, var_rew: 0.1461740359389867
steps: 1374975, episodes: 55000, mean episode reward: -10.87428037574815, agent episode reward: [-2.094114223115102, -8.780166152633047], time: 30.856
Running avgs for agent 0: q_loss: 0.6508121490478516, p_loss: 2.231053352355957, mean_rew: -0.08107557640192714, var_rew: 0.10670890111595853
Running avgs for agent 1: q_loss: 0.314916729927063, p_loss: 9.037156105041504, mean_rew: -0.3338829040462648, var_rew: 0.1461711868309406
steps: 1399975, episodes: 56000, mean episode reward: -10.694320678399363, agent episode reward: [-1.9040938699224972, -8.790226808476866], time: 31.427
Running avgs for agent 0: q_loss: 0.6326532959938049, p_loss: 2.3114163875579834, mean_rew: -0.08269293356670947, var_rew: 0.10660298466351412
Running avgs for agent 1: q_loss: 0.29896050691604614, p_loss: 9.005993843078613, mean_rew: -0.33396479848671573, var_rew: 0.14427327852446983
steps: 1424975, episodes: 57000, mean episode reward: -10.556499816920411, agent episode reward: [-1.8846115277395048, -8.671888289180906], time: 30.677
Running avgs for agent 0: q_loss: 0.6213437914848328, p_loss: 2.3544492721557617, mean_rew: -0.08253453266699355, var_rew: 0.10736506650346134
Running avgs for agent 1: q_loss: 0.2938694953918457, p_loss: 9.025793075561523, mean_rew: -0.3359815234711231, var_rew: 0.14521259848769694
steps: 1449975, episodes: 58000, mean episode reward: -10.634350301597001, agent episode reward: [-2.093914634629096, -8.540435666967907], time: 30.618
Running avgs for agent 0: q_loss: 0.614753007888794, p_loss: 2.389620780944824, mean_rew: -0.08405487622610114, var_rew: 0.10740203231538493
Running avgs for agent 1: q_loss: 0.2962912619113922, p_loss: 9.046515464782715, mean_rew: -0.33738096399769496, var_rew: 0.1459362293813999
steps: 1474975, episodes: 59000, mean episode reward: -10.633836671887092, agent episode reward: [-1.9510524627780639, -8.682784209109029], time: 31.311
Running avgs for agent 0: q_loss: 0.6133849024772644, p_loss: 2.3964428901672363, mean_rew: -0.08388823221842957, var_rew: 0.1069383368139811
Running avgs for agent 1: q_loss: 0.2766329348087311, p_loss: 9.011824607849121, mean_rew: -0.3370203913435457, var_rew: 0.14465860330603583
steps: 1499975, episodes: 60000, mean episode reward: -10.597928421597159, agent episode reward: [-1.8217400184163532, -8.776188403180804], time: 30.942
Running avgs for agent 0: q_loss: 0.6041083931922913, p_loss: 2.3935322761535645, mean_rew: -0.08400774569882116, var_rew: 0.10633648266540689
Running avgs for agent 1: q_loss: 0.2754867672920227, p_loss: 9.022908210754395, mean_rew: -0.3391295367096648, var_rew: 0.14482312102497588
Traceback (most recent call last):
  File "train_vanila.py", line 230, in <module>
    train(arglist)
  File "train_vanila.py", line 219, in train
    rew_file_name = arglist.plots_dir + arglist.exp_name + '_rewards.pkl'

